<!DOCTYPE html><html><head><title>TF-IDF is about what matters</title></head><body>
<h1>TF-IDF is about what matters</h1><p><a href="http://planspace.org/20150524-tfidf_is_about_what_matters/" target="_new">Original URL</a></p>
<p><blockquote>Sunday May 24, 2015 TF-IDF is term frequency inverse document frequency. But what does that mean? Consider a collection (or corpus, or set) of four sentences (or documents, or strings) made up of&hellip;</blockquote></p>
<div><article>

<p class="date">Sunday May 24, 2015</p>
<p>TF-IDF is <em>term frequency inverse document frequency</em>. But what does that mean?</p>
<p><img alt="cat and dog" src="http://planspace.org/20150524-tfidf_is_about_what_matters/cat_and_dog.png"></p>


<p>Consider a collection (or corpus, or set) of four sentences (or documents, or strings) made up of words (or terms, or tokens):</p>
<ol>
<li>the cat and dog sat</li>
<li>the dog and cat sat</li>
<li>the cat sat and sat</li>
<li>the cat killed the dog</li>
</ol>
<p>One thing we can do is transform this corpus to a bag of words representation, which just means we&#x2019;re keeping track of what words there are but not what order they&#x2019;re in. We could use binary values to represent whether a word appears or not:</p>
<pre><code class="language-text"> the cat and dog sat killed
1 1 1 1 1 1 0
2 1 1 1 1 1 0
3 1 1 1 0 1 0
4 1 1 0 1 0 1</code></pre>

<p>In this representation we've lost word order, but we've also lost <em>term frequency</em>: we can't tell that sentence three has twice as much sitting as sentence two. Let's use the counts:</p>
<pre><code class="language-text"> the cat and dog sat killed
1 1 1 1 1 1 0
2 1 1 1 1 1 0
3 1 1 1 0 2 0
4 2 1 0 1 0 1</code></pre>

<p>Now we can see how many times each term appears in each document.</p>
<p>Is it good that &#x201C;killed&#x201D; is given the same score as &#x201C;cat&#x201D; in sentence four? The word &#x201C;killed&#x201D; is much rarer, after all; it is more &#x201C;special&#x201D; to sentence four. Let&#x2019;s count the number of sentences that have each word, and call this the <em>document frequency</em>:</p>
<pre><code class="language-text"> the cat and dog sat killed
 4 4 3 3 3 1</code></pre>

<p>The document frequencies are high for words that are boring, so dividing by the document frequencies will give us low scores for boring words and high scores for interesting words:</p>
<pre><code class="language-text"> the cat and dog sat killed
1 0.25 0.25 0.33 0.33 0.33 0
2 0.25 0.25 0.33 0.33 0.33 0
3 0.25 0.25 0.33 0 0.67 0
4 0.50 0.25 0 0.33 0 1.00</code></pre>

<p>These scores match the intuition that &#x201C;killed&#x201D; is a very important word. In addition to making some intuitive sense, scores like these tend to work well in machine learning tasks.</p>
<p>All we did was this:</p>
<p>\[ \frac{\text{number of times the word is in this document}}{\text{number of documents the word is in}} \]</p>
<p>Or, for short:</p>
<p>\[ \frac{\text{term frequency}}{\text{document frequency}} \]</p>
<p>Dividing by a number is the same as multiplying by its inverse, \( \frac{1}{\text{a number}} \), so we can call this transformation <em>term frequency inverse document frequency</em>.</p>
<p>This is not a very difficult idea. You could have invented it yourself. But it gets so buried in terminology and secondary implementation details that people sometimes talk about it as if it were a deep and mysterious modeling technique. (It's not.)</p>
<p>Here's an <a href="http://www.cs.utexas.edu/~ml/papers/marlin-dissertation-06.pdf">example</a> of how the exposition can lose people. It starts nicely:</p>
<blockquote>
<p>&#x201C;Tokens that occur frequently in a given string should have higher contribution to similarity than those that occur few times, as should those tokens that are rare among the set of strings under consideration.&#x201D;</p>
</blockquote>
<p>Who could disagree? Then it jumps to this:</p>
<blockquote>
<p>&#x201C;The Term Frequency-Inverse Document Frequency (TF-IDF) weighting scheme achieves this by associating a weight \( w_{v_i,s} = \frac{N(v_i,s)}{\text{max}_{v_j \in s} N(v_j,s)} \cdot \text{log} \frac{N}{N(v_i)} \) with every token \( v_i \) from string \( s \), where \( N(v_i, s) \) is the number of times \( v_i \) occurs in \( s \) (term frequency), \( N \) is the number of strings in the overall corpus under consideration, and \( N(v_i) \) is the number of strings in the corpus that include \( v_i \) (document frequency).&#x201D;</p>
</blockquote>
<p>There's some notation there, and we've also thrown in two normalizations and a log. None of this is a particularly big deal, but it makes TF-IDF seem complicated. There are <a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">a lot of ways</a> you could do the particulars of TF-IDF but they all achieve essentially what we did above by just counting and dividing.</p>
<p>It can also be confusing when results don't match what you expect. Here is what you get by default (with <a href="http://planspace.org/20150524-tfidf_is_about_what_matters/tfidf.py">this script</a>) from <a href="http://scikit-learn.org/">sklearn</a>'s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer</a>:</p>
<pre><code class="language-text"> the cat and dog sat killed
1 0.39 0.39 0.48 0.48 0.48 0
2 0.39 0.39 0.48 0.48 0.48 0
3 0.32 0.32 0.40 0 0.79 0
4 0.63 0.31 0 0.38 0 0.60</code></pre>

<p>The formula <code>sklearn</code> version 0.16.1 uses to compute TF-IDF is this:</p>
<p>\[ \text{term frequency} \cdot \left ( \text{log} \left ( \frac{\text{number of documents} + 1}{\text{document frequency} +1} \right ) + 1 \right ) \]</p>
<p>And then <code>sklearn</code> also scales the rows of the results to each have unit length.</p>
<p>So you could be surprised that the results of a TF-IDF implementation don't exactly match the method you were thinking of. As above, there are a lot of small choices in exactly how to calculate TF-IDF, and <code>sklearn</code> chooses one particular implementation.</p>

<p>I've seen better classification performance with TF-IDF than with raw counts, but I haven't seen a systematic comparison across various TF-IDF methods; it might be interesting to see such a comparison.</p>

<p>Thanks to <a href="https://twitter.com/hallr">Rob Hall</a> for doing <a href="https://github.com/ga-students/DAT_SF_13/tree/master/lectures#session-14-natural-language-processing">work</a> that helped inspire this post!</p>

 </article>
 </div>
</body></html>
