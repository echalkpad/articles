# The libraries of Babel, Mendel and Turing

[Original URL](http://haggstrom.blogspot.com/2015/04/the-libraries-of-babel-mendel-and-turing.html)

> There is much disagreement concerning the dangers associated with a breakthrough in artificial intelligence. Could such an event lead to our extinction, or the End of the Human Era as James Barrat...

There is much disagreement concerning the dangers associated with a breakthrough in artificial intelligence. Could such an event lead to our extinction, or _the End of the Human Era_ as James Barrat put it in the subtitle of his book [_Our Final Invention_](http://www.adlibris.com/se/bok/our-final-invention-artificial-intelligence-and-the-end-of-the-human-era-9780312622374)? Thinkers like [Eliezer Yudkowsky](https://intelligence.org/files/AIPosNegFactor.pdf) and [Nick Bostrom](http://haggstrom.blogspot.se/2014/09/superintelligence-review.html) argue that the risk is real and that we need to solve the (very difficult) problem of how to avoid an AI Armageddon. As is clear from [my review of Bostrom's book _Superintelligence_](http://haggstrom.blogspot.se/2014/09/superintelligence-review.html), I find their arguments sufficiently compelling to warrant serious attention.

Many others consider (for a wide variety of reasons) the risk to be an illusion. The latest contributor to this highly diverse camp is computer scientist [Thore Husfeldt](http://thorehusfeldt.net/), with his essay [_The Monster in the Library of Turing_](http://thorehusfeldt.net/2015/04/10/the-monster-in-the-library-of-turing/), which is a review of _Superintelligence_ to appear in the Swedish philosophy journal [_Filosofisk tidskrift_](http://www.bokforlagetthales.se/filosofisktidskrift/index.htm) and [published in English translation on his blog](http://thorehusfeldt.net/2015/04/10/the-monster-in-the-library-of-turing/). I find Husfeldt's essay very interesting, and what sets him apart from nearly every other critic of the Bostromian-Yudkowskian position (such as [Selmer Bringsjord](http://kryten.mm.rpi.edu/SB_singularity_math_final.pdf), [Rodney Brooks](http://www.rethinkrobotics.com/artificial-intelligence-tool-threat/), [David Deutsch](http://beginningofinfinity.com/), [Steven Pinker](http://edge.org/response-detail/26243), [John Searle](http://www.nybooks.com/articles/archives/2014/oct/09/what-your-computer-cant-know/) and [David Sumpter](http://haggstrom.blogspot.se/2013/10/guest-post-by-david-sumpter-why.html), just to mention a few of those whose criticisms I debunk at shorter or greater length in my upcoming book _Here Be Dragons: Science, Techonlogy and the Future of Humanity_ (Oxford University Press, to appear)) is that neither does his argument consist in attacking straw men, nor does it succumb to any obvious fallacy. Husfeldt has read _Superintelligence_ carefully and seems to accept many of its arguments, but disagrees on (at least) one crucial point. To make his case, he invokes a new metaphor: the **Library of Turing**, building on Jorge Luis Borges' **Library of Babel** and Daniel Dennett's **Library of Mendel**. Let me breifly describe these three libraries:

- **The Library of Babel** goes back to [Jorge Luis Borges' short story from 1941 with the same name](http://hyperdiscordia.crywalt.com/library_of_babel.html). It contains of all of the 25

  <sup>1,312,000</sup>

   books with 1,312,000 characters (we learn from the story that _"each book is of four hundred and ten pages; each page, of forty lines, each line, of some eighty letters which are black in color"_; 1,312,000=410·40·80) taken from a 25-letter alphabet, including blank space. 25

  <sup>1,312,000</sup>

   is an enormously large number, so large that the number of books that would fit in the observable universe is ridiculously small by comparison. We can still try to imagine the library. It contains many wonderful books, including Shakespeare's _Hamlet_ and Solzhenitsyn's _One Day in the Life of Ivan Denisovich_, but in contains far more books consisting of pure gibberish. The library is so huge that, even if we assume the books to be ordered alphabetically, the interesting books are virtually impossible to find, unless you know _exactly_ what you are looking for. To know which of the library's 25 buildings to go to, you need to know the book's first letter; to know which of the 25 floors of that building to go to, you need to know the book's second letter, and so on and so forth. If you are sitting in front of a word processor, you actually have the Library of Babel at your fingertips: if you first specify the book's first letter by typing it, then specify the second letter similarly, all the way down to the 1,312,000th and last, then - ta-dah! - there is your book.
- **The Library of Mendel**, invented by Daniel Dennett in his 1995 book [_Darwin's Dangerous Idea_](http://www.adlibris.com/se/bok/darwins-dangerous-idea-9780684824710), consists of all DNA sequences of length up to (say) 10,000,000,000\. Here we have only four characters (A, C, G and T) but the greater lengths allowed compared to the Library of Babel more than make up for that, so that the number of DNA sequences in the Library of Mendel is even larger than the number of books in the Library of Babel. Just like there are very many interesting books in the Library of Babel, there are very many DNA sequences in the Library of Mendel that code for viable complex organisms. These two subsets from the respective libraries are what Dennett call **Vast but Vanishing**, where **Vast** means roughly that their number is far far bigger than the number of elementary particles in the observable universe, and **Vanishing** means that their proportion of the entire library is so small that the reciprocal of that proportion is Vast.
- **The Library of Turing** - [Thore Husfeldt's new invention](http://thorehusfeldt.net/2015/04/10/the-monster-in-the-library-of-turing/) - consists of all sequences of length up to (say) 10

  <sup>20</sup>

   (a very large number, but far from Vast) from an 128-character alphabet - sequences that are interpreted as computer programs written in a given programming language, which Husfeldt takes to be Lisp, but any [Turing-complete](http://en.wikipedia.org/wiki/Turing_completeness) language will do. As is the case in the libraries of Babel and Mendel, most sequences are just gibberish, but there is a Vast but Vanishing subset of interesting programs.

Husfeldt accepts that the human brain is probably far from an optimal arrangement of matter to produce intelligence, and he accepts functionalism. This leads him to belive (as Bostrom does, and Yudkowsky, and me) that somewhere in the Library of Turing, there is some program that far outstrips our feeble human powers of intelligence. But can we find it? [Here is Husfeldt:](http://thorehusfeldt.net/2015/04/10/the-monster-in-the-library-of-turing/)

The point here is that, although presumably Vast, the set of superhumanly intelligent programs in the Library of Turing is also Vanishing, and therefore geological time scales will be far far from sufficient for brute force to succeed.

If the Library of Turing entirely lacked structure, this would be the end of the argument: if there were no structure, there would be no way to improve on brute force search, so there would be no hope (or, to be pedantically precise, a Vanishingly small hope) to come up with a superhumanly intelligent computer program. Intelligent design proponent William Dembski, in his book [_No Free Lunch_](http://www.adlibris.com/se/bok/no-free-lunch-9780742558106), tried (for the purpose of concluding that we have been deliberately designed by an intelligent being) to employ this type of argument to prove that Darwinian evolution is impossible. He hid away the (utterly unreasonable) unstructuredness assumption behind mathematical smokescreens, so as to render it undetectable to most readers, but [he failed to make it undetectable to yours truly](http://link.springer.com/article/10.1007%2Fs10539-006-9040-z#page-1).

The unstructuredness assumption is just as unreasonable for the Library of Turing as it is for the Library of Mendel, since if the former lacked structure, we would not be able to write useful computer programs. Now, Husfeldt, unlike Dembski, is an honest thinker, so of course he makes no attempt to pretend that the Library of Turing is unstructured, and he admits that there _might_ be ways to improve on brute force search sufficiently well that a superhumanly intelligent program can be found, but intuits that it can't be done:

Here Husfeldt's intuition and mine part from each other. I agree with him that most likely [P≠NP](http://en.wikipedia.org/wiki/P_versus_NP_problem), but I don't see why that would be such a blow to our prospects of finding a superintelligent program. We can do a lot of wonderful things without solving NP-hard problems in polynomial time, and I think the success that Nature has had in coming up with the brain of _Homo sapiens_ and many other impressive things is very suggestive. The Library of Turing is probably at least as structured as the Library of Babel, offering at least as navigable a landscape to search in if we should choose to imitate Nature's method of evolution by natural selection. And we have the bonus option of using our own intelligence to make clever macro-moves in the landscape. I am not in a position to tell whether symbolic reasoning is a more promising approach than machine learning, but there seem to be many potential ways forward. I think there's a good chance (or risk!) we'll create superintelligence before the end of the persent century, and perhaps a lot sooner than that.

This divergence of our intuitions notwithstanding, I find Husfeldt's essay interesting and stimulating, and it ends on a constructive note, suggesting a number of research directions that may turn out worthwhile even in case he is right about the inaccessability of _"the monster in the Library of Turing"_. There's just one passage I find a little bit misleading: Computation is a resource, exponential growth is immense, and the universe is finite. Bostrom pays little attention to this problem. For instance, in his discussion of current efforts on brain simulation, he writes:

Well, to me, scaling _is_ the problem. Here (and in combination with the essay's overall focus on exponential complexity), Husfeldt seems to suggest that the problem of emulating a brain of size _n_ would suffer from a combinatorial explosion and the accompanying exponential increase in computational complexity. Does he really mean this? I don't want to claim that whole brain emulation is an easy project ([it most certainly isn't!](http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf)), but to me the scaling problem here sounds like it should land in a linear complexity in _n_, or perhaps a low-degree polynomial, but not exponential, as I don't see where the combinatorial explosion would be coming from.
