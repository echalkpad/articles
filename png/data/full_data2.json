{
    "articles": [{
        "title": "Build a JavaScript Command Line Interface (CLI) with Node.js",
        "url": "https://www.sitepoint.com/javascript-command-line-interface-cli-node-js/",
        "excerpt": "This article was peer reviewed by Dan Prince. Thanks to all of SitePoint&#x2019;s peer reviewers for making SitePoint content the best it can be! As great as Node.js is for&hellip;",
        "date_saved": "Sat Jul 30 02:13:21 EDT 2016",
        "html_file": "build_a_javascript_command_line_interface_cli_with_nodejs.html",
        "png_file": "build_a_javascript_command_line_interface_cli_with_nodejs.png",
        "md_file": "build_a_javascript_command_line_interface_cli_with_nodejs.md",
        "content": "<div><div class=\"ArticleCopy language-\">\n \n<p><img src=\"https://dab1nmslvvntp.cloudfront.net/wp-content/uploads/2016/07/1468406617node-github1d-01-01.png\" alt=\"Build a Node CLI\" width=\"751\" class=\"alignleft size-full wp-image-135109\"></p>\n<p><em>This article was peer reviewed by <a href=\"https://www.sitepoint.com/author/dprince\">Dan Prince</a>. Thanks to all of SitePoint&#x2019;s peer reviewers for making SitePoint content the best it can be!</em></p>\n<p>As great as Node.js is for &#x201C;traditional&#x201D; web applications, its potential uses are far broader. Microservices, REST APIs, tooling, working with the Internet of Things and even desktop applications&#x2014;it&#x2019;s got your back.</p>\n\n<p>Another area where Node.js is really useful is for building command-line applications&#x2014;and that&#x2019;s what we&#x2019;re going to be doing today. We&#x2019;re going to start by looking at a number of third-party packages designed to help work with the command-line, then build a real-world example from scratch.</p>\n<p>What we&#x2019;re going to build is a tool for initializing a Git repository. Sure, it&#x2019;ll run <code>git init</code> under the hood, but it&#x2019;ll do more than just that. It will also create a remote repository on Github right from the command line, allow the user to interactively create a <code>.gitignore</code> file and finally perform an initial commit and push.</p>\n<p>As ever, the code accompanying this tutorial can be found on our <a href=\"https://github.com/sitepoint-editors/ginit\">GitHub repo</a>.</p>\n<h2 id=\"whybuildacommandlinetoolwithnodejs\">Why Build a Command-line Tool with Node.js?</h2>\n<p>Before we dive in and start building, it&#x2019;s worth looking at why we might choose Node.js to build a command-line application.</p>\n<p>The most obvious advantage is that if you&#x2019;re reading this, you&#x2019;re probably already familiar with it&#x2014;and indeed, with JavaScript.</p>\n<p>Another key advantage, as we&#x2019;ll see as we go along, is that the strong Node.js ecosystem means that among the hundreds of thousands of packages available for all manner of purposes, there are a number which are specifically designed to help build powerful command-line tools.</p>\n<p>Finally, we can use <code>npm</code> to manage any dependencies, rather than have to worry about OS-specific package managers such as Aptitude, Yum or Homebrew.</p>\n<p class=\"tip\">That said, that&#x2019;s not necessarily true, in that your command-line tool may have other external dependencies.</p>\n<h2 id=\"whatweregoingtobuildintroducingginit\">What We&#x2019;re Going to Build&#x2014;Introducing ginit</h2>\n<p><img src=\"https://dab1nmslvvntp.cloudfront.net/wp-content/uploads/2016/07/1467718020Ginit-in-action.png\" alt=\"Ginit, our Node CLI in action\"></p>\n<p>For this tutorial, We&#x2019;re going to create a command-line utility which I&#x2019;m calling <strong>ginit</strong>. It&#x2019;s <code>git init</code>, but on steroids.</p>\n<p>You&#x2019;re probably wondering what on earth that means.</p>\n<p>As you no doubt already know, <code>git init</code> initializes a git repository in the current folder. However, that&#x2019;s usually only one of a number of repetitive steps involved in the process of hooking up a new or existing project to Git. For example, as part of a typical workflow, you might well:</p>\n<ol>\n<li>Initialise the local repository by running <code>git init</code></li>\n<li>Create a remote repository, for example on Github or Bitbucket; typically by leaving the command-line and firing up a web browser</li>\n<li>Add the remote</li>\n<li>Create a <code>.gitignore</code> file</li>\n<li>Add your project files</li>\n<li>Commit the initial set of files</li>\n<li>Push up to the remote repository</li>\n</ol>\n<p>There are often more steps involved, but we&#x2019;ll stick to those for the purposes of our app. Nevertheless, these steps are pretty repetitive. Wouldn&#x2019;t it be better if we could do all this from the command-line, with no copying-and-pasting of Git URLs and such-like?</p>\n<p>So what ginit will do is create a Git repository in the current folder, create a remote repository&#x2014;we&#x2019;ll be using Github for this&#x2014;and then add it as a remote. Then it will provide a simple interactive &#x201C;wizard&#x201D; for creating a <code>.gitignore</code> file, add the contents of the folder and push it up to the remote repository. It might not save you hours, but it&#x2019;ll remove some of the initial friction when starting a new project.</p>\n<p>With that in mind, let&#x2019;s get started.</p>\n<h2 id=\"theapplicationdependencies\">The Application Dependencies</h2>\n<p>One thing is for certain&#x2014;in terms of appearence, the console will never have the sophistication of a graphical user interface. Nevertheless, that doesn&#x2019;t mean it has to be plain, ugly, monochrome text. You might be surprised by just how much you can do visually, while at the same time keeping it functional. We&#x2019;ll be looking at a couple of libraries for enhancing the display: <a href=\"https://www.npmjs.com/package/chalk\">chalk</a> for colorizing the output and <a href=\"https://www.npmjs.com/package/clui\">clui</a> to add some additional visual components. Just for fun, we&#x2019;ll use <a href=\"https://www.npmjs.com/package/clui\">figlet</a> to create a fancy ASCII-based banner and we&#x2019;ll also use <a href=\"https://www.npmjs.com/package/clear\">clear</a> to clear the console.</p>\n<p>In terms of input and output, the low-level <a href=\"https://nodejs.org/api/readline.html\">Readline</a> Node.js module could be used to prompt the user and request input, and in simple cases is more than adequate. But we&#x2019;re going to take advantage of a third-party package which adds a greater degree of sophistication&#x2014;<a href=\"https://www.npmjs.com/package/inquirer\">Inquirer</a>. As well as providing a mechanism for asking questions, it also implements simple input controls; think radio buttons and checkboxes, but in the console.</p>\n<p>We&#x2019;ll also be using <a href=\"https://www.npmjs.com/package/minimist\">minimist</a> to parse command-line arguments.</p>\n<p>Here&#x2019;s a complete list of the packages we&#x2019;ll use specifically for developing on the command-line:</p>\n<ul>\n<li><a href=\"https://www.npmjs.com/package/chalk\">chalk</a> &#x2013; colorizes the output</li>\n<li><a href=\"https://www.npmjs.com/package/clear\">clear</a> &#x2013; clears the terminal screen</li>\n<li><a href=\"https://www.npmjs.com/package/clui\">clui</a> &#x2013; draws command line tables, gauges and spinners</li>\n<li><a href=\"https://www.npmjs.com/package/figlet\">figlet</a> &#x2013; creates ASCII art from text</li>\n<li><a href=\"https://www.npmjs.com/package/inquirer\">inquirer</a> &#x2013; creates interactive command line user interface</li>\n<li><a href=\"https://www.npmjs.com/package/minimist\">minimist</a> &#x2013; parses argument options</li>\n<li><a href=\"https://www.npmjs.com/package/preferences\">preferences</a> &#x2013; manage CLI application encrypted preferences</li>\n</ul>\n<p>Additionally, we&#x2019;ll also be using the following:</p>\n<ul>\n<li><a href=\"https://www.npmjs.com/package/github\">github</a> &#x2013; Node wrapper for the GitHub API</li>\n<li><a href=\"https://www.npmjs.com/package/lodash\">lodash</a> &#x2013; JavaScript utility library</li>\n<li><a href=\"https://www.npmjs.com/package/simple-git\">simple-git</a> &#x2013; runs Git commands in a Node.js application</li>\n<li><a href=\"https://www.npmjs.com/package/touch\">touch</a> &#x2013; implementation of the *Nix touch command</li>\n</ul>\n<h2 id=\"gettingstarted\">Getting Started</h2>\n<p>Although we&#x2019;re going to create the application from scratch, don&#x2019;t forget that you can also grab a copy of the code from the <a href=\"https://github.com/sitepoint-editors/ginit\">repository which accompanies this article</a>.</p>\n<p>Create a new directory for the project. You don&#x2019;t have to call it <code>ginit</code>, of course.</p>\n<pre><code class=\"bash language-bash\">mkdir ginit\ncd ginit\n</code></pre>\n<p>Create a new <code>package.json</code> file:</p>\n<pre><code class=\"bash language-bash\">npm init\n</code></pre>\n<p>Follow the simple wizard, for example:</p>\n<pre><code>name: (ginit)\nversion: (1.0.0)\ndescription: \"git init\" on steroids\nentry point: (index.js)\ntest command:\ngit repository:\nkeywords: Git CLI\nauthor: [YOUR NAME]\nlicense: (ISC)\n</code></pre>\n<p>Now install the depenencies:</p>\n<pre><code class=\"bash language-bash\">npm install chalk clear clui figlet inquirer minimist preferences github lodash simple-git touch --save\n</code></pre>\n<p>Alternatively, simply copy-and-paste the following <code>package.json</code> file&#x2014;modifying the <code>author</code> appropriately&#x2014;or grab it from the <a href=\"https://github.com/sitepoint-editors/ginit/blob/master/package.json\">repository which accompanies this article</a>:</p>\n<pre><code class=\"javascript language-javascript\">{\n \"name\": \"ginit\",\n \"version\": \"1.0.0\",\n \"description\": \"\\\"git init\\\" on steroids\",\n \"main\": \"index.js\",\n \"keywords\": [\n \"Git\",\n \"CLI\"\n ],\n \"author\": \"Lukas White &lt;hello@lukaswhite.com&gt;\",\n \"license\": \"ISC\",\n \"dependencies\": {\n \"chalk\": \"^1.1.3\",\n \"clear\": \"0.0.1\",\n \"clui\": \"^0.3.1\",\n \"figlet\": \"^1.1.2\",\n \"github\": \"^2.1.0\",\n \"inquirer\": \"^1.1.0\",\n \"lodash\": \"^4.13.1\",\n \"minimist\": \"^1.2.0\",\n \"preferences\": \"^0.2.1\",\n \"simple-git\": \"^1.40.0\",\n \"touch\": \"^1.0.0\"\n }\n}\n</code></pre>\n<p>Now create an <code>index.js</code> file in the same folder and <code>require</code> all of the dependencies:</p>\n<pre><code class=\"javascript language-javascript\">var chalk = require('chalk');\nvar clear = require('clear');\nvar CLI = require('clui');\nvar figlet = require('figlet');\nvar inquirer = require('inquirer');\nvar Preferences = require('preferences');\nvar Spinner = CLI.Spinner;\nvar GitHubApi = require('github');\nvar _ = require('lodash');\nvar git = require('simple-git')();\nvar touch = require('touch');\nvar fs = require('fs');\n</code></pre>\n<p class=\"tip\">Note that the simple-git package exports a function which needs to be called.</p>\n<h2 id=\"addingsomehelpermethods\">Adding Some Helper Methods</h2>\n<p>In the course of the application, we&#x2019;ll need to do the following:</p>\n<ul>\n<li>Get the current directory (to get a default repo name)</li>\n<li>Check whether a directory exists (to determine whether the current folder is already a Git repository by looking for a folder named <code>.git</code>).</li>\n</ul>\n<p>This sounds straight forward, but there are a couple of gotchyas to take into consideration.</p>\n<p>Firstly, you might be tempted to use the <code>fs</code> module&#x2019;s <a href=\"https://nodejs.org/api/fs.html#fs_fs_realpathsync_path_options\">realpathSync</a> method to get the current directory:</p>\n<pre><code class=\"javascript language-javascript\">path.basename(path.dirname(fs.realpathSync(__filename)));\n</code></pre>\n<p>This will work when we are calling the application from the same directory (e.g. using <code>node index.js</code>), but bear in mind that we&#x2019;re going to be making our console application available globally. This means we&#x2019;ll want the name of the directory we&#x2019;re working in, not the directory where the application resides. For this purpose it&#x2019;s better to use <a href=\"https://nodejs.org/api/process.html#process_process_cwd\">process.cwd</a>:</p>\n<pre><code class=\"javascript language-javascript\">path.basename(process.cwd());\n</code></pre>\n<p>Secondly, the preferred method of checking whether a file or directory exists <a href=\"http://stackoverflow.com/a/4482701\">keeps changing</a>&#x2014;the current way is to use <code>fs.stat</code> / <code>fs.statSync</code>. These throw an error if there&#x2019;s no file, so we need to use a <code>try..catch</code> block.</p>\n<p>Finally, it&#x2019;s worth noting that when you&#x2019;re writing a command line application, using the synchronous version of these sorts of methods is just fine.</p>\n<p>Putting that all together, let&#x2019;s create a utilty package in <code>lib/files.js</code>:</p>\n<pre><code class=\"javascript language-javascript\">var fs = require('fs');\nvar path = require('path');\n\nmodule.exports = {\n getCurrentDirectoryBase : function() {\n return path.basename(process.cwd());\n },\n\n directoryExists : function(filePath) {\n try {\n return fs.statSync(filePath).isDirectory();\n } catch (err) {\n return false;\n }\n }\n};\n</code></pre>\n<p>Go back to <code>index.js</code> and ensure you <code>require</code> the new file:</p>\n<pre><code class=\"javascript language-javascript\">var files = require('./lib/files');\n</code></pre>\n<p>With this in place, we can start developing the application.</p>\n<h2 id=\"initializingtheapp\">Initializing the Node CLI</h2>\n<p>Now let&#x2019;s implement the start-up phase of our console application.</p>\n<p>In order to demonstrate some of the packages we&#x2019;ve installed to enhance the console output, let&#x2019;s clear the screen and then display a banner:</p>\n<pre><code class=\"javascript language-javascript\">clear();\nconsole.log(\n chalk.yellow(\n figlet.textSync('Ginit', { horizontalLayout: 'full' })\n )\n);\n</code></pre>\n<p>The output from this is shown below.</p>\n<p><img src=\"https://dab1nmslvvntp.cloudfront.net/wp-content/uploads/2016/07/1467718857Ginit-logo.png\" alt=\"The welcome banner on our Node CLI, created using Chalk and Figlet\"></p>\n<p>Next up, let&#x2019;s run a simple check to ensure that the current folder isn&#x2019;t already a Git repository. That&#x2019;s easy&#x2014;we just check for the existence of a <code>.git</code> folder using the utility method we just created:</p>\n<pre><code class=\"javascript language-javascript\">if (files.directoryExists('.git')) {\n console.log(chalk.red('Already a git repository!'));\n process.exit();\n}\n</code></pre>\n<p class=\"tip\">Notice we&#x2019;re using the <a href=\"https://www.npmjs.com/package/chalk\">chalk module</a> to show a red-colored message.</p>\n<h2 id=\"promptingtheuserforinput\">Prompting the User for Input</h2>\n<p>The next thing we need to do is create a function which will prompt the user for their Github credentials.</p>\n<p>We can use <a href=\"https://www.npmjs.com/package/inquirer\">Inquirer</a> for this. The module includes a number of methods for various types of prompts, which are roughly analogous to HTML form controls. In order to collect the user&#x2019;s Github username and password, we&#x2019;re going to use the <code>input</code> and <code>password</code> types respectively. Here is the code:</p>\n<pre><code class=\"javascript language-javascript\">function getGithubCredentials(callback) {\n var questions = [\n {\n name: 'username',\n type: 'input',\n message: 'Enter your Github username or e-mail address:',\n validate: function( value ) {\n if (value.length) {\n return true;\n } else {\n return 'Please enter your username or e-mail address';\n }\n }\n },\n {\n name: 'password',\n type: 'password',\n message: 'Enter your password:',\n validate: function(value) {\n if (value.length) {\n return true;\n } else {\n return 'Please enter your password';\n }\n }\n }\n ];\n\n inquirer.prompt(questions).then(callback);\n}\n</code></pre>\n<p>As you can see, <code>inquirer.prompt()</code> asks the user a series of questions, provided in the form of an array as the first argument. Each question is made up of an object which defines the <code>name</code> of the field, the <code>type</code> (we&#x2019;re just using <code>input</code> and <code>password</code> respectively here, but later we&#x2019;ll look at a more advanced example), the prompt (<code>message</code>) to display and a validation callback (<code>validate</code>).</p>\n<p>The input the user provides will be passed in to the callback, so we&#x2019;ll end up with simple object with two properties; <code>username</code> and <code>password</code>.</p>\n<p>You can test all of this out by adding the following to <code>index.js</code>:</p>\n<pre><code class=\"javascript language-javascript\">getGithubCredentials(function(){\n console.log(arguments);\n});\n</code></pre>\n<p>And then running the script using <code>node index.js</code>.</p>\n<p><img src=\"https://dab1nmslvvntp.cloudfront.net/wp-content/uploads/2016/07/1467718862Prompting-the-user-for-input.png\" alt=\"Getting user input with Inquirer\"></p>\n<h2 id=\"dealingwithgithubauthentication\">Dealing With GitHub Authentication</h2>\n<p>The next step is to create a function to retrieve an OAuth token for the Github API. Essentially we&#x2019;re going to &#x201C;exchange&#x201D; the username and password for a token.</p>\n<p>Of course we don&#x2019;t want users to have to enter their credentials every time they use the tool; instead, we&#x2019;ll store the OAuth token for subsequent requests. This is where the <a href=\"https://www.npmjs.com/package/preferences\">preferences</a> package comes in.</p>\n<h3 id=\"storingpreferences\">Storing Preferences</h3>\n<p>Storing preferences is outwardly quite straightforward; you can simply read and write to/from a JSON file, without the need for a third-party package. However the preferences package provides a few key advantages:</p>\n<ol>\n<li>It determines the most appropriate location for the file for you, taking into account your operating system and the current user.</li>\n<li>There&#x2019;s no need to explicitly read or write to the file, you simply modify a preferences object and that&#x2019;s taken care of for you in the background.</li>\n<li>The preferences data is encrypted. That&#x2019;s significant in the context of this example, since we&#x2019;re going to be storing sensitive user data.</li>\n</ol>\n<p>To use it, simply create an instance, passing it an application identifier, for example:</p>\n<pre><code class=\"javascript language-javascript\">var prefs = new Preferences('ginit');\n</code></pre>\n<p>If the preferences file doesn&#x2019;t exist, it&#x2019;ll return an empty object and create the file in the background. If there&#x2019;s already a preferences file, the contents will be decoded into JSON and made available to your application. You can now use <code>prefs</code> as a simple object, getting or setting properties as required. As mentioned above, you don&#x2019;t need to worry about saving it afterwards&#x2014;that gets taken care of for you.</p>\n<p class=\"tip\">On Mac OSX/Linux, you&#x2019;ll find the file in <code>/Users/[YOUR-USERNME]/.config/preferences/ginit.pref</code></p>\n<h2 id=\"communicatingwiththegithubapi\">Communicating with the GitHub API</h2>\n<p>Let&#x2019;s create an instance of the Github API. We&#x2019;ll be using this in a couple of places, so we&#x2019;ll make it available to our script globally:</p>\n<pre><code class=\"javascript language-javascript\">var github = new GitHubApi({\n version: '3.0.0'\n});\n</code></pre>\n<p>Now comes the function which checks whether we&#x2019;ve already got an access token:</p>\n<pre><code class=\"javascript language-javascript\">function getGithubToken(callback) {\n var prefs = new Preferences('ginit');\n\n if (prefs.github &amp;&amp; prefs.github.token) {\n return callback(null, prefs.github.token);\n }\n\n // Fetch token\n getGithubCredentials(function(credentials) {\n ...\n });\n}\n</code></pre>\n<p>If a <code>prefs</code> object exists and it has <code>github</code> and <code>github.token</code> properties, this means that there is already a token in storage. In this case the callback function (passed in as an argument) is executed and control returns to the invoking function. We&#x2019;ll get to that later on.</p>\n<p>If no token is detected, we need to fetch one. Of course getting an OAuth token involves a network request, which means a short wait for the user. This gives us an opportunity to look at the <a href=\"https://www.npmjs.com/package/clui\">clui</a> package which provides some enhancements for console-based applications, among them an animated spinner.</p>\n<p>Creating a spinner is easy:</p>\n<pre><code class=\"javascript language-javascript\">var status = new Spinner('Authenticating you, please wait...');\nstatus.start();\n</code></pre>\n<p>Once you&#x2019;re done, simply stop it and it will disappear from the screen:</p>\n<pre><code class=\"javascript language-javascript\">status.stop();\n</code></pre>\n<p class=\"tip\">You can also set the caption dynamically using the <code>update</code> method. This could be useful if you have some indication of progress, for example displaying the percentage complete.</p>\n<p>Here&#x2019;s the code to authenticate with Github:</p>\n<pre><code class=\"javascript language-javascript\">getGithubCredentials(function(credentials) {\n var status = new Spinner('Authenticating you, please wait...');\n status.start();\n\n github.authenticate(\n _.extend(\n {\n type: 'basic',\n },\n credentials\n )\n );\n\n github.authorization.create({\n scopes: ['user', 'public_repo', 'repo', 'repo:status'],\n note: 'ginit, the command-line tool for initalizing Git repos'\n }, function(err, res) {\n status.stop();\n if ( err ) {\n return callback( err );\n }\n if (res.token) {\n prefs.github = {\n token : res.token\n };\n return callback(null, res.token);\n }\n return callback();\n });\n});\n</code></pre>\n<p>Let&#x2019;s step through this:</p>\n<ol>\n<li>We prompt the user for their credentials using the <code>getGithubCredentials</code> method we defined earlier.</li>\n<li>We use <a href=\"https://github.com/mikedeboer/node-github#authentication\">basic authentication</a> prior to trying to obtain an OAuth token.</li>\n<li>We attempt to <a href=\"https://github.com/mikedeboer/node-github#creating-tokens-for-your-application\">create an access token for our application</a>.</li>\n<li>If we manage to get an access token, we set it in the preferences for next time.</li>\n<li>We then return the token.</li>\n</ol>\n<p>Any access tokens you create, whether manually or via the API as we&#x2019;re doing here, you&#x2019;ll be able to <a href=\"https://github.com/settings/tokens\">see them here</a>. During the course of development, you may find you need to delete ginit&#x2019;s access token&#x2014;identifiable by the <code>note</code> parameter supplied above&#x2014;so that you can re-generate it.</p>\n<p class=\"tip\">If you have two-factor authentication enabled on your Github account, the process is slightly more complicated. You&#x2019;ll need to request the confirmation code&#x2014;for example one sent via SMS&#x2014;then supply it using the <code>X-Github-OTP</code> header. See <a href=\"https://developer.github.com/v3/auth/#working-with-two-factor-authentication\">the documentation</a> for further information.</p>\n<p>If you&#x2019;ve been following along and would like to try out what we have so far, ensure that you still have the following to the bottom of <code>index.js</code> and run the code as before:</p>\n<h2 id=\"creatingarepository\">Creating a Repository</h2>\n<p>Once we&#x2019;ve got an OAuth token, we can use it to create a remote repository with Github.</p>\n<p>Again we can use Inquirer to ask a series of questions. We need a name for the repo, we&#x2019;ll ask for an optional description and we also need to know whether it should be public or private.</p>\n<p>We&#x2019;ll use <a href=\"https://www.npmjs.com/package/minimist\">minimist</a> to grab defaults for the name and description from optional command-line arguments. For example:</p>\n<pre><code class=\"bash language-bash\">ginit my-repo \"just a test repository\"\n</code></pre>\n<p>This will set the default name to <code>my-repo</code> and the description to <code>just a test repository</code>.</p>\n<p>The following line will place the arguments in an array indexed by an underscore:</p>\n<pre><code class=\"javascript language-javascript\">var argv = require('minimist')(process.argv.slice(2));\n// { _: [ 'my-repo', 'just a test repository' ] }\n</code></pre>\n<p class=\"tip\">This only really scratches the surface of the minimist package. You can also use it to intepret flags, switches and name/value pairs. Check out the documentation for more information.</p>\n<p>Here&#x2019;s the code, to parse the command line arguments and ask a series of questions:</p>\n<pre><code class=\"javascript language-javascript\">function createRepo(callback) {\n var argv = require('minimist')(process.argv.slice(2));\n\n var questions = [\n {\n type: 'input',\n name: 'name',\n message: 'Enter a name for the repository:',\n default: argv._[0] || files.getCurrentDirectoryBase(),\n validate: function( value ) {\n if (value.length) {\n return true;\n } else {\n return 'Please enter a name for the repository';\n }\n }\n },\n {\n type: 'input',\n name: 'description',\n default: argv._[1] || null,\n message: 'Optionally enter a description of the repository:'\n },\n {\n type: 'list',\n name: 'visibility',\n message: 'Public or private:',\n choices: [ 'public', 'private' ],\n default: 'public'\n }\n ];\n\n inquirer.prompt(questions).then(function(answers) {\n var status = new Spinner('Creating repository...');\n status.start();\n\n var data = {\n name : answers.name,\n description : answers.description,\n private : (answers.visibility === 'private')\n };\n\n github.repos.create(\n data,\n function(err, res) {\n status.stop();\n if (err) {\n return callback(err);\n }\n return callback(null, res.ssh_url);\n }\n );\n });\n}\n</code></pre>\n<p>Once we have that information, we can simply use the github package to <a href=\"https://mikedeboer.github.io/node-github/#api-repos-create\">create a repo</a>, which will give us a URL for the newly created repository. We can then set that up as a remote in our local Git repository. First, however, let&#x2019;s interactively create a <code>.gitignore</code> file.</p>\n<h2 id=\"creatingagitignorefile\">Creating a .gitignore File</h2>\n<p>For the next step, we&#x2019;ll to create a simple command-line &#x201C;wizard&#x201D; to generate a <code>.gitignore</code> file. If the user is running our application in an existing project directory, let&#x2019;s show them a list of files and directories already in the current working directory, and allow them to select which ones to ignore.</p>\n<p>The Inquirer package provides a <code>checkbox</code> input type for just that.</p>\n<p><img src=\"https://dab1nmslvvntp.cloudfront.net/wp-content/uploads/2016/07/1467718853Inquirers-checkboxes-.png\" alt=\"Inquirer's checkboxes in action\"></p>\n<p>The first thing we need to do is scan the current directory, ignoring the <code>.git</code> folder and any existing <code>.gitignore</code> file (we do this by making use of lodash&#x2019;s <a href=\"https://lodash.com/docs#without\">without</a> method):</p>\n<pre><code class=\"javascript language-javascript\">function createGitignore(callback) {\n var filelist = _.without(fs.readdirSync('.'), '.git', '.gitignore');\n ...\n}\n</code></pre>\n<p>If there&#x2019;s nothing to add, there&#x2019;s no point in continuing so let&#x2019;s simply <code>touch</code> the current <code>.gitignore</code> file and bail out of the function:</p>\n<pre><code class=\"javascript language-javascript\">function createGitignore(callback) {\n var filelist = _.without(fs.readdirSync('.'), '.git', '.gitignore');\n\n if (filelist.length) {\n ...\n } else {\n touch( '.gitignore' );\n return callback();\n }\n}\n</code></pre>\n<p>Finally let&#x2019;s utilize Inquirer&#x2019;s checkbox &#x201C;widget&#x201D; to list the files. Once &#x201C;submitted&#x201D; we then generate a <code>.gitignore</code> by joining up the selected list of files, separated with a newline:</p>\n<pre><code class=\"javascript language-javascript\">function createGitignore(callback) {\n var filelist = _.without(fs.readdirSync('.'), '.git', '.gitignore');\n\n if (filelist.length) {\n inquirer.prompt(\n [\n {\n type: 'checkbox',\n name: 'ignore',\n message: 'Select the files and/or folders you wish to ignore:',\n choices: filelist,\n default: ['node_modules', 'bower_components']\n }\n ]\n ).then(function( answers ) {\n if (answers.ignore.length) {\n fs.writeFileSync( '.gitignore', answers.ignore.join( '\\n' ) );\n } else {\n touch( '.gitignore' );\n }\n return callback();\n }\n );\n } else {\n touch('.gitignore');\n return callback();\n }\n}\n</code></pre>\n<p>Notice that we can also provide a list of defaults&#x2014;in this case we&#x2019;re pre-selecting <code>node_modules</code> and <code>bower_components</code>, should they exist.</p>\n<p>Our function now pretty much guarantees we&#x2019;ve got a <code>.gitignore</code> file, so we can proceed with initializing a Git repository.</p>\n<h2 id=\"interactingwithgitfromwithintheapp\">Interacting with Git From Within the App</h2>\n<p>There are a number of ways to interact with Git but perhaps the simplest is to use the <a href=\"https://www.npmjs.com/package/simple-git\">simple-git</a> package. This provides a set of chainable methods which, behind the scenes, run the Git executable.</p>\n<p>These are the repetitive tasks we will use it to automate:</p>\n<ol>\n<li>Run <code>git init</code></li>\n<li>Add the <code>.gitignore</code> file</li>\n<li>Add the remaining contents of the working directory</li>\n<li>Perform an initial commit</li>\n<li>Add the newly-created remote repository</li>\n<li>Push the working directory up to the remote</li>\n</ol>\n<p>Here&#x2019;s the code:</p>\n<pre><code class=\"javascript language-javascript\">function setupRepo( url, callback ) {\n var status = new Spinner('Setting up the repository...');\n status.start();\n\n git\n .init()\n .add('.gitignore')\n .add('./*')\n .commit('Initial commit')\n .addRemote('origin', url)\n .push('origin', 'master')\n .then(function(){\n status.stop();\n return callback();\n });\n}\n</code></pre>\n<h2 id=\"puttingitalltogether\">Putting it all together</h2>\n<p>Finally, we&#x2019;ll create a function to obtain the token and authenticate the user:</p>\n<pre><code class=\"javascript language-javascript\">function githubAuth(callback) {\n getGithubToken(function(err, token) {\n if (err) {\n return callback(err);\n }\n github.authenticate({\n type : 'oauth',\n token : token\n });\n return callback(null, token);\n });\n}\n</code></pre>\n<p>Before moving on to the code which handles the main logic of the app.</p>\n<pre><code class=\"javascript language-javascript\">githubAuth(function(err, authed) {\n if (err) {\n switch (err.code) {\n case 401:\n console.log(chalk.red('Couldn\\'t log you in. Please try again.'));\n break;\n case 422:\n console.log(chalk.red('You already have an access token.'));\n break;\n }\n }\n if (authed) {\n console.log(chalk.green('Sucessfully authenticated!'));\n createRepo(function(err, url){\n if (err) {\n console.log('An error has occured');\n }\n if (url) {\n createGitignore(function() {\n setupRepo(url, function(err) {\n if (!err) {\n console.log(chalk.green('All done!'));\n }\n });\n });\n }\n });\n }\n});\n</code></pre>\n<p>As you can see, it ensures the user is authenticated, before calling all of our other functions (<code>createRepo</code>, <code>createGitignore</code>, <code>setupRepo</code>) sequentially. It also handles any errors and offers the user appropriate feedback.</p>\n<p>You can check out the completed <a href=\"https://github.com/sitepoint-editors/ginit/blob/master/index.js\">index.js</a> file on our GitHub repo.</p>\n<h2 id=\"makingtheginitcommandavailableglobally\">Making the ginit Command Available Globally</h2>\n<p>The one remaining thing to do is to make our command available globally. To do this, we&#x2019;ll need to add a <a href=\"https://en.wikipedia.org/wiki/Shebang_%28Unix%29\">shebang</a> line to the top of <code>index.js</code>:</p>\n<pre><code class=\"javascript language-javascript\">#!/usr/bin/env node\n</code></pre>\n<p>Next we need to add a <code>bin</code> property to our <code>package.json</code> file. This maps the command name (<code>ginit</code>) to the name of the file to be executed (relative to <code>package.json</code>).</p>\n<pre><code class=\"javascript language-javascript\">\"bin\": {\n \"ginit\": \"./index.js\"\n}\n</code></pre>\n<p>After that install the module globally and you will have a working shell command:</p>\n<pre><code class=\"bash language-bash\">npm install -g\n</code></pre>\n<p class=\"tip\">This will also work on Windows, as <a href=\"http://stackoverflow.com/a/10398567/1136887\">npm will helpfully install a cmd wrapper alongside your script</a>.</p>\n<h2 id=\"takingitfurther\">Taking it Further</h2>\n<p>We&#x2019;ve got a fairly nifty, albeit simple command-line app for initialising Git repositories. But there&#x2019;s plenty more you could do to enhance it further.</p>\n<p>If you&#x2019;re a Bitbucket user, you could adapt the program to use the Bitbucket API to create a repository&#x2014;there&#x2019;s a <a href=\"https://www.npmjs.com/package/bitbucket-api\">Node.js API wrapper available</a> to help you get started. You may wish to add an additional command-line option or prompt to ask the user whether they want to use Github or Bitbucket&#x2014;Inquirer would be perfect for just that&#x2014;or merely replace the Github specific code with a Bitbucket alternative.</p>\n<p>You could also provide the facility to specify your own set of defaults for the <code>.gitgnore</code> file, instead of hard-coded list. The preferences package might be suitable here, or you could provide a set of &#x201C;templates&#x201D;&#x2014;perhaps prompting the user for the type of project. You might also want to look at integrating it with the <a href=\"https://www.gitignore.io/\">.gitignore.io</a> command-line tool / API.</p>\n<p>Beyond all that you may also wish to add additional validation, provide the ability to skip certain sections, and more&#x2014;if you have any other ideas, do let me know in the comments.</p>\n </div>\n\n </div>"
    }, {
        "title": "GNU ddrescue Manual",
        "url": "http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html",
        "excerpt": "Next:&#xA0;Introduction, Up:&#xA0;(dir) This manual is for GNU ddrescue (version 1.20, 10 September 2015). Copyright &#xA9; 2004-2015 Antonio Diaz Diaz. This manual is free documentation: you have&hellip;",
        "date_saved": "Sat Jul 30 02:13:23 EDT 2016",
        "html_file": "gnu_ddrescue_manual.html",
        "png_file": "gnu_ddrescue_manual.png",
        "md_file": "gnu_ddrescue_manual.md",
        "content": "<div><body>\n<div class=\"node\">\n<a name=\"Top\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Introduction\">Introduction</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#dir\">(dir)</a>\n\n</div>\n\n\n\n<p>This manual is for GNU ddrescue (version 1.20, 10 September 2015).\n\n</p>\n\n \nCopyright &#xA9; 2004-2015 Antonio Diaz Diaz.\n\n <p>This manual is free documentation: you have unlimited permission\nto copy, distribute and modify it.\n\n</p><div class=\"node\">\n<a name=\"Introduction\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Basic-concepts\">Basic concepts</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">1 Introduction</h2>\n\n<p><a name=\"index-introduction-1\"></a>\nGNU ddrescue is a data recovery tool. It copies data from one file or\nblock device (hard disc, cdrom, etc) to another, trying to rescue the\ngood parts first in case of read errors.\n\n </p><p>The basic operation of ddrescue is fully automatic. That is, you don't\nhave to wait for an error, stop the program, restart it from a new\nposition, etc.\n\n </p><p>If you use the mapfile feature of ddrescue, the data is rescued very\nefficiently, (only the needed blocks are read). Also you can interrupt\nthe rescue at any time and resume it later at the same point. The\nmapfile is an essential part of ddrescue's effectiveness. Use it unless\nyou know what you are doing.\n\n </p><p>Ddrescue does not write zeros to the output when it finds bad sectors in\nthe input, and does not truncate the output file if not asked to. So,\nevery time you run it on the same output file, it tries to fill in the\ngaps without wiping out the data already rescued.\n\n </p><p>Automatic merging of backups: If you have two or more damaged copies of\na file, cdrom, etc, and run ddrescue on all of them, one at a time, with\nthe same output file, you will probably obtain a complete and error-free\nfile. This is so because the probability of having the same area damaged\nin all copies is low (if the errors are randomly located). Using the\nmapfile, only the needed blocks are read from the second and successive\ncopies.\n\n </p><p>Ddrescue recommends lzip for compression of backups because the lzip\nformat is designed for long-term archiving and provides data recovery\ncapabilities which nicely complement those of ddrescue. (Ddrescue fills\nunreadable sectors with data from other copies, while lziprecover\ncorrects corrupt sectors with data from other copies). If the cause of\nfile corruption is damaged media, the combination ddrescue + lziprecover\nis the best option for recovering data from multiple damaged copies. \nSee <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#lziprecover_002dexample\">lziprecover-example</a>, for an example.\n\n </p><p>Because ddrescue needs to read and write at random places, it only works\non seekable (random access) input and output files.\n\n </p><p>If your system supports it, ddrescue can use direct disc access to read\nthe input file, bypassing the kernel cache.\n\n </p><p>Ddrescue also features a \"fill mode\" able to selectively overwrite parts\nof the output file, which has a number of interesting uses like wiping\ndata, marking bad areas or even, in some cases, \"repair\" damaged\nsectors.\n\n </p><p>One of the great strengths of ddrescue is that it is interface-agnostic,\nand so can be used for any kind of device supported by your kernel (ATA,\nSATA, SCSI, old MFM drives, floppy discs, or even flash media cards like\nSD).\n\n</p><div class=\"node\">\n<a name=\"Basic-concepts\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Important-advice\">Important advice</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Introduction\">Introduction</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">2 Basic concepts</h2>\n\n<dl>\n<dt>Block</dt><dd>Any amount of data. A block is described by its starting position and\nits size. The starting position (or beginning position) is the lowest\nposition in the block. The end of the block is its starting position\nplus its size.\n\n <br></dd><dt>Cluster</dt><dd>Group of consecutive sectors read or written in one go.\n\n <br></dd><dt>Device</dt><dd>Piece of hardware containing data. Hard disc drives, cdrom drives, USB\npendrives, are devices. /dev/hda, /dev/sdb, are device names.\n\n <br></dd><dt>File</dt><dd>Files are named units of data which are stored by the operating system\nfor you to retrieve later by name. Devices and partitions are accessed\nby means of their associated file names.\n\n <br></dd><dt>Partition</dt><dd>Every part in which a device is divided. A partition normally contains a\nfile system. /dev/hda1, /dev/sdb3, are partition names.\n\n <br></dd><dt>Recoverable formats</dt><dd>As ddrescue uses standard library functions to read data from the device\nbeing rescued, only mountable device formats can be rescued with\nddrescue. CD-ROMs and DVDs can be mounted and they can be rescued,\n\"compact disc digital audio\" CDs can't, \"video CDs\"[1] maybe.<br> [1]\nhttp://en.wikipedia.org/wiki/Video_CD\n\n <br></dd><dt>Rescue domain</dt><dd>Block or set of blocks to be acted upon (rescued, listed, etc). You can\ndefine it with the options '<samp><p class=\"samp\">--input-position</p></samp>', '<samp><p class=\"samp\">--size</p></samp>' and\n'<samp><p class=\"samp\">--domain-mapfile</p></samp>'. The rescue domain defaults to the whole input\nfile or mapfile. If ddrescue can't determine the size of the input file,\nthe rescue domain defaults to the maximum size of a block (at least\n2^63&#xA0;-&#xA0;1 bytes, or 8 EiB minus 1 byte).\n\n <p>Ddrescue will never try to read any data outside of the rescue domain\nexcept when unaligned direct disc access is requested (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Direct-disc-access\">Direct disc access</a>). If it does, please, report it as a bug.\n\n </p><p>The amount of data rescued, number of errors, etc, shown by ddrescue may\nvary or even become zero if you limit the rescue domain. Don't worry,\nthey have not disappeared; they are simply out of the specified rescue\ndomain.\n\n <br></p></dd><dt>Sector</dt><dd>Hardware block. Smallest accessible amount of data on a device.\n\n </dd></dl>\n\n<div class=\"node\">\n<a name=\"Important-advice\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Algorithm\">Algorithm</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Basic-concepts\">Basic concepts</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">3 Using ddrescue safely</h2>\n\n<p><a name=\"index-using-ddrescue-safely-3\"></a>\nDdrescue is like any other power tool. You need to understand what it\ndoes, and you need to understand some things about the machines it does\nthose things to, in order to use it safely.\n\n </p><p>Never try to rescue a r/w mounted partition. The resulting copy may be\nuseless. It is best that the device or partition to be rescued is not\nmounted at all, not even read-only.\n\n </p><p>Never try to repair a file system on a drive with I/O errors; you will\nprobably lose even more data.\n\n </p><p>If you use a device or a partition as destination, any data stored there\nwill be overwritten.\n\n </p><p>Some systems may change device names on reboot (eg. udev enabled\nsystems). If you reboot, check the device names before restarting\nddrescue.\n\n </p><p>If you interrupt the rescue and then reboot, any partially copied\npartitions should be hidden before allowing them to be touched by any\noperating system that tries to mount and \"fix\" the partitions it sees.\n\n</p><div class=\"node\">\n<a name=\"Algorithm\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Invoking-ddrescue\">Invoking ddrescue</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Important-advice\">Important advice</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">4 Algorithm</h2>\n\n<p><a name=\"index-algorithm-4\"></a>\nGNU ddrescue is not a derivative of dd, nor is related to dd in any way\nexcept in that both can be used for copying data from one device to\nanother. The key difference is that ddrescue uses a sophisticated\nalgorithm to copy data from failing drives causing them as little\nadditional damage as possible.\n\n </p><p>Ddrescue manages efficiently the status of the rescue in progress and\ntries to rescue the good parts first, scheduling reads inside bad (or\nslow) areas for later. This maximizes the amount of data that can be\nfinally recovered from a failing drive.\n\n </p><p>The standard dd utility can be used to save data from a failing drive,\nbut it reads the data sequentially, which may wear out the drive without\nrescuing anything if the errors are at the beginning of the drive.\n\n </p><p>Other programs read the data sequentially but switch to small size reads\nwhen they find errors. This is a bad idea because it means spending more\ntime at error areas, damaging the surface, the heads and the drive\nmechanics, instead of getting out of them as fast as possible. This\nbehavior reduces the chances of rescuing the remaining good data.\n\n </p><p>The algorithm of ddrescue is as follows (the user may interrupt the\nprocess at any point, but be aware that a bad drive can block ddrescue\nfor a long time until the kernel gives up):\n\n </p><p>1) Optionally read a mapfile describing the status of a multi-part or\npreviously interrupted rescue. If no mapfile is specified or is empty or\ndoes not exist, mark all the rescue domain as non-tried.\n\n </p><p>2) (First phase; Copying) Read the non-tried parts of the input file,\nmarking the failed blocks as non-trimmed and skipping beyond them. Skip\nalso beyond slow areas. The skipped areas are tried later in two\nadditional passes (before trimming), reversing the direction after each\npass until all the rescue domain is tried. The third pass is a sweeping\npass, with skipping disabled. (The purpose is to delimit large errors\nfast, keep the mapfile small, and produce good starting points for\ntrimming). Only non-tried areas are read in large blocks. Trimming,\nscraping and retrying are done sector by sector. Each sector is tried at\nmost two times; the first in this step as part of a large block read,\nthe second in one of the steps below as a single sector read.\n\n </p><p>3) (Second phase; Trimming) Trimming is done in one pass. For each\nnon-trimmed block, read forwards one sector at a time from the leading\nedge of the block until a bad sector is found. Then read backwards one\nsector at a time from the trailing edge of the block until a bad sector\nis found. Then mark the bad sectors found (if any) as bad-sector, and\nmark the rest of the block as non-scraped without trying to read it.\n\n </p><p>4) (Third phase; Scraping) Scrape together the data not recovered by the\ncopying or trimming phases. Scraping is done in one pass. Each\nnon-scraped block is read forwards, one sector at a time. Any bad\nsectors found are marked as bad-sector.\n\n </p><p>5) (Fourth phase; Retrying) Optionally try to read again the bad sectors\nuntil the specified number of retry passes is reached. The direction is\nreversed after each pass. Every bad sector is tried only once in each\npass. Ddrescue can't know if a bad sector is unrecoverable or if it will\nbe eventually read after some retries.\n\n </p><p>6) Optionally write a mapfile for later use.\n\n </p>\nThe total error size (errsize) is the sum of the sizes of all the\nbad-sector blocks. It increases during the trimming and scraping phases,\nand may decrease during the retrying phase. Non-trimmed and non-scraped\nblocks are not considered errors. Note that as ddrescue retries the\nfailed blocks, the good data found may divide them into smaller blocks,\ndecreasing the total error size but increasing the number of errors.\n\n <p>The '<samp><span class=\"samp\">remaining time</span></samp>' is calculated using the average rate of the\nlast 60 seconds and does not take into account that some parts may be\nexcluded from the rescue (for example with '<samp><span class=\"samp\">--no-trim</span></samp>'), or that\nsome areas may be unrecoverable. Therefore it may be very imprecise, may\nvary widely during the rescue, and may show a non-zero value at the end\nof the rescue. In particular it may go down to a few seconds at the end\nof the first pass, just to grow to hours or days in the following\npasses. Such is the nature of ddrescue; the good parts are usually\nrecovered fast, while the rest may take a long time.\n\n </p><p>The mapfile is periodically saved to disc, as well as when ddrescue\nfinishes or is interrupted. So in case of a crash you can resume the\nrescue with little recopying. The interval between saves varies from 30\nseconds to 5 minutes depending on mapfile size (larger mapfiles are\nsaved at longer intervals).\n\n </p><p>Also, the same mapfile can be used for multiple commands that copy\ndifferent areas of the input file, and for multiple recovery attempts\nover different subsets. See this example:\n\n</p><p class=\"noindent\">Rescue the most important part of the disc first.\n</p><pre class=\"example\"> ddrescue -i0 -s50MiB /dev/hdc hdimage mapfile\n ddrescue -i0 -s1MiB -d -r3 /dev/hdc hdimage mapfile\n</pre>\n <p class=\"noindent\">Then rescue some key disc areas.\n</p><pre class=\"example\"> ddrescue -i30GiB -s10GiB /dev/hdc hdimage mapfile\n ddrescue -i230GiB -s5GiB /dev/hdc hdimage mapfile\n</pre>\n <p class=\"noindent\">Now rescue the rest (does not recopy what is already done).\n</p><pre class=\"example\"> ddrescue /dev/hdc hdimage mapfile\n ddrescue -d -r3 /dev/hdc hdimage mapfile\n</pre>\n <div class=\"node\">\n<a name=\"Invoking-ddrescue\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Mapfile-structure\">Mapfile structure</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Algorithm\">Algorithm</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">5 Invoking ddrescue</h2>\n\n<p><a name=\"index-invoking-ddrescue-5\"></a><a name=\"index-options-6\"></a><a name=\"index-usage-7\"></a><a name=\"index-version-8\"></a>\nThe format for running ddrescue is:\n\n</p><pre class=\"example\"> ddrescue [<var>options</var>] <var>infile</var> <var>outfile</var> [<var>mapfile</var>]\n</pre>\n <p><var>infile</var> and <var>outfile</var> may be files, devices or partitions. \n<var>mapfile</var> is a regular file and must be placed in an existing\ndirectory. If <var>mapfile</var> does not exist, ddrescue will create it.\n\n </p><p>Always use a mapfile unless you know you won't need it. Without a\nmapfile, ddrescue can't resume a rescue, only reinitiate it.\n\n </p><p>ddrescue supports the following options:\n\n </p><dl>\n<dt><code>-h</code><dt><code>--help</code></dt></dt><dd>Print an informative help message describing the options and exit.\n\n <br></dd><dt><code>-V</code><dt><code>--version</code></dt></dt><dd>Print the version number of ddrescue on the standard output and exit.\n\n <br></dd><dt><code>-a </code><var>bytes</var><dt><code>--min-read-rate=</code><var>bytes</var></dt></dt><dd>Minimum read rate of good non-tried areas, in bytes per second. If the\nread rate falls below this value, ddrescue will skip ahead a variable\namount depending on rate and error histories. The skipped blocks are\ntried in additional passes (before trimming) where the minimum read rate\nis divided by ten before each pass, until there are no more non-tried\nblocks left.\n\n <p>If <var>bytes</var> is 0 (auto), the minimum read rate is recalculated for\neach block as (average_rate&#xA0;/&#xA0;10). Values above device capabilities\nare ignored.\n\n <br></p></dd><dt><code>-A</code><dt><code>--try-again</code></dt></dt><dd>Mark all non-trimmed and non-scraped blocks inside the rescue domain as\nnon-tried before beginning the rescue. Try this if the drive stops\nresponding and ddrescue immediately starts scraping failed blocks when\nrestarted. If '<samp><p class=\"samp\">--retrim</p></samp>' is also specified, mark all failed blocks\ninside the rescue domain as non-tried.\n\n <br></dd><dt><code>-b </code><var>bytes</var><dt><code>--sector-size=</code><var>bytes</var></dt></dt><dd>Sector (hardware block) size of input device in bytes (usually 512 for\nhard discs and 3.5\" floppies, 1024 for 5.25\" floppies, and 2048 for\ncdroms). Defaults to 512.\n\n <br></dd><dt><code>-B</code><dt><code>--binary-prefixes</code></dt></dt><dd>Show units with binary prefixes (powers of 1024).<br>\nSI prefixes (powers of 1000) are used by default. (See table below).\n\n <br></dd><dt><code>-c </code><var>sectors</var><dt><code>--cluster-size=</code><var>sectors</var></dt></dt><dd>Number of sectors to copy at a time. Defaults to 64&#xA0;KiB&#xA0;/&#xA0;sector_size. \nTry smaller values for slow drives. The number of sectors per track (18\nor 9) is a good value for floppies.\n\n <br></dd><dt><code>-C</code><dt><code>--complete-only</code></dt></dt><dd>Limit rescue domain to the blocks listed in the <var>mapfile</var>. Do not\nread new data beyond <var>mapfile</var> limits. This is useful when reading\nfrom devices of undefined size (like raw devices), when the drive\nreturns an incorrect size, or when reading from a partial copy. It can\nonly be used after a first rescue attempt, possibly limited with the\n'<samp><p class=\"samp\">--size</p></samp>' option, has produced a complete <var>mapfile</var>.\n\n <br></dd><dt><code>-d</code><dt><code>--idirect</code></dt></dt><dd>Use direct disc access (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Direct-disc-access\">Direct disc access</a>) to read from\n<var>infile</var>, bypassing the kernel cache. (Opens the file with the\nO_DIRECT flag). Sector size must be correctly set for this to work. Not\nall systems support this.\n\n <p>If your system does not support direct disc access, ddrescue will warn\nyou. If the sector size is not correctly set, all reads will result in\nerrors and no data will be rescued.\n\n <br></p></dd><dt><code>-D</code><dt><code>--odirect</code></dt></dt><dd>Use direct disc access to write to <var>outfile</var>, bypassing the kernel\ncache. (Opens the file with the O_DIRECT flag). Sector size must be\ncorrectly set for this to work. Not all systems support this.\n\n <p>If your system does not support direct disc access, ddrescue will warn\nyou. If the sector size is not correctly set, a write error will result\nand no data will be rescued. Some OSs have a bug that prevents them from\ndetecting write errors properly (or at all) on some devices if direct\ndisc access is not used for <var>outfile</var>.\n\n <br></p></dd><dt><code>-e [+]</code><var>n</var><dt><code>--max-errors=[+]</code><var>n</var></dt></dt><dd>Maximum number of error areas allowed before giving up. Defaults to\ninfinity. If <var>n</var> is preceded by '<samp></samp>' the number refers to new\nerror areas found in this run, not counting those already present in the\n<var>mapfile</var>.\n\n <br></dd><dt><code>-E </code><var>bytes</var><dt><code>--max-error-rate=</code><var>bytes</var></dt></dt><dd>Maximum rate of errors allowed before giving up, in bytes per second. \nDefaults to infinity. The rate being measured is that of actually failed\nreads, so the rescue may finish because of this rate being exceeded even\nif the total error size (errsize) does not change because the areas\nbeing tried are being marked as non-trimmed or non-scraped, or are\nalready marked as errors.\n\n <br></dd><dt><code>-f</code><dt><code>--force</code></dt></dt><dd>Force overwrite of <var>outfile</var>. Needed when <var>outfile</var> is not a\nregular file, but a device or partition. This option is just a safeguard\nto prevent the inadvertent destruction of partitions, and is ignored for\nregular files.\n\n <br></dd><dt><code>-F </code><var>types</var><dt><code>--fill-mode=</code><var>types</var></dt></dt><dd>Fill the blocks in <var>outfile</var> specified as any of <var>types</var> in\n<var>mapfile</var>, with data read from <var>infile</var>. <var>types</var> contains\none or more of the status characters defined in the chapter Mapfile\nstructure (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Mapfile-structure\">Mapfile structure</a>) and an optional '<samp></samp>' for\nsector location data. See the chapter Fill mode (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Fill-mode\">Fill mode</a>) for\na complete description of the fill mode.\n\n <br></dd><dt><code>-G</code><dt><code>--generate-mode</code></dt></dt><dd>Generate an approximate <var>mapfile</var> from the <var>infile</var> and\n<var>outfile</var> of the original rescue run. Note that you must keep the\noriginal offset between '<samp><p class=\"samp\">--input-position</p></samp>' and\n'<samp><p class=\"samp\">--output-position</p></samp>' of the original rescue run. See the chapter\nGenerate mode (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Generate-mode\">Generate mode</a>) for a complete description of the\ngenerate mode.\n\n <br></dd><dt><code>-H </code><var>file</var><dt><code>--test-mode=</code><var>file</var></dt></dt><dd>Builds a map of good/bad blocks using the mapfile <var>file</var> and uses it\nto simulate read errors in <var>infile</var>. The blocks marked as finished\nin <var>file</var> will be read normally. All other block types will be\nconsidered read errors without even trying to read them from\n<var>infile</var>. This mode is an aid in improving the algorithm of ddrescue\nand is also useful to verify that ddrescue produces accurate results in\npresence of read errors. Use '<samp></samp>' as <var>file</var> to read from\nstandard input.\n\n <br></dd><dt><code>-i </code><var>bytes</var><dt><code>--input-position=</code><var>bytes</var></dt></dt><dd>Starting position of the rescue domain in <var>infile</var>, in bytes. \nDefaults to 0. This is not the point from which ddrescue starts copying. \n(For example, if you pass the '<samp><p class=\"samp\">--reverse</p></samp>' option to ddrescue, it\nstarts copying from the end of the rescue domain). In fill mode it\nrefers to a position in the <var>infile</var> of the original rescue run. See\nthe chapter Fill mode (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Fill-mode\">Fill mode</a>) for details.\n\n <br></dd><dt><code>-I</code><dt><code>--verify-input-size</code></dt></dt><dd>Compare the size of <var>infile</var> with the size calculated from the list\nof blocks contained in the <var>mapfile</var>, and exit with status 1 if they\ndiffer. This is not enabled by default because the size of some devices\ncan't be known in advance and because the size derived from the\n<var>mapfile</var> may be incomplete, for example after doing a partial\nrescue.\n\n <br></dd><dt><code>-J</code><dt><code>--verify-on-error</code></dt></dt><dd>After every read error, read again the last good sector found and verify\nthat it returns the same data. Exit with status 2 if the read fails or\nreturns inconsistent data. This option performs one extra read after\neach error, wearing the drive faster. Use it only on drives that stop\nresponding or return garbage data after finding errors. You may need to\npower cycle the drive before restarting ddrescue.\n\n <br></dd><dt><code>-K [</code><var>initial</var><code>][,</code><var>max</var><code>]</code><dt><code>--skip-size=[</code><var>initial</var><code>][,</code><var>max</var><code>]</code></dt></dt><dd>Set limits to skip size during the copying phase. At least one of\n<var>initial</var> or <var>max</var> must be specified. <var>initial</var> is the size\nto skip on the first read error or slow read, in bytes. <var>max</var> is the\nmaximum size to skip. The values given will be rounded to the next\nmultiple of sector size. The skip size will be doubled for each read\nerror or slow read until it reaches <var>max</var> or, if <var>max</var> is\nomitted, 1% of the size of <var>infile</var> or 1 GiB (whichever is smaller),\nand will be reset to <var>initial</var> when good data is found. Valid values\nrange from 64 KiB to 1 GiB. <var>initial</var> defaults to 64 KiB. An\n<var>initial</var> value of 0 disables skipping entirely.\n\n <p>If ddrescue is having difficulties skipping away from a large area with\nscattered errors, or if the device has large error areas at regular\nintervals, you can increase the initial skip size with this option. \nInversely, if ddrescue is skipping too much, leaving large non-tried\nareas behind each error (which will be read later in the usually slower\nbackwards direction), you can reduce the maximum skip size, or disable\nskipping.\n\n </p><p>'<samp><span class=\"samp\">--skip-size</span></samp>' is independent from '<samp><span class=\"samp\">--cluster-size</span></samp>'. The size\nto skip is calculated from the end of the block that just failed.\n\n <br></p></dd><dt><code>-L</code><dt><code>--loose-domain</code></dt></dt><dd>Accept an incomplete synthetic (user fabricated) domain mapfile or\ntest-mode mapfile and fill the gaps in the list of data blocks with\nnon-tried blocks. The blocks in the mapfile must be strictly ascending\nand non-overlapping, but they do not need to be contiguous. This option\nallows making quick edits to a mapfile without all the size calculations\ninvolved in making all data blocks contiguous again.\n\n <br></dd><dt><code>-m </code><var>file</var><dt><code>--domain-mapfile=</code><var>file</var></dt></dt><dd>Restrict the rescue domain to the blocks marked as finished in the\nmapfile <var>file</var>. This is useful for merging partially recovered\nimages of backups, or if the destination drive fails during the rescue. \nUse '<samp></samp>' as <var>file</var> to read from standard input.\n\n <br></dd><dt><code>-M</code><dt><code>--retrim</code></dt></dt><dd>Mark all failed blocks inside the rescue domain as non-trimmed before\nbeginning the rescue. The effect is similar to '<samp><p class=\"samp\">--retry-passes=1</p></samp>',\nbut the bad sectors are tried in a different order, making perhaps\npossible to rescue some of them.\n\n <br></dd><dt><code>-n</code><dt><code>--no-scrape</code></dt></dt><dd>Skip the scraping phase. Avoids spending a lot of time trying to rescue\nthe most difficult parts of the file.\n\n <br></dd><dt><code>-N</code><dt><code>--no-trim</code></dt></dt><dd>Skip the trimming phase. Specially useful in the first parts of a\nmulti-part rescue.\n\n <br></dd><dt><code>-o </code><var>bytes</var><dt><code>--output-position=</code><var>bytes</var></dt></dt><dd>Starting position of the image of the rescue domain in <var>outfile</var>, in\nbytes. Defaults to '<samp><p class=\"samp\">--input-position</p></samp>'. The bytes below <var>bytes</var>\naren't touched if they exist and truncation is not requested. Else they\nare set to 0.\n\n <br></dd><dt><code>-O</code><dt><code>--reopen-on-error</code></dt></dt><dd>Close <var>infile</var> and then reopen it after every read error and, if\n'<samp><p class=\"samp\">--min-read-rate</p></samp>' is set, after every slow read encountered both\nduring the copying phase. Use this option if you notice a permanent drop\nin transfer rate after finding read errors or slow areas. But be warned\nthat most probably the slowing-down is intentionally caused by the\nkernel in an attempt to increase the probability of reading data from\nthe device.\n\n <br></dd><dt><code>-p</code><dt><code>--preallocate</code></dt></dt><dd>Preallocate space on disc for <var>outfile</var>. Only space for regular\nfiles can be preallocated. If preallocation succeeds, rescue will not\nfail due to lack of free space on disc. If ddrescue can't determine the\nsize to preallocate, you may need to specify it with some combination of\nthe '<samp><p class=\"samp\">--input-position</p></samp>', '<samp><p class=\"samp\">--output-position</p></samp>', '<samp><p class=\"samp\">--size</p></samp>',\nand '<samp><p class=\"samp\">--domain-mapfile</p></samp>' options.\n\n <br></dd><dt><code>-P[</code><var>lines</var><code>]</code><dt><code>--data-preview[=</code><var>lines</var><code>]</code></dt></dt><dd>Show <var>lines</var> lines of the latest data read in '<samp><p class=\"samp\">16-byte&#xA0;hex&#xA0;+&#xA0;ASCII</p></samp>' format. Valid values for <var>lines</var> range from 1 to 32. If\n<var>lines</var> is omitted, a default value of 3 is used.\n\n <br></dd><dt><code>-q</code><dt><code>--quiet</code></dt></dt><dd>Quiet operation. Suppress all messages.\n\n <br></dd><dt><code>-r </code><var>n</var><dt><code>--retry-passes=</code><var>n</var></dt></dt><dd>Exit after given number of retry passes. Defaults to 0. -1 means\ninfinity. Every bad sector is tried only once in each pass. To retry bad\nsectors detected on a previous run, you must specify a non-zero number\nof retry passes.\n\n <br></dd><dt><code>-R</code><dt><code>--reverse</code></dt></dt><dd>Reverse the direction of all passes (copying, trimming, scraping and\nretrying). Every pass that is normally run forwards will now be run\nbackwards, and vice versa. '<samp><p class=\"samp\">--reverse</p></samp>' does not modify the size of\nthe blocks copied during each phase, just the order in which they are\ntried.\n\n <br></dd><dt><code>-s </code><var>bytes</var><dt><code>--size=</code><var>bytes</var></dt></dt><dd>Maximum size of the rescue domain, in bytes. It limits the amount of\ninput data to be copied. If ddrescue can't determine the size of the\ninput file, you may need to specify it with this option. Note that this\noption does not specify the size of the resulting <var>outfile</var>. For\nexample, the following command creates an <var>outfile</var> 300 bytes long,\nbut only writes data on the last 200 bytes:\n\n <pre class=\"example\"> ddrescue -i 100 -s 200 infile outfile mapfile\n</pre>\n <br></dd><dt><code>-S</code><dt><code>--sparse</code></dt></dt><dd>Use sparse writes for <var>outfile</var>. (The blocks of zeros are not\nactually allocated on disc). May save a lot of disc space in some cases. \nNot all systems support this. Only regular files can be sparse.\n\n <br></dd><dt><code>-t</code><dt><code>--truncate</code></dt></dt><dd>Truncate <var>outfile</var> to zero size before writing to it. Only works for\nregular files, not for drives or partitions.\n\n <br></dd><dt><code>-T </code><var>interval</var><dt><code>--timeout=</code><var>interval</var></dt></dt><dd>Maximum time since last successful read allowed before giving up. \nDefaults to infinity. <var>interval</var> is a rational number (like 1.5 or\n1/2) optionally followed by one of '<samp></samp>', '<samp></samp>', '<samp></samp>' or\n'<samp></samp>', meaning seconds, minutes, hours and days respectively. If no\nunit is specified, it defaults to seconds.\n\n <br></dd><dt><code>-u</code><dt><code>--unidirectional</code></dt></dt><dd>Run all passes in the same direction. Forwards by default, or backwards\nif the option '<samp><p class=\"samp\">--reverse</p></samp>' is also given.\n\n <br></dd><dt><code>-v</code><dt><code>--verbose</code></dt></dt><dd>Verbose mode. Further -v's (up to 4) increase the verbosity level.\n\n <br></dd><dt><code>-w</code><dt><code>--ignore-write-errors</code></dt></dt><dd>Make fill mode ignore write errors. This is useful to avoid ddrescue\nexiting because of new errors developing while wiping the good sectors\nof a failing drive. Fill mode normally writes to <var>outfile</var> one\ncluster at a time. With this option, after the first write error is\nfound in an area, the rest of that area is filled sector by sector.\n\n <br></dd><dt><code>-x </code><var>bytes</var><dt><code>--extend-outfile=</code><var>bytes</var></dt></dt><dd>Extend the size of <var>outfile</var> to make it at least <var>bytes</var> long. \nIf the size of <var>outfile</var> is already equal or longer than <var>bytes</var>\nthen this option does nothing. Use this option to guarantee a minimum\nsize for <var>outfile</var>. Only regular files can be extended.\n\n <br></dd><dt><code>-X</code><dt><code>--exit-on-error</code></dt></dt><dd>Exit after the first read error is encountered during the copying phase. \nThis is similar but different to '<samp><p class=\"samp\">--timeout=0</p></samp>', which waits until\nthe screen status is refreshed (at least 1 second). If there is at least\none successful read per second, '<samp><p class=\"samp\">--timeout=0</p></samp>' does not make\nddrescue to exit.\n\n <p>This is also similar but different to '<samp><span class=\"samp\">--max-errors=+0</span></samp>', which\nexits when a new error area is found. If the read errors are adjacent to\nexisting error areas, no new error areas are produced (just enlarged),\nand '<samp><span class=\"samp\">--max-errors=+0</span></samp>' does not make ddrescue to exit.\n\n <br></p></dd><dt><code>-y</code><dt><code>--synchronous</code></dt></dt><dd>Use synchronous writes for <var>outfile</var>. (Issue a fsync call after\nevery write). May be useful when forcing the drive to remap its bad\nsectors.\n\n <br></dd><dt><code>-1 </code><var>file</var><dt><code>--log-rates=</code><var>file</var></dt></dt><dd>Log rates and error sizes every second in <var>file</var>. If <var>file</var>\nalready exists, it will be overwritten. Every time the screen is updated\nwith new details, some of those details (time, input position, current\nand average rates, number of errors and error size) are written to\n<var>file</var> in a format usable by plotting utilities like gnuplot. This\nallows a posterior analysis of the drive to see if it has any weak zones\n(areas where the transfer rate drops well below the sustained average).\n\n <br></dd><dt><code>-2 </code><var>file</var><dt><code>--log-reads=</code><var>file</var></dt></dt><dd>Log all read operations in <var>file</var>. If <var>file</var> already exists, it\nwill be overwritten. Every read attempt and its result (position, size,\ncopied size and error size) is written to <var>file</var>. (The position\nwritten is always the beginning of the block tried, even if reading\nbackwards). A line is also written at the beginning of each phase\n(copying, trimming, scraping and retrying). Finally, a line with a time\nmark is written every second (unless the read takes more time). Use this\noption with caution because <var>file</var> may become very large very\nquickly. Use lzip to compress <var>file</var> if you need to store or\ntransmit it.\n\n <br></dd><dt><code>--ask</code></dt><dd>Ask for user confirmation before starting the copy. If the first letter\nof the answer is '<samp></samp>', ddrescue starts copying. Else it exits with\nstatus 1.<br>\nIf they can be obtained, ddrescue shows the model and serial number of\nthe input and output devices. Else it shows the size in bytes of the\ncorresponding file or device.\n\n <br></dd><dt><code>--cpass=</code><var>n</var><code>[,</code><var>n</var><code>]</code></dt><dd>Select what pass(es) to run during the copying phase. Valid values for\n<var>n</var> range from 0 to 3. '<samp><p class=\"samp\">--cpass=0</p></samp>' skips the copying phase\nentirely. To run only the given pass(es), specify also '<samp><p class=\"samp\">--no-trim</p></samp>'\nand '<samp><p class=\"samp\">--no-scrape</p></samp>'.\n\n <br></dd><dt><code>--max-read-rate=</code><var>bytes</var></dt><dd>Maximum read rate, in bytes per second. If <var>bytes</var> is too small, the\nactual read rate is rounded up to the equivalent of a whole number of\ncluster reads per second. Use this option to limit the bandwidth used by\nddrescue, for example when recovering over a network.\n\n <br></dd><dt><code>--pause=</code><var>interval</var></dt><dd>Time to wait between passes. Defaults to 0. <var>interval</var> is formatted\nas in the option '<samp><p class=\"samp\">--timeout</p></samp>' above.\n\n </dd></dl>\n\n <p>Numbers given as arguments to options (positions, sizes, rates, etc) may\nbe expressed as decimal, hexadecimal or octal values (using the same\nsyntax as integer constants in C++), and may be followed by a multiplier\nand an optional '<samp><span class=\"samp\">B</span></samp>' for \"byte\".\n\n </p><p>Table of SI and binary prefixes (unit multipliers):\n\n </p><table summary=\"\"><tr><td valign=\"top\">Prefix </td><td valign=\"top\">Value </td><td valign=\"top\">| </td><td valign=\"top\">Prefix </td><td valign=\"top\">Value\n<br></td></tr><tr><td valign=\"top\"></td><td valign=\"top\"></td><td valign=\"top\">| </td><td valign=\"top\">s </td><td valign=\"top\">sectors\n<br></td></tr><tr><td valign=\"top\">k </td><td valign=\"top\">kilobyte (10^3 = 1000) </td><td valign=\"top\">| </td><td valign=\"top\">Ki </td><td valign=\"top\">kibibyte (2^10 = 1024)\n<br></td></tr><tr><td valign=\"top\">M </td><td valign=\"top\">megabyte (10^6) </td><td valign=\"top\">| </td><td valign=\"top\">Mi </td><td valign=\"top\">mebibyte (2^20)\n<br></td></tr><tr><td valign=\"top\">G </td><td valign=\"top\">gigabyte (10^9) </td><td valign=\"top\">| </td><td valign=\"top\">Gi </td><td valign=\"top\">gibibyte (2^30)\n<br></td></tr><tr><td valign=\"top\">T </td><td valign=\"top\">terabyte (10^12) </td><td valign=\"top\">| </td><td valign=\"top\">Ti </td><td valign=\"top\">tebibyte (2^40)\n<br></td></tr><tr><td valign=\"top\">P </td><td valign=\"top\">petabyte (10^15) </td><td valign=\"top\">| </td><td valign=\"top\">Pi </td><td valign=\"top\">pebibyte (2^50)\n<br></td></tr><tr><td valign=\"top\">E </td><td valign=\"top\">exabyte (10^18) </td><td valign=\"top\">| </td><td valign=\"top\">Ei </td><td valign=\"top\">exbibyte (2^60)\n<br></td></tr><tr><td valign=\"top\">Z </td><td valign=\"top\">zettabyte (10^21) </td><td valign=\"top\">| </td><td valign=\"top\">Zi </td><td valign=\"top\">zebibyte (2^70)\n<br></td></tr><tr><td valign=\"top\">Y </td><td valign=\"top\">yottabyte (10^24) </td><td valign=\"top\">| </td><td valign=\"top\">Yi </td><td valign=\"top\">yobibyte (2^80)\n <br></td></tr></table>\n\n \nExit status: 0 for a normal exit, 1 for environmental problems (file not\nfound, invalid flags, I/O errors, etc), 2 to indicate a corrupt or\ninvalid input file, 3 for an internal consistency error (eg, bug) which\ncaused ddrescue to panic.\n\n <p>If ddrescue is interrupted by a signal, it updates <var>mapfile</var> and\nthen terminates by raising the signal received.\n\n</p><div class=\"node\">\n<a name=\"Mapfile-structure\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Emergency-save\">Emergency save</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Invoking-ddrescue\">Invoking ddrescue</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">6 Mapfile structure</h2>\n\n<p><a name=\"index-mapfile-structure-9\"></a>\nNOTE: In versions of ddrescue prior to 1.20 the mapfile was called\n'<samp><span class=\"samp\">logfile</span></samp>'. The format is the same; only the name has changed.\n\n </p><p>The mapfile is a text file easy to read and edit. It is formed by three\nparts, the heading comments, the status line, and the list of data\nblocks. Any line beginning with '<samp><span class=\"samp\">#</span></samp>' is a comment line.\n\n </p><p>The heading comments contain the version of ddrescue or ddrescuelog that\ncreated the mapfile, the command line used, and the time when the\nprogram started. If the mapfile was created by ddrescue it will also\ncontain the current time when the mapfile was saved and a copy of the\nstatus message from the screen describing the operation being performed\n(copying, trimming, finished, etc). They are intended as information for\nthe user.\n\n </p><p>The first non-comment line is the status line. It contains a\nnon-negative integer and a status character. The integer is the position\nbeing tried in the input file. (The beginning of the block being tried\nin a forward pass or the end of the block in a backward pass). The\nstatus character is one of these:\n\n </p><table summary=\"\"><tr><td valign=\"top\">Character </td><td valign=\"top\">Meaning\n<br></td></tr><tr><td valign=\"top\">'?' </td><td valign=\"top\">copying non-tried blocks\n<br></td></tr><tr><td valign=\"top\">'*' </td><td valign=\"top\">trimming non-trimmed blocks\n<br></td></tr><tr><td valign=\"top\">'/' </td><td valign=\"top\">scraping non-scraped blocks\n<br></td></tr><tr><td valign=\"top\">'-' </td><td valign=\"top\">retrying bad sectors\n<br></td></tr><tr><td valign=\"top\">'F' </td><td valign=\"top\">filling specified blocks\n<br></td></tr><tr><td valign=\"top\">'G' </td><td valign=\"top\">generating approximate mapfile\n<br></td></tr><tr><td valign=\"top\">'+' </td><td valign=\"top\">finished\n <br></td></tr></table>\n\n <p>The blocks in the list of data blocks must be contiguous and\nnon-overlapping.\n\n </p><p>Every line in the list of data blocks describes a block of data. It\ncontains 2 non-negative integers and a status character. The first\ninteger is the starting position of the block in the input file, the\nsecond integer is the size (in bytes) of the block. The status character\nis one of these:\n\n </p><table summary=\"\"><tr><td valign=\"top\">Character </td><td valign=\"top\">Meaning\n<br></td></tr><tr><td valign=\"top\">'?' </td><td valign=\"top\">non-tried block\n<br></td></tr><tr><td valign=\"top\">'*' </td><td valign=\"top\">failed block non-trimmed\n<br></td></tr><tr><td valign=\"top\">'/' </td><td valign=\"top\">failed block non-scraped\n<br></td></tr><tr><td valign=\"top\">'-' </td><td valign=\"top\">failed block bad-sector(s)\n<br></td></tr><tr><td valign=\"top\">'+' </td><td valign=\"top\">finished block\n <br></td></tr></table>\n\n<p class=\"noindent\">And here is an example mapfile:\n\n</p><pre class=\"example\"> # Mapfile. Created by GNU ddrescue version 1.20\n # Command line: ddrescue -d -c18 /dev/fd0 fdimage mapfile\n # Start time: 2015-07-21 09:37:44\n # Current time: 2015-07-21 09:38:19\n # Copying non-tried blocks... Pass 1 (forwards)\n # current_pos current_status\n 0x00120000 ?\n # pos size status\n 0x00000000 0x00117000 +\n 0x00117000 0x00000200 -\n 0x00117200 0x00001000 /\n 0x00118200 0x00007E00 links_data.json package.json 0x00120000 0x00048000 ?\n</pre>\n <p>If you edit the file, you may use decimal, hexadecimal or octal values,\nusing the same syntax as integer constants in C++.\n\n</p><div class=\"node\">\n<a name=\"Emergency-save\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Optical-media\">Optical media</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Mapfile-structure\">Mapfile structure</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">7 Saving the mapfile in case of trouble</h2>\n\n<p><a name=\"index-emergency-save-10\"></a>\nThe mapfile is an essential part of ddrescue's effectiveness. Without a\nmapfile, ddrescue can't resume a rescue, only reinitiate it. Given that\na difficult rescue may take days to complete, it would be a serious\ndrawback if the mapfile were lost because of a solvable problem like a\nlack of space on the device the mapfile is written to.\n\n </p><p>In case of trouble writing the mapfile, ddrescue will print a message\nlike this:\n\n</p><pre class=\"example\"> Error writing mapfile '<var>mapfile</var>': No space left on device\n Fix the problem and press ENTER to retry,\n or E+ENTER for an emergency save and exit,\n or Q+ENTER to abort.\n</pre>\n <p>You may try to fix the problem, for example deleting some files to make\nroom for the mapfile, and press &lt;Return&gt; to retry.\n\n </p><p>If the problem can't be fixed, you may press &lt;e&gt; followed by\n&lt;Return&gt; to try an emergency save and exit. Ddrescue will try to\nwrite the mapfile to the file <samp><span class=\"file\">ddrescue.map</span></samp> in the current\ndirectory or, if this fails, to <samp><span class=\"file\">$HOME/ddrescue.map</span></samp>. If the\nmapfile is written succesfully, ddrescue will exit with status 1. Else\nit will print the above message again.\n\n </p><p>Or you may press &lt;q&gt; followed by &lt;Return&gt; to quit and exit with\nstatus 1. In this case the contents of the mapfile will be lost.\n\n</p><div class=\"node\">\n<a name=\"Optical-media\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Examples\">Examples</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Emergency-save\">Emergency save</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">8 Copying CD-ROMs and DVDs</h2>\n\n<p><a name=\"index-optical-media-11\"></a>\nDdrescue may be better than dd for copying recordable CD-ROMs because\nthe two lead out sectors at the end of some of them may cause a read\nerror that prevents the whole last record from being copied by dd,\npotentially losing data. Also dd may create an image larger than the\noriginal if the '<samp><span class=\"samp\">sync</span></samp>' conversion and a block size larger than the\nsector size are specified.\n\n </p><p>Recordable CD and DVD media keep their data only for a finite time\n(typically for some years). After that time, data loss develops slowly\nwith read errors growing from the outer media region towards the inside. \nJust make two (or more) copies of every important CD-ROM/DVD you burn so\nthat you can later recover them with ddrescue.\n\n </p><p>If you have only one copy of a CD-ROM or DVD that fails when being\ncopied, and if you have access to multiple optical media drives, you\nhave a better chance of recovering the bad sectors since one drive may\nfail to read a particular sector, but another drive might be able to\nsqueeze the data out of it, depending on the laser frequency and the\nsensitivity of the laser-sensor that reads the reflected laser light.\n\n </p>\nExample 1: Rescue a CD-ROM in /dev/cdrom.\n\n<pre class=\"example\"> ddrescue -n -b2048 /dev/cdrom cdimage mapfile\n ddrescue -d -r1 -b2048 /dev/cdrom cdimage mapfile\n (if errsize is zero, cdimage now contains a complete image of the\n CD-ROM and you can write it to a blank CD-ROM)\n</pre>\n \nExample 2: Rescue a CD-ROM in /dev/cdrom from two copies.\n\n<pre class=\"example\"> ddrescue -n -b2048 /dev/cdrom cdimage mapfile\n ddrescue -d -b2048 /dev/cdrom cdimage mapfile\n (insert second copy in the CD drive)\n ddrescue -d -r1 -b2048 /dev/cdrom cdimage mapfile\n (if errsize is zero, cdimage now contains a complete image of the\n CD-ROM and you can write it to a blank CD-ROM)\n</pre>\n \nExample 3: Rescue a CD-ROM in /dev/cdrom using two CD drives from two\ndifferent computers, writing the image into an USB drive nounted on\n/mnt/mem.\n\n<pre class=\"example\"> ddrescue -n -b2048 /dev/cdrom /mnt/mem/cdimage /mnt/mem/mapfile\n ddrescue -d -r1 -b2048 /dev/cdrom /mnt/mem/cdimage /mnt/mem/mapfile\n (umount the USB drive and move both USB drive and CD-ROM to second\n computer)\n ddrescue -d -r1 -b2048 /dev/cdrom /mnt/mem/cdimage /mnt/mem/mapfile\n (if errsize is zero, /mnt/mem/cdimage now contains a complete image\n of the CD-ROM and you can write it to a blank CD-ROM)\n</pre>\n \nExample 4: Merge the partially recovered images of 3 identical DVDs\nusing their mapfiles as domain mapfiles.\n\n<pre class=\"example\"> ddrescue -m mapfile1 dvdimage1 dvdimage mapfile\n ddrescue -m mapfile2 dvdimage2 dvdimage mapfile\n ddrescue -m mapfile3 dvdimage3 dvdimage mapfile\n (if errsize is zero, dvdimage now contains a complete image of the\n DVD and you can write it to a blank DVD)\n</pre>\n \n<a name=\"lziprecover_002dexample\"></a>Example 5: Rescue a lzip compressed backup from two copies on CD-ROM\nwith error-checked merging of copies. \n(See the\n<a href=\"http://www.nongnu.org/lzip/manual/lziprecover_manual.html\">lziprecover manual</a>\nfor details about lziprecover).\n\n<pre class=\"example\"> ddrescue -d -r1 -b2048 /dev/cdrom cdimage1 mapfile1\n mount -t iso9660 -o loop,ro cdimage1 /mnt/cdimage\n cp /mnt/cdimage/backup.tar.lz rescued1.tar.lz\n umount /mnt/cdimage\n (insert second copy in the CD drive)\n ddrescue -d -r1 -b2048 /dev/cdrom cdimage2 mapfile2\n mount -t iso9660 -o loop,ro cdimage2 /mnt/cdimage\n cp /mnt/cdimage/backup.tar.lz rescued2.tar.lz\n umount /mnt/cdimage\n lziprecover -m -v -o backup.tar.lz rescued1.tar.lz rescued2.tar.lz\n Input files merged successfully.\n lziprecover -tv backup.tar.lz\n backup.tar.lz: ok\n</pre>\n <div class=\"node\">\n<a name=\"Examples\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Direct-disc-access\">Direct disc access</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Optical-media\">Optical media</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">9 A small tutorial with examples</h2>\n\n<p><a name=\"index-examples-12\"></a>\nThis tutorial is for those already able to use the dd command. If you\ndon't know what dd is, better search the net for some introductory\nmaterial about dd and GNU ddrescue first.\n\n </p><p>A failing drive tends to develop more and more errors as time passes. \nBecause of this, you should rescue the data from a drive as soon as you\nnotice the first error. Be diligent because every time a physically\ndamaged drive powers up and is able to output some data, it may be the\nvery last time that it ever will.\n\n </p><p>You should make a copy of the failing drive with ddrescue, and then try\nto repair the copy. If your data is really important, use the first copy\nas a master for a second copy, and try to repair the second copy. If\nsomething goes wrong, you have the master intact to try again.\n\n </p><p>If you are trying to rescue a whole partition, first repair the copy\nwith e2fsck or some other tool appropriate for the type of partition you\nare trying to rescue, then mount the repaired copy somewhere and try to\nrecover the files in it.\n\n </p><p>If the drive is so damaged that the file system in the rescued partition\ncan't be repaired or mounted, you will have to browse the rescued data\nwith an hex editor and extract the desired parts by hand or use a file\nrecovery tool like photorec.\n\n </p><p>If the partition table is damaged, you may try to rescue the whole disc,\nthen try to repair the partition table and the partitions on the copy.\n\n </p><p>If the damaged drive is not listed in /dev, then you cannot rescue it. \nAt least not with ddrescue.\n\n </p><p>See <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Optical-media\">Optical media</a>, for rescue examples of CD-ROMs and DVDs.\n\n </p>\nExample 1: Rescue a whole disc with two ext2 partitions in /dev/hda to\n/dev/hdb.<br>\nNote: you do not need to partition /dev/hdb beforehand, but if the\npartition table on /dev/hda is damaged, you'll need to recreate it\nsomehow on /dev/hdb.\n\n<pre class=\"example\"> ddrescue -f -n /dev/hda /dev/hdb mapfile\n ddrescue -d -f -r3 /dev/hda /dev/hdb mapfile\n fdisk /dev/hdb\n e2fsck -v -f /dev/hdb1\n e2fsck -v -f /dev/hdb2\n</pre>\n \nExample 2: Rescue an ext2 partition in /dev/hda2 to /dev/hdb2.<br>\nNote: you need to create the hdb2 partition with fdisk first. hdb2\nshould be of appropriate type and size.\n\n<pre class=\"example\"> ddrescue -f -n /dev/hda2 /dev/hdb2 mapfile\n ddrescue -d -f -r3 /dev/hda2 /dev/hdb2 mapfile\n e2fsck -v -f /dev/hdb2\n mount -t ext2 -o ro /dev/hdb2 /mnt\n (read rescued files from /mnt)\n</pre>\n \nExample 3: While rescuing the whole drive /dev/hda to /dev/hdb, /dev/hda\nfreezes up at position 12345678.\n\n<pre class=\"example\"> ddrescue -f /dev/hda /dev/hdb mapfile &lt;-- /dev/hda freezes here\n (restart /dev/hda or reboot computer)\n (restart copy at a safe distance from the troubled sector)\n ddrescue -f -i 12350000 /dev/hda /dev/hdb mapfile\n (then copy backwards down to the troubled sector)\n ddrescue -f -R /dev/hda /dev/hdb mapfile\n</pre>\n \nExample 4: While rescuing the whole drive /dev/hda to /dev/hdb, /dev/hdb\nfails and you have to rescue data to a third drive, /dev/hdc.\n\n<pre class=\"example\"> ddrescue -f -n /dev/hda /dev/hdb mapfile1 &lt;-- /dev/hdb fails here\n ddrescue -f -m mapfile1 /dev/hdb /dev/hdc mapfile2\n ddrescue -f -n /dev/hda /dev/hdc mapfile2\n ddrescue -d -f -r3 /dev/hda /dev/hdc mapfile2\n</pre>\n \nExample 5: While rescuing the whole drive /dev/hda to /dev/hdb, /dev/hda\nstops responding and disappears from /dev.\n\n<pre class=\"example\"> ddrescue -f -n /dev/hda /dev/hdb mapfile &lt;-- /dev/hda fails here\n (restart /dev/hda or reboot computer as many times as needed)\n ddrescue -f -n -A /dev/hda /dev/hdb mapfile\n ddrescue -d -f -r3 /dev/hda /dev/hdb mapfile\n</pre>\n <div class=\"node\">\n<a name=\"Direct-disc-access\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Fill-mode\">Fill mode</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Examples\">Examples</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">10 Direct disc access</h2>\n\n<p><a name=\"index-direct-disc-access-13\"></a><a name=\"index-raw-devices-14\"></a>\nIf you notice that the positions and sizes in <var>mapfile</var> are always\nmultiples of the sector size, maybe your kernel is caching the disc\naccesses and grouping them. In this case you may want to use direct disc\naccess for <var>infile</var>, or read from a raw device, to bypass the kernel\ncache and rescue more of your data.\n\n </p><p>NOTE! Sector size must be correctly set with the '<samp><span class=\"samp\">--sector-size</span></samp>'\noption for direct disc access to work.\n\n </p><p>NOTE: Direct disc access can copy arbitrary domains by reading whole\nsectors and then writing only the requested part. This is the only case\nwhere ddrescue will try to read data outside of the rescue domain.\n\n </p><p>Try the '<samp><span class=\"samp\">--idirect</span></samp>' option first. If direct disc access is not\navailable in your system, try raw devices. Read your system\ndocumentation to find how to bind a raw device to a regular block\ndevice. Some OSs provide raw access through special device names, like\n/dev/rdisk.\n\n </p><p>Ddrescue aligns its I/O buffer to the sector size so that it can be used\nfor direct disc access or to read from raw devices. For efficiency\nreasons, also aligns it to the memory page size if page size is a\nmultiple of sector size. On some systems, ddrescue can't determine the\nsize of a raw device, so an explicit '<samp><span class=\"samp\">--size</span></samp>' or\n'<samp><span class=\"samp\">--complete-only</span></samp>' option may be needed.\n\n </p><p>Using direct disc access, or reading from a raw device, may be slower or\nfaster than normal cached reading depending on your OS and hardware. In\ncase it is slower you may want to make a first pass using normal cached\nreads and use direct disc access, or a raw device, only to recover the\ngood sectors inside the failed blocks.\n\n </p>\nExample 1: using direct disc access.\n\n<pre class=\"example\"> ddrescue -f -n /dev/hdb1 /dev/hdc1 mapfile\n ddrescue -d -f -r3 /dev/hdb1 /dev/hdc1 mapfile\n e2fsck -v -f /dev/hdc1\n mount -t ext2 -o ro /dev/hdc1 /mnt\n</pre>\n \nExample 2: using a raw device.\n\n<pre class=\"example\"> raw /dev/raw/raw1 /dev/hdb1\n ddrescue -f -n /dev/hdb1 /dev/hdc1 mapfile\n ddrescue -C -f -r3 /dev/raw/raw1 /dev/hdc1 mapfile\n raw /dev/raw/raw1 0 0\n e2fsck -v -f /dev/hdc1\n mount -t ext2 -o ro /dev/hdc1 /mnt\n</pre>\n <div class=\"node\">\n<a name=\"Fill-mode\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Generate-mode\">Generate mode</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Direct-disc-access\">Direct disc access</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">11 Fill mode</h2>\n\n<p><a name=\"index-fill-Mode-15\"></a>\nWhen ddrescue is invoked with the '<samp><span class=\"samp\">--fill-mode</span></samp>' option it operates\nin \"fill mode\", which is different from the default \"rescue mode\". That\nis, if you use the '<samp><span class=\"samp\">--fill-mode</span></samp>' option, ddrescue does not rescue\nanything. It only fills with data read from <var>infile</var> the blocks of\n<var>outfile</var> whose status character from <var>mapfile</var> coincides with\none of the type characters specified as argument to the\n'<samp><span class=\"samp\">--fill-mode</span></samp>' option.\n\n </p><p>If the argument of the '<samp><span class=\"samp\">--fill-mode</span></samp>' option contains an '<samp><span class=\"samp\">l</span></samp>',\nddrescue will write location data (position, sector number and status)\ninto each sector filled. With bad sectors filled in this way, it should\nbe possible to retry the recovery of important files, as location of the\nerror is known by looking into the unfinished copy of the file.\n\n </p><p>In fill mode <var>infile</var> may have any size. If it is too small, the\ndata will be duplicated as many times as necessary to fill the input\nbuffer. If it is too big, only the data needed to fill the input buffer\nwill be read. Then the same data will be written to every cluster or\nsector to be filled.\n\n </p><p>Note that in fill mode <var>infile</var> is always read from position 0. If\nyou specify a '<samp><span class=\"samp\">--input-position</span></samp>', it refers to the original\n<var>infile</var> from which <var>mapfile</var> was built, and is only used to\ncalculate the offset between input and output positions.\n\n </p><p>Note also that when filling the <var>infile</var> of the original rescue run\nyou should not set '<samp><span class=\"samp\">--output-position</span></samp>', whereas when filling the\n<var>outfile</var> of the original rescue run you should keep the original\noffset between '<samp><span class=\"samp\">--input-position</span></samp>' and '<samp><span class=\"samp\">--output-position</span></samp>'.\n\n </p><p>The '<samp><span class=\"samp\">--fill-mode</span></samp>' option implies the '<samp><span class=\"samp\">--complete-only</span></samp>' option.\n\n </p><p>In fill mode <var>mapfile</var> is updated to allow resumability when\ninterrupted or in case of a crash, but as nothing is being rescued\n<var>mapfile</var> is not destroyed. The status line is the only part of\n<var>mapfile</var> that is modified.\n\n </p>\nThe fill mode has a number of uses. See the following examples:\n\n<p class=\"noindent\">Example 1: Mark parts of the rescued copy to allow finding them when\nexamined in an hex editor. For example, the following command line fills\nall blocks marked as '<samp><span class=\"samp\">-</span></samp>' (bad-sector) with copies of the string\n'<samp><span class=\"samp\">BAD&#xA0;SECTOR&#xA0;</span></samp>':\n\n</p><pre class=\"example\"> printf \"BAD SECTOR \" &gt; tmpfile\n ddrescue --fill-mode=- tmpfile outfile mapfile\n</pre>\n <p class=\"noindent\">Example 2: Wipe only the good sectors, leaving the bad sectors alone. \nThis way, the drive will still test bad (i.e., with unreadable sectors). \nThis is the fastest way of wiping a failing drive, and is specially\nuseful when sending the drive back to the manufacturer for warranty\nreplacement.\n\n</p><pre class=\"example\"> ddrescue --fill-mode=+ --force /dev/zero bad_drive mapfile\n</pre>\n <p class=\"noindent\">Example 3: Force the drive to remap the bad sectors, making it usable\nagain. If the drive has only a few bad sectors, and they are not caused\nby drive age, you can probably just rewrite those sectors, and the drive\nwill reallocate them automatically to new \"spare\" sectors that it keeps\nfor just this purpose. WARNING! This may not work on your drive.\n\n</p><pre class=\"example\"> ddrescue --fill-mode=- -f --synchronous /dev/zero bad_drive mapfile\n</pre>\n \nFill mode can also help you to figure out, independently of the file\nsystem used, what files are partially or entirely in the bad areas of\nthe disc. Just follow these steps:\n\n <p>1) Copy the damaged drive with ddrescue until finished. Do not use\nsparse writes. This yields a mapfile with only finished ('<samp><span class=\"samp\">+</span></samp>') and\nbad-sector ('<samp><span class=\"samp\">-</span></samp>') blocks.\n\n </p><p>2) Fill the bad-sector blocks of the copied drive or image file with a\nstring not present in any file, for example \"DEADBEEF\". Use\n'<samp><span class=\"samp\">--fill-mode=l-</span></samp>' if you want location data.\n\n </p><p>3) Mount the copied drive (or the image file, via loopback device)\nread-only.\n\n </p><p>4) Grep for the fill string in all the files. Those files containing the\nstring reside (at least partially) in damaged disc areas. Note that if\nall the damaged areas are in unused space, grep will not find the string\nin any file, which means that no files are damaged.\n\n </p><p>5) Take note of the location data of any important files that you want\nto retry.\n\n </p><p>6) Unmount the copied drive or image file.\n\n </p><p>7) Retry the sectors belonging to the important files until they are\nrescued or until it is clear that they can't be rescued.\n\n </p><p>8) Optionally fill the bad-sector blocks of the copied drive or image\nfile with zeros to restore the disc image.\n\n</p><p class=\"noindent\">Example 4: Figure out what files are in the bad areas of the disc.\n\n</p><pre class=\"example\"> ddrescue -b2048 /dev/cdrom cdimage mapfile\n printf \"DEADBEEF\" &gt; tmpfile\n ddrescue --fill-mode=l- tmpfile cdimage mapfile\n rm tmpfile\n mount -t iso9660 -o loop,ro cdimage /mnt/cdimage\n find /mnt/cdimage -type f -exec grep -l \"DEADBEEF\" '{}' ';'\n (note that my_thesis.txt has a bad sector at pos 0x12345000)\n umount /mnt/cdimage\n ddrescue -b2048 -i0x12345000 -s2048 -dr9 /dev/cdrom cdimage mapfile\n ddrescue --fill-mode=- /dev/zero cdimage mapfile\n mount -t iso9660 -o loop,ro cdimage /mnt/cdimage\n cp -a /mnt/cdimage/my_thesis.txt /safe/place/my_thesis.txt\n</pre>\n <div class=\"node\">\n<a name=\"Generate-mode\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Ddrescuelog\">Ddrescuelog</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Fill-mode\">Fill mode</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">12 Generate mode</h2>\n\n<p><a name=\"index-generate-Mode-16\"></a>\nWhen ddrescue is invoked with the '<samp><span class=\"samp\">--generate-mode</span></samp>' option it\noperates in \"generate mode\", which is different from the default \"rescue\nmode\". That is, if you use the '<samp><span class=\"samp\">--generate-mode</span></samp>' option, ddrescue\ndoes not rescue anything. It only tries to generate a <var>mapfile</var> for\nlater use.\n\n </p><p>So you didn't read the manual and started ddrescue without a\n<var>mapfile</var>. Now, two days later, your computer crashed and you can't\nknow how much data ddrescue managed to save. And even worse, you can't\nresume the rescue; you have to restart it from the very beginning.\n\n </p><p>Or maybe you started copying a drive with <code>dd&#xA0;conv=noerror,sync</code> and are now in the same situation described above. \nIn this case, note that you can't use a copy made by dd unless it was\ninvoked with the '<samp><span class=\"samp\">sync</span></samp>' conversion argument.\n\n </p><p>Don't despair (yet). Ddrescue can in some cases generate an approximate\n<var>mapfile</var>, from <var>infile</var> and the (partial) copy in\n<var>outfile</var>, that is almost as good as an exact <var>mapfile</var>. It\nmakes this by simply assuming that sectors containing all zeros were not\nrescued.\n\n </p><p>However, if the destination of the copy was a drive or a partition, (or\nan existing regular file and truncation was not requested), most\nprobably you will need to restart ddrescue from the very beginning. \n(This time with a <var>mapfile</var>, of course). The reason is that old data\nmay be present in the drive that have not been overwritten yet, and may\nbe thus non-tried but non-zero.\n\n </p><p>For example, if you first tried one of these commands:\n</p><pre class=\"example\"> ddrescue infile outfile\n or\n dd if=infile of=outfile conv=noerror,sync\n</pre>\n <p>then you can generate an approximate mapfile with this command:\n</p><pre class=\"example\"> ddrescue --generate-mode infile outfile mapfile\n</pre>\n <p class=\"noindent\">Note that you must keep the original offset between\n'<samp><span class=\"samp\">--input-position</span></samp>' and '<samp><span class=\"samp\">--output-position</span></samp>' of the original\nrescue run.\n\n</p><div class=\"node\">\n<a name=\"Ddrescuelog\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Invoking-ddrescuelog\">Invoking ddrescuelog</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Generate-mode\">Generate mode</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">13 Ddrescuelog</h2>\n\n<p><a name=\"index-ddrescuelog-17\"></a>\nDdrescuelog is a tool that manipulates ddrescue mapfiles, shows mapfile\ncontents, converts mapfiles to/from other formats, compares mapfiles,\ntests rescue status, and can delete a mapfile if the rescue is done. \nDdrescuelog operations can be restricted to one or several parts of the\nmapfile if the domain setting options are used.\n\n </p><p>When performing logic operations (AND, OR, XOR) on mapfiles of different\nextension, only the blocks present in both files are processed.\n\n </p><p>Here are some examples of how to use ddrescuelog, alone or in\ncombination with other tools.\n\n </p>\nExample 1: Delete the mapfile if the rescue is finished (all data have\nbeen recovered without errors left).\n\n<pre class=\"example\"> ddrescue -f /dev/hda /dev/hdb mapfile\n ddrescuelog -d mapfile\n</pre>\n \nExample 2: Rescue two ext2 partitions in /dev/hda to\n/dev/hdb and repair the file systems using badblock lists generated with\nddrescuelog. File system block size is 4096.<br>\nNote: you do need to partition /dev/hdb beforehand.\n\n<pre class=\"example\"> fdisk /dev/hdb &lt;-- partition /deb/hdb\n ddrescue -f /dev/hda1 /dev/hdb1 mapfile1\n ddrescue -f /dev/hda2 /dev/hdb2 mapfile2\n ddrescuelog -l- -b4096 mapfile1 &gt; badblocks1\n ddrescuelog -l- -b4096 mapfile2 &gt; badblocks2\n e2fsck -v -f -L badblocks1 /dev/hdb1\n e2fsck -v -f -L badblocks2 /dev/hdb2\n</pre>\n \nExample 3: Rescue a whole disc with two ext2 partitions in /dev/hda to\n/dev/hdb and repair the file systems using badblock lists generated with\nddrescuelog. Disc sector size is 512, file system block size is 4096. \nArguments to options '<samp></samp>' and '<samp></samp>' are the starting positions\nand sizes of the partitions being rescued.<br>\nNote: you do not need to partition /dev/hdb beforehand, but if the\npartition table on /dev/hda is damaged, you'll need to recreate it\nsomehow on /dev/hdb.\n\n<pre class=\"example\"> ddrescue -f /dev/hda /dev/hdb mapfile\n fdisk /dev/hdb &lt;-- get partition sizes\n ddrescuelog -l- -b512 -i63s -o0 -s767457s -b4096 mapfile &gt; badblocks1\n ddrescuelog -l- -b512 -i767520s -o0 -s96520s -b4096 mapfile &gt; badblocks2\n e2fsck -v -f -L badblocks1 /dev/hdb1\n e2fsck -v -f -L badblocks2 /dev/hdb2\n</pre>\n <div class=\"node\">\n<a name=\"Invoking-ddrescuelog\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Problems\">Problems</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Ddrescuelog\">Ddrescuelog</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">14 Invoking ddrescuelog</h2>\n\n<p><a name=\"index-invoking-ddrescuelog-18\"></a>\nThe format for running ddrescuelog is:\n\n</p><pre class=\"example\"> ddrescuelog [<var>options</var>] <var>mapfile</var>\n</pre>\n <p>Use '<samp><span class=\"samp\">-</span></samp>' as <var>mapfile</var> to read the mapfile from standard input or\nto write the mapfile created by '<samp><span class=\"samp\">--create-mapfile</span></samp>' to standard\noutput.\n\n </p><p>Ddrescuelog supports the following options:\n\n </p><dl>\n<dt><code>-h</code><dt><code>--help</code></dt></dt><dd>Print an informative help message describing the options and exit.\n\n <br></dd><dt><code>-V</code><dt><code>--version</code></dt></dt><dd>Print the version number of ddrescuelog on the standard output and exit.\n\n <br></dd><dt><code>-a </code><var>old_types</var><code>,</code><var>new_types</var><dt><code>--change-types=</code><var>old_types</var><code>,</code><var>new_types</var></dt></dt><dd>Change the status of every block in the rescue domain from one type in\n<var>old_types</var> to the corresponding type in <var>new_types</var>, much like\nthe '<samp></samp>' command does, and write the resulting mapfile to standard\noutput. <var>old_types</var> and <var>new_types</var> are strings of block status\ncharacters as defined in the chapter Mapfile structure (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Mapfile-structure\">Mapfile structure</a>). Blocks whose status is not in <var>old_types</var> are left\nunchanged. If <var>new_types</var> is shorter than <var>old_types</var> the last\ntype of <var>new_types</var> is repeated as many times as necessary.\n\n <br></dd><dt><code>-b </code><var>bytes</var><dt><code>--block-size=</code><var>bytes</var></dt></dt><dd>Block size used by ddrescuelog. Depending on the requested operation it\nmay be the sector size of the input device, the block size of the\nrescued file system, etc. Defaults to 512.\n\n <br></dd><dt><code>-B</code><dt><code>--binary-prefixes</code></dt></dt><dd>Show units with binary prefixes (powers of 1024).<br>\nSI prefixes (powers of 1000) are used by default. (See table above,\n<a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Invoking-ddrescue\">Invoking ddrescue</a>).\n\n <br></dd><dt><code>-c[</code><var>type1</var><var>type2</var><code>]</code><dt><code>--create-mapfile[=</code><var>type1</var><var>type2</var><code>]</code></dt></dt><dd>Create a <var>mapfile</var> from a list of block numbers read from standard\ninput. Only blocks included in the rescue domain will be added to\n<var>mapfile</var>.\n\n <p><var>type1</var> and <var>type2</var> are block status characters as defined in\nthe chapter Mapfile structure (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Mapfile-structure\">Mapfile structure</a>). <var>type1</var>\nsets the type for blocks included in the list, while <var>type2</var> sets\nthe type for the rest of <var>mapfile</var>. If not specified, <var>type1</var>\ndefaults to '<samp><span class=\"samp\">+</span></samp>' and <var>type2</var> defaults to '<samp><span class=\"samp\">-</span></samp>'.\n\n <br></p></dd><dt><code>-C[</code><var>type</var><code>]</code><dt><code>--complete-mapfile[=</code><var>type</var><code>]</code></dt></dt><dd>Complete a synthetic (user fabricated) <var>mapfile</var> by filling the gaps\nwith blocks of type <var>type</var>, and write the completed mapfile to\nstandard output. <var>type</var> is one of the block status characters\ndefined in the chapter Mapfile structure (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Mapfile-structure\">Mapfile structure</a>). If\n<var>type</var> is not specified, the gaps are filled with non-tried blocks. \nAll gaps in <var>mapfile</var> are filled. Domain options are ignored.\n\n <br></dd><dt><code>-d</code><dt><code>--delete-if-done</code></dt></dt><dd>Delete the given <var>mapfile</var> if all the blocks in the rescue domain\nhave been successfully recovered. The exit status is 0 if <var>mapfile</var>\ncould be deleted, 1 otherwise.\n\n <br></dd><dt><code>-D</code><dt><code>--done-status</code></dt></dt><dd>Test if all the blocks in the rescue domain have been successfully\nrecovered. The exit status is 0 if all tested blocks are finished, 1\notherwise.\n\n <br></dd><dt><code>-f</code><dt><code>--force</code></dt></dt><dd>Force overwrite of <var>mapfile</var>.\n\n <br></dd><dt><code>-i </code><var>bytes</var><dt><code>--input-position=</code><var>bytes</var></dt></dt><dd>Starting position of the rescue domain, in bytes. Defaults to 0. It\nrefers to a position in the original <var>infile</var>.\n\n <br></dd><dt><code>-l </code><var>types</var><dt><code>--list-blocks=</code><var>types</var></dt></dt><dd>Print on standard output the block numbers of the blocks specified as\nany of <var>types</var> in <var>mapfile</var> and included in the rescue domain. \n<var>types</var> contains one or more of the block status characters defined\nin the chapter Mapfile structure (see <a href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Mapfile-structure\">Mapfile structure</a>).\n\n <p>The list format is one block number per line in decimal, like the output\nof the badblocks program, so that it can be used as input for e2fsck or\nother similar filesystem repairing tool.\n\n <br></p></dd><dt><code>-L</code><dt><code>--loose-domain</code></dt></dt><dd>Accept an incomplete synthetic (user fabricated) domain mapfile or\ncompare-as-domain mapfile and fill the gaps in the list of data blocks\nwith non-tried blocks. The blocks in the mapfile must be strictly\nascending and non-overlapping, but they do not need to be contiguous. \nThis option allows making quick edits to a mapfile without all the size\ncalculations involved in making all data blocks contiguous again.\n\n <br></dd><dt><code>-m </code><var>file</var><dt><code>--domain-mapfile=</code><var>file</var></dt></dt><dd>Restrict the rescue domain to the blocks marked as finished in the\nmapfile <var>file</var>. Use '<samp></samp>' as <var>file</var> to read from standard\ninput.\n\n <br></dd><dt><code>-n</code><dt><code>--invert-mapfile</code></dt></dt><dd>Invert the types of the blocks in <var>mapfile</var> which are included in\nthe rescue domain, and write the resulting mapfile to standard output. \nFinished blocks ('<samp></samp>') are changed to bad-sector ('<samp></samp>'), all\nother types are changed to finished. '<samp><p class=\"samp\">--invert-mapfile</p></samp>' is\nequivalent to '<samp><p class=\"samp\">--change-types=?*/-+,++++-</p></samp>'\n\n <br></dd><dt><code>-o </code><var>bytes</var><dt><code>--output-position=</code><var>bytes</var></dt></dt><dd>Starting position of the image of the rescue domain in the original\n<var>outfile</var>, in bytes. Is used by the '<samp><p class=\"samp\">--list-blocks</p></samp>' option. \nDefaults to '<samp><p class=\"samp\">--input-position</p></samp>'.\n\n <br></dd><dt><code>-p </code><var>file</var><dt><code>--compare-mapfile=</code><var>file</var></dt></dt><dd>Compare the types of the blocks included in the rescue domain. The exit\nstatus is 0 if all the blocks tested are the same in both <var>file</var> and\n<var>mapfile</var>, 1 otherwise.\n\n <br></dd><dt><code>-P </code><var>file</var><dt><code>--compare-as-domain=</code><var>file</var></dt></dt><dd>Compare only the blocks marked as finished in the rescue domain. The\nexit status is 0 if all the blocks tested are the same in both\n<var>file</var> and <var>mapfile</var>, 1 otherwise. Two files comparing equal\nwith this option are equivalent when used as domain mapfiles.\n\n <br></dd><dt><code>-q</code><dt><code>--quiet</code></dt></dt><dd>Quiet operation. Suppress all messages.\n\n <br></dd><dt><code>-s </code><var>bytes</var><dt><code>--size=</code><var>bytes</var></dt></dt><dd>Maximum size of the rescue domain, in bytes. It refers to a size in the\noriginal <var>infile</var>.\n\n <br></dd><dt><code>-t</code><dt><code>--show-status</code></dt></dt><dd>Print a summary of the contents of each <var>mapfile</var> to the standard\noutput. This option allows more than one <var>mapfile</var>. If the domain\nsetting options are used, the summary can be restricted to one or\nseveral parts of <var>mapfile</var>.\n\n <br></dd><dt><code>-v</code><dt><code>--verbose</code></dt></dt><dd>Verbose mode. Further -v's (up to 4) increase the verbosity level.\n\n <br></dd><dt><code>-x </code><var>file</var><dt><code>--xor-mapfile=</code><var>file</var></dt></dt><dd>Perform a logical XOR (exclusive OR) operation between the finished\nblocks in <var>file</var> and those in <var>mapfile</var>, and write the resulting\nmapfile to standard output. In other words, in the resulting mapfile a\nblock is only shown as finished if it was finished in either of the two\ninput mapfiles but not in both.\n\n <br></dd><dt><code>-y </code><var>file</var><dt><code>--and-mapfile=</code><var>file</var></dt></dt><dd>Perform a logical AND operation between the finished blocks in\n<var>file</var> and those in <var>mapfile</var>, and write the resulting mapfile\nto standard output. In other words, in the resulting mapfile a block is\nonly shown as finished if it was finished in both input mapfiles.\n\n <br></dd><dt><code>-z </code><var>file</var><dt><code>--or-mapfile=</code><var>file</var></dt></dt><dd>Perform a logical OR operation between the finished blocks in <var>file</var>\nand those in <var>mapfile</var>, and write the resulting mapfile to standard\noutput. In other words, in the resulting mapfile a block is shown as\nfinished if it was finished in either of the two input mapfiles.\n\n </dd></dl>\n\n <p>Exit status: 0 for a normal exit, 1 for environmental problems (file not\nfound, invalid flags, I/O errors, etc), 2 to indicate a corrupt or\ninvalid input file, 3 for an internal consistency error (eg, bug) which\ncaused ddrescuelog to panic.\n\n</p><div class=\"node\">\n<a name=\"Problems\"></a>\n\nNext:&#xA0;<a rel=\"next\" accesskey=\"n\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Concept-index\">Concept index</a>,\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Invoking-ddrescuelog\">Invoking ddrescuelog</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"chapter\">15 Reporting bugs</h2>\n\n<p><a name=\"index-bugs-19\"></a><a name=\"index-getting-help-20\"></a>\nThere are probably bugs in ddrescue. There are certainly errors and\nomissions in this manual. If you report them, they will get fixed. If\nyou don't, no one will ever know about them and they will remain unfixed\nfor all eternity, if not longer.\n\n </p><p>If you find a bug in GNU ddrescue, please send electronic mail to\n<a href=\"mailto:bug-ddrescue@gnu.org\">bug-ddrescue@gnu.org</a>. Include the version number, which you can\nfind by running <code>ddrescue&#xA0;--version</code>.\n\n</p><div class=\"node\">\n<a name=\"Concept-index\"></a>\n\nPrevious:&#xA0;<a rel=\"previous\" accesskey=\"p\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Problems\">Problems</a>,\nUp:&#xA0;<a rel=\"up\" accesskey=\"u\" href=\"http://www.gnu.org/software/ddrescue/manual/ddrescue_manual.html#Top\">Top</a>\n\n</div>\n\n<h2 class=\"unnumbered\">Concept index</h2>\n\n</body></div>"
    }, {
        "title": "Higher Order Components: A React Application Design Pattern",
        "url": "https://www.sitepoint.com/react-higher-order-components/",
        "excerpt": "This article is by guest author Jack Franklin. SitePoint guest posts aim to bring you engaging content from prominent writers and speakers of the JavaScript community In this article we will discuss&hellip;",
        "date_saved": "Sat Jul 30 02:13:24 EDT 2016",
        "html_file": "higher_order_components_a_react_application_design_pattern.html",
        "png_file": "higher_order_components_a_react_application_design_pattern.png",
        "md_file": "higher_order_components_a_react_application_design_pattern.md",
        "content": "<div><div class=\"ArticleCopy language-jsx\">\n <p><em>This article is by guest author <strong>Jack Franklin</strong>. SitePoint guest posts aim to bring you engaging content from prominent writers and speakers of the JavaScript community</em></p>\n\n<p>In this article we will discuss how to use Higher Order Components to keep your React applications tidy, well structured and easy to maintain. We&#x2019;ll discuss how pure functions keep code clean and how these same principles can be applied to React components.</p>\n<h2 id=\"purefunctions\">Pure Functions</h2>\n<p>A function is considered pure if it adheres to the following properties:</p>\n<ul>\n<li>All the data it deals with are declared as arguments</li>\n<li>It does not mutate data it was given or any other data (these are often referred to as <em>side effects</em>).</li>\n<li>Given the same input, it will <em>always</em> return the same output.</li>\n</ul>\n<p>For example, the <code>add</code> function below is pure:</p>\n<pre><code class=\"javascript language-javascript\">function add(x, y) {\n return x + y;\n}\n</code></pre>\n<p>However, the function <code>badAdd</code> below is impure:</p>\n<pre><code class=\"javascript language-javascript\">var y = 2;\nfunction badAdd(x) { \n return x + y;\n}\n</code></pre>\n<p>This function is not pure because it references data that it hasn&#x2019;t directly been given. As a result, it&#x2019;s possible to call this function with the same input and get different output:</p>\n<pre><code class=\"javascript language-javascript\">var y = 2;\nbadAdd(3) // 5\ny = 3;\nbadAdd(3) // 6\n</code></pre>\n<p>To read more about pure functions you can read <a href=\"https://www.sitepoint.com/an-introduction-to-reasonably-pure-functional-programming/\">&#x201C;An introduction to reasonably pure programming&#x201D;</a> by Mark Brown.</p>\n<p>Whilst pure functions are very useful, and make debugging and testing an application much easier, occasionally you will need to create impure functions that have side effects, or modify the behavior of an existing function that you are unable to access directly (a function from a library, for example). To enable this we need to look at higher order functions.</p>\n<h2 id=\"higherorderfunctions\">Higher Order Functions</h2>\n<p>A higher order function is a function that when called, returns another function. Often they also take a function as an argument, but this is not required for a function to be considered higher order.</p>\n<p>Let&#x2019;s say we have our <code>add</code> function from above, and we want to write some code so that when we call it we log the result to the console before returning the result. We&#x2019;re unable to edit the <code>add</code> function, so instead we can create a new function:</p>\n<pre><code class=\"javascript language-javascript\">function addAndLog(x, y) { \n var result = add(x, y);\n console.log('Result', result);\n return result;\n}\n</code></pre>\n<p>We decide that logging results of functions is useful, and now we want to do the same with a <code>subtract</code> function. Rather than duplicate the above, we could write a <em>higher order function</em> that can take a function and return a new function that calls the given function and logs the result before then returning it:</p>\n<pre><code class=\"javascript language-javascript\">function logAndReturn(func) { \n return function() { \n var args = Array.prototype.slice.call(arguments);\n var result = func.apply(null, args);\n console.log('Result', result);\n return result;\n }\n}\n</code></pre>\n<p>Now we can take this function and use it to add logging to <code>add</code> and <code>subtract</code>:</p>\n<pre><code class=\"javascript language-javascript\">var addAndLog = logAndReturn(add);\naddAndLog(4, 4) // 8 is returned, &#x2018;Result 8&#x2019; is logged\n\nvar subtractAndLog = logAndReturn(subtract);\nsubtractAndLog(4, 3) // 1 is returned, &#x2018;Result 1&#x2019; is logged;\n</code></pre>\n<p><code>logAndReturn</code> is a HOF because it takes a function as its argument and returns a new function that we can call. These are really useful for wrapping existing functions that you can&#x2019;t change in behavior. For more information on this, check M. David Green&#x2019;s article <a href=\"https://www.sitepoint.com/higher-order-functions-javascript/\">&#x201C;Higher-Order Functions in JavaScript</a> which goes into much more detail on the subject. </p>\n<p>Additionally you can check out <a href=\"https://codepen.io/SitePoint/pen/akKmYp?editors=0012\">this CodePen</a>, which shows the above code in action.</p>\n<h2 id=\"higherordercomponents\">Higher Order Components</h2>\n<p>Moving into React land, we can use the same logic as above to take existing React components and give them some extra behaviours.</p>\n<p>In this section we&#x2019;re going to use <a href=\"https://github.com/reactjs/react-router\">React Router</a>, the de facto routing solution for React. If you&#x2019;d like to get started with the library I highly recommend the <a href=\"https://github.com/reactjs/react-router-tutorial\">React Router Tutorial</a> on GitHub.</p>\n\n<p>React Router provides a <code>&lt;Link&gt;</code> component that is used to link between pages in a React application. One of the properties that this <code>&lt;Link&gt;</code> component takes is <code>activeClassName</code>. When a <code>&lt;Link&gt;</code> has this property and it is currently active (the user is on a URL that the link points to), the component will be given this class, enabling the developer to style it.</p>\n<p>This is a really useful feature, and in our hypothetical application we decide that we always want to use this property. However, after doing so we quickly discover that this is making all our <code>&lt;Link&gt;</code> components very verbose:</p>\n<pre><code class=\"jsx language-jsx\">&lt;Link to=\"/\" activeClassName=\"active-link\"&gt;Home&lt;/Link&gt;\n&lt;Link to=\"/about\" activeClassName=\"active-link\"&gt;About&lt;/Link&gt;\n&lt;Link to=\"/contact\" activeClassName=\"active-link\"&gt;Contact&lt;/Link&gt;\n</code></pre>\n<p>Notice that we are having to repeat the class name property every time. Not only does this make our components verbose, it also means that if we decide to change the class name we&#x2019;ve got to do it in a lot of places.</p>\n<p>Instead, we can write a component that wraps the <code>&lt;Link&gt;</code> component:</p>\n<pre><code class=\"jsx language-jsx\">var AppLink = React.createClass({ \n render: function() { \n return (\n &lt;Link to={this.props.to} activeClassName=\"active-link\"&gt;\n {this.props.children}\n &lt;/Link&gt;;\n );\n }\n});\n</code></pre>\n<p>And now we can use this component, which tidies up our links:</p>\n<pre><code class=\"jsx language-jsx\">&lt;AppLink to=\"/\"&gt;Home&lt;/AppLink&gt;\n&lt;AppLink to=\"/about\"&gt;About&lt;/AppLink&gt;\n&lt;AppLink to=\"/contact\"&gt;Contact&lt;/AppLink&gt;\n</code></pre>\n<p>You can <a href=\"https://plnkr.co/edit/ohD1yy?p=preview\">see this example working on Plunker</a>.</p>\n<p>In the React ecosystem these components are known as higher order components because they take an existing component and manipulate it slightly <em>without changing the existing component</em>. You can also think of these as wrapper components, but you&#x2019;ll find them commonly referred to as higher order components in React-based content.</p>\n<h2 id=\"functionalstatelesscomponents\">Functional, Stateless Components</h2>\n<p>React 0.14 introduced support for functional, stateless components. These are components that have the following characteristics:</p>\n<ul>\n<li>They do not have any state.</li>\n<li>They do not use any React lifecycle methods (such as <code>componentWillMount()</code>).</li>\n<li>They only define the <code>render</code> method and nothing more.</li>\n</ul>\n<p>When a component adheres to the above, we can define it as a function, rather than using <code>React.createClass</code> (or <code>class App extends React.Component</code> if you&#x2019;re using ES2015 classes). For example, the two expressions below both produce the same component:</p>\n<pre><code class=\"jsx language-jsx\">var App = React.createClass({\n render: function() {\n return &lt;p&gt;My name is { this.props.name }&lt;/p&gt;;\n }\n});\n\nvar App = function(props) {\n return &lt;p&gt;My name is { props.name }&lt;/p&gt;;\n}\n</code></pre>\n<p>In the functional, stateless component instead of referring to <code>this.props</code> we&#x2019;re instead passed <code>props</code> as an argument. You can read more about this <a href=\"https://facebook.github.io/react/docs/reusable-components.html#stateless-functions\">on the React documentation</a>.</p>\n<p>Because higher order components often wrap an existing component you&#x2019;ll often find you can define them as a functional component. For the rest of this article I&#x2019;ll do that whenever possible.</p>\n<h2 id=\"betterhigherordercomponents\">Better Higher Order Components</h2>\n<p>The above component works, but we can do much better. The <code>AppLink</code> component that we created isn&#x2019;t quite fit for purpose.</p>\n<h3 id=\"acceptingmultipleproperties\">Accepting multiple properties</h3>\n<p>The <code>&lt;AppLink&gt;</code> component expects two properties:</p>\n<ul>\n<li><code>this.props.to</code> which is the URL the link should take the user to</li>\n<li><code>this.props.children</code> which is the text shown to the user</li>\n</ul>\n<p>However, the <code>&lt;Link&gt;</code> component accepts many more properties, and there might be a time when you want to pass extra properties along with the two above which we nearly always want to pass. We haven&#x2019;t made <code>&lt;AppLink&gt;</code> very extensible by hard coding the exact properties we need.</p>\n<h3 id=\"thejsxspread\">The JSX spread</h3>\n<p>JSX, the HTML-like syntax we use to define React elements, supports the spread operator for passing an object to a component as properties. For example, the code samples below achieve the same thing:</p>\n<pre><code class=\"jsx language-jsx\">var props = { a: 1, b: 2};\n&lt;Foo a={props.a} b={props.b} /&gt;\n\n&lt;Foo {...props} /&gt;\n</code></pre>\n<p>Using <code>{...props}</code> spreads each key in the object and passes it to <code>Foo</code> as an individual property.</p>\n<p>We can make use of this trick with <code>&lt;AppLink&gt;</code> so we support any arbitrary property that <code>&lt;Link&gt;</code> supports. By doing this we also future proof ourselves; if <code>&lt;Link&gt;</code> adds any new properties in the future our wrapper component will already support them. While we&#x2019;re at it, I&#x2019;m also going to change <code>AppLink</code> to be a functional component.</p>\n<pre><code class=\"jsx language-jsx\">var AppLink = function(props) {\n return &lt;Link {...props} activeClassName=\"active-link\" /&gt;;\n}\n</code></pre>\n<p>Now <code>&lt;AppLink&gt;</code> will accept any properties and pass them through. Note that we also can use the self closing form instead of explicitly referencing <code>{props.children}</code> in-between the <code>&lt;Link&gt;</code> tags. React allows <code>children</code> to be passed as a regular prop or as child elements of a component between the opening and closing tag. </p>\n<p><a href=\"https://plnkr.co/edit/hgBZ96?p=preview\">You can see this working on Plunker</a>.</p>\n<h3 id=\"propertyorderinginreact\">Property ordering in React</h3>\n<p>Imagine that for one specific link on your page, you have to use a different <code>activeClassName</code>. You try passing it into <code>&lt;AppLink&gt;</code>, since we pass all properties through:</p>\n<pre><code class=\"jsx language-jsx\">&lt;AppLink to=&#x201C;/special-link&#x201D; activeClassName=&#x201C;special-active&#x201D;&gt;Special Secret Link&lt;/AppLink&gt;\n</code></pre>\n<p>However, this doesn&#x2019;t work. The reason is because of the ordering of properties when we render the <code>&lt;Link&gt;</code> component:</p>\n<pre><code class=\"jsx language-jsx\"> return &lt;Link {...props} activeClassName=\"active-link\" /&gt;;\n</code></pre>\n<p>When you have the same property multiple times in a React component, the <em>last declaration wins</em>. This means that our last <code>activeClassName=&#x201C;active-link&#x201D;</code> declaration will always win, since it&#x2019;s placed <em>after</em> <code>{...this.props}</code>. To fix this we can reorder the properties so we spread <code>this.props</code> last. This means that we set sensible defaults that we&#x2019;d like to use, but the user can override them if they really need to: </p>\n<pre><code class=\"javascript language-javascript\"> return &lt;Link activeClassName=\"active-link\" {...props} /&gt;;\n</code></pre>\n<p>Once again, you can <a href=\"https://plnkr.co/edit/imLPgAQAB430lT1oBdU2?p=preview\">see this change in action on Plunker</a>.</p>\n<p>By creating higher order components that wrap existing ones but with additional behavior, we keep our code base clean and defend against future changes by not repeating properties and keeping their values in just one place.</p>\n<h2 id=\"higherordercomponentcreators\">Higher Order Component Creators</h2>\n<p>Often you&#x2019;ll have a number of components that you&#x2019;ll need to wrap in the same behavior. This is very similar to earlier in this article when we wrapped <code>add</code> and <code>subtract</code> to add logging to them. </p>\n<p>Let&#x2019;s imagine in your application you have an object that contains information on the current user who is authenticated on the system. You need some of your React components to be able to access this information, but rather than blindly making it accessible for every component you want to be more strict about which components receive the information.</p>\n<p>The way to solve this is to create a function that we can call with a React component. The function will then return a new React component that will render the given component but with an extra property which will give it access to the user information.</p>\n<p>That sounds pretty complicated, but it&#x2019;s made more straightforward with some code:</p>\n<pre><code class=\"jsx language-jsx\">function wrapWithUser(Component) {\n // information that we don&#x2019;t want everything to access\n var secretUserInfo = {\n name: 'Jack Franklin',\n favouriteColour: 'blue'\n };\n\n // return a newly generated React component\n // using a functional, stateless component\n return function(props) {\n // pass in the user variable as a property, along with\n // all the other props that we might be given\n return &lt;Component user={secretUserInfo} {...props} /&gt;\n }\n}\n</code></pre>\n<p>The function takes a React component (which is easy to spot given React components have to have capital letters at the beginning) and returns a new function that will render the component it was given with an extra property of <code>user</code>, which is set to the <code>secretUserInfo</code>.</p>\n<p>Now let&#x2019;s take a component, <code>&lt;AppHeader&gt;</code>, which wants access to this information so it can display the logged in user:</p>\n<pre><code class=\"jsx language-jsx\">var AppHeader = function(props) {\n if (props.user) {\n return &lt;p&gt;Logged in as {props.user.name}&lt;/p&gt;;\n } else {\n return &lt;p&gt;You need to login&lt;/p&gt;;\n }\n}\n</code></pre>\n<p>The final step is to connect this component up so it is given <code>this.props.user</code>. We can create a new component by passing this one into our <code>wrapWithUser</code> function.</p>\n<pre><code class=\"javascript language-javascript\">var ConnectedAppHeader = wrapWithUser(AppHeader);\n</code></pre>\n<p>We now have a <code>&lt;ConnectedAppHeader&gt;</code> component that can be rendered, and will have access to the <code>user</code> object. </p>\n<p><a href=\"http://codepen.io/SitePoint/pen/oLyzdo?editors=0010\">View this example on CodePen</a> if you&#x2019;d like to see it in action.</p>\n<p>I chose to call the component <code>ConnectedAppHeader</code> because I think of it as being connected with some extra piece of data that not every component is given access to. </p>\n<p>This pattern is very common in React libraries, particularly in <a href=\"http://redux.js.org\">Redux</a>, so being aware of how it works and the reasons it&#x2019;s being used will help you as your application grows and you rely on other third party libraries that use this approach.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>This article has shown how, by applying principles of functional programming such as pure functions and higher order components to React, you can create a codebase that&#x2019;s easier to maintain and work with on a daily basis. </p>\n<p>By creating higher order components you&#x2019;re able to keep data defined in only one place, making refactoring easier. Higher order function creators enable you to keep most data private and only expose pieces of data to the components that really need it. By doing this you make it obvious which components are using which bits of data, and as your application grows you&#x2019;ll find this beneficial.</p>\n<p>If you have any questions I&#x2019;d love to hear them; feel free to leave a comment or grab me as <a href=\"http://twitter.com/jack_franklin\">@Jack_Franklin</a> on Twitter.</p>\n </div>\n </div>"
    }, {
        "title": "Learn How to Use Awk Built-in Variables \u2013 Part 10",
        "url": "http://www.tecmint.com/awk-built-in-variables-examples/",
        "excerpt": "As we uncover the section of Awk features, in this part of the series, we shall walk through the concept of built-in variables in Awk. There are two types of variables you can use in Awk, these are;&hellip;",
        "date_saved": "Sat Jul 30 02:13:25 EDT 2016",
        "html_file": "learn_how_to_use_awk_builtin_variables_u2013_part_10.html",
        "png_file": "learn_how_to_use_awk_builtin_variables_u2013_part_10.png",
        "md_file": "learn_how_to_use_awk_builtin_variables_u2013_part_10.md",
        "content": "<div><div class=\"entry-inner\">\n<p>As we uncover the section of <strong>Awk</strong> features, in this part of the series, we shall walk through the concept of built-in variables in Awk. There are two types of variables you can use in Awk, these are; <strong>user-defined</strong> variables, which we covered in <a href=\"http://www.tecmint.com/learn-awk-variables-numeric-expressions-and-assignment-operators/\" target=\"_blank\">Part 8</a> and <strong>built-in</strong> variables.</p>\n<div id=\"attachment_21559\" class=\"wp-caption aligncenter\"><a href=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-Built-in-Variables-Examples.png\"><img src=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-Built-in-Variables-Examples.png\" class=\"size-full wp-image-21559\" alt=\"Awk Built in Variables Examples\" width=\"720\"></a><p class=\"wp-caption-text\">Awk Built in Variables Examples</p></div>\n<p><strong>Built-in</strong> variables have values already defined in <strong>Awk</strong>, but we can also carefully alter those values, the built-in variables include:</p>\n<ol>\n<li><code>FILENAME</code> : current input file name( do not change variable name)</li>\n<li><code>FR</code> : number of the current input line (that is input line 1, 2, 3&#x2026; so on, do not change variable name)</li>\n<li><code>NF</code> : number of fields in current input line (do not change variable name)</li>\n<li><code>OFS</code> : output field separator</li>\n<li><code>FS</code> : input field separator</li>\n<li><code>ORS</code> : output record separator</li>\n<li><code>RS</code> : input record separator</li>\n</ol>\n<p>Let us proceed to illustrate the use of some of the Awk built-in variables above:</p>\n<p>To read the filename of the current input file, you can use the <code>FILENAME</code> built-in variable as follows:</p>\n<pre>$ awk ' { print FILENAME } ' ~/domains.txt \n</pre>\n<div id=\"attachment_21549\" class=\"wp-caption aligncenter\"><a href=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-FILENAME-Variable.png\"><img src=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-FILENAME-Variable.png\" class=\"size-full wp-image-21549\" alt=\"Awk FILENAME Variable\" width=\"689\"></a><p class=\"wp-caption-text\">Awk FILENAME Variable</p></div>\n<ins class=\"adsbygoogle\"></ins><p>You will realize that, the filename is printed out for each input line, that is the default behavior of <strong>Awk</strong> when you use <code>FILENAME</code> built-in variable.</p>\n<p>Using <code>NR</code> to count the number of lines (records) in an input file, remember that, it also counts the empty lines, as we shall see in the example below.</p>\n<p>When we view the file <strong>domains.txt</strong> using <a href=\"http://www.tecmint.com/13-basic-cat-command-examples-in-linux/\" target=\"_blank\">cat command</a>, it contains <strong>14</strong> lines with text and empty <strong>2</strong> lines:</p>\n<pre>$ cat ~/domains.txt\n</pre>\n<div id=\"attachment_21553\" class=\"wp-caption aligncenter\"><a href=\"http://www.tecmint.com/wp-content/uploads/2016/07/Print-Contents-of-File.png\"><img src=\"http://www.tecmint.com/wp-content/uploads/2016/07/Print-Contents-of-File.png\" class=\"size-full wp-image-21553\" alt=\"Print Contents of File\" width=\"676\"></a><p class=\"wp-caption-text\">Print Contents of File</p></div>\n<pre>$ awk ' END { print \"Number of records in file is: \", NR } ' ~/domains.txt \n</pre>\n<div id=\"attachment_21551\" class=\"wp-caption aligncenter\"><a href=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-Count-Number-of-Lines.png\"><img src=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-Count-Number-of-Lines.png\" class=\"size-full wp-image-21551\" alt=\"Awk Count Number of Lines\" width=\"706\"></a><p class=\"wp-caption-text\">Awk Count Number of Lines</p></div>\n<p>To count the number of fields in a record or line, we use the NR built-in variable as follows:</p>\n<pre>$ cat ~/names.txt\n</pre>\n<div id=\"attachment_21552\" class=\"wp-caption aligncenter\"><a href=\"http://www.tecmint.com/wp-content/uploads/2016/07/List-File-Contents.png\"><img src=\"http://www.tecmint.com/wp-content/uploads/2016/07/List-File-Contents.png\" class=\"size-full wp-image-21552\" alt=\"List File Contents\" width=\"678\"></a><p class=\"wp-caption-text\">List File Contents</p></div>\n<pre>$ awk '{ print \"Record:\",NR,\"has\",NF,\"fields\" ; }' ~/names.txt\n</pre>\n<div id=\"attachment_21554\" class=\"wp-caption aligncenter\"><a href=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-Count-Number-of-Fields-in-File.png\"><img src=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-Count-Number-of-Fields-in-File.png\" class=\"size-full wp-image-21554\" alt=\"Awk Count Number of Fields in File\" width=\"704\"></a><p class=\"wp-caption-text\">Awk Count Number of Fields in File</p></div>\n<p>Next, you can also specify an input field separator using the <code>FS</code> built-in variable, it defines how <strong>Awk</strong> divides input lines into fields.</p>\n<p>The default value for <code>FS</code> is <strong>space</strong> and <strong>tab</strong>, but we can change the value of <code>FS</code> to any character that will instruct Awk to divide input lines accordingly.</p>\n<p>There are two methods to do this:</p>\n<ol>\n<li>one method is to use the <strong>FS</strong> built-in variable</li>\n<li>and the second is to invoke the <strong>-F</strong> Awk option</li>\n</ol>\n<p>Consider the file <strong>/etc/passwd</strong> on a Linux system, the fields in this file are divided using the <code>:</code> character, so we can specify it as the new input field separator when we want to filter out certain fields as in the following examples:</p>\n<p>We can use the <code>-F</code> option as follows:</p>\n<pre>$ awk -F':' '{ print $1, $4 ;}' /etc/passwd\n</pre>\n<div id=\"attachment_21555\" class=\"wp-caption aligncenter\"><a href=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-Filter-Fields-in-Password-File.png\"><img src=\"http://www.tecmint.com/wp-content/uploads/2016/07/Awk-Filter-Fields-in-Password-File.png\" class=\"size-full wp-image-21555\" alt=\"Awk Filter Fields in Password File\" width=\"698\"></a><p class=\"wp-caption-text\">Awk Filter Fields in Password File</p></div>\n<p>Optionally, we can also take advantage of the <code>FS</code> built-in variable as below:</p>\n<pre>$ awk ' BEGIN { FS=&#x201C;:&#x201D; ; } { print $1, $4 ; } ' /etc/passwd\n</pre>\n<div id=\"attachment_21556\" class=\"wp-caption aligncenter\"><a href=\"http://www.tecmint.com/wp-content/uploads/2016/07/Filter-Fields-in-File-Using-Awk.png\"><img src=\"http://www.tecmint.com/wp-content/uploads/2016/07/Filter-Fields-in-File-Using-Awk.png\" class=\"size-full wp-image-21556\" alt=\"Filter Fields in File Using Awk\" width=\"702\"></a><p class=\"wp-caption-text\">Filter Fields in File Using Awk</p></div>\n<p>To specify an output field separator, use the <code>OFS</code> built-in variable, it defines how the output fields will be separated using the character we use as in the example below:</p>\n<pre>$ awk -F':' ' BEGIN { OFS=\"==&gt;\" ;} { print $1, $4 ;}' /etc/passwd\n</pre>\n<div id=\"attachment_21557\" class=\"wp-caption aligncenter\"><a href=\"http://www.tecmint.com/wp-content/uploads/2016/07/Add-Separator-to-Field-in-File.png\"><img src=\"http://www.tecmint.com/wp-content/uploads/2016/07/Add-Separator-to-Field-in-File.png\" class=\"size-full wp-image-21557\" alt=\"Add Separator to Field in File\" width=\"701\"></a><p class=\"wp-caption-text\">Add Separator to Field in File</p></div>\n<p>In this <strong>Part 10</strong>, we have explored the idea of using Awk built-in variables which come with predefined values. But we can also change these values, though, it is not recommended to do so unless you know what you are doing, with adequate understanding.</p>\n<p>After this, we shall progress to cover how we can use shell variables in Awk command operations, therefore, stay connected to <strong>Tecmint</strong>.</p>\n</div>\n</div>"
    }, {
        "title": "Lehm",
        "url": "https://mustardamus.github.io/lehm/",
        "excerpt": "Commands Once you've installed Lehm with npm install lehm -g you'll have access to the lehm executable. It only has three commands. If you need a refresher on them, just run it without any command,&hellip;",
        "date_saved": "Sat Jul 30 02:13:27 EDT 2016",
        "html_file": "lehm.html",
        "png_file": "lehm.png",
        "md_file": "lehm.md",
        "content": "<div><div id=\"commands\" class=\"content section\">\n <h3 class=\"title\">Commands</h3>\n\n <p>\n Once you've installed Lehm with <code>npm install lehm -g</code>\n you'll have access to the <code>lehm</code> executable. It only has\n three commands. If you need a refresher on them, just run it without\n any command, which will be the same as <code>lehm --help</code>.\n </p>\n\n <div class=\"columns\">\n <div class=\"column is-4\">\n <code><span>lehm</span> list</code>\n <pre><span>$</span> lehm list\nTemplates Path: ~/templates\n\njs-component\njs-component: component.js | template.html | style.css\n\nNode.js Module - Scaffold for a Node.js Module\nnode-module: {{ moduleName }}/{{ moduleName }}.js | {{ moduleName }}/package.json</pre>\n\n <p>\n List all templates and their metas and files that are found in the\n configured templates path.\n Also see\n <a href=\"https://mustardamus.github.io/lehm/#metas\">Template Metas</a>.\n </p>\n </div>\n <div class=\"column is-4\">\n <code><span>lehm</span> create [name]</code>\n <pre><span>$</span> lehm create\n? Choose Template (Use arrow keys)\n&#x276F; js-component\n Node.js Module (node-module) - Scaffold for a Node.js Module\n\n<span>$</span> lehm create node-module\n? Name of your Node Module: Lehm\n? ...</pre>\n <p>\n Start creating files from a template. If you do not pass the\n <code>[name]</code>, you will be presented with a list of all\n templates to choose from.\n </p>\n </div>\n <div class=\"column is-4\">\n <code><span>lehm</span> config</code>\n <pre><span>$</span> lehm config\n? Templates Path /Users/mustardamus/Code/templates\n? Handlebars Delimiters ({{ }}) \nSaved.</pre>\n\n <p>\n Save the templates path and handlebars delimiters to\n <code>~/.lehmrc</code>. Also see\n <a href=\"https://mustardamus.github.io/lehm/#configuration\">Configuration</a>.\n </p>\n </div>\n </div>\n </div>\n\n <div id=\"templates\" class=\"content section\">\n <h3 class=\"title\">Template Tags</h3>\n\n <p>\n Lehm uses <a href=\"http://handlebarsjs.com\">Handlebars.js</a> to\n parse and transform the templates. By default the delimiters are\n <code>{{ }}</code>, but you can change them if they clash with the\n content of your templates. See how in the\n <a href=\"https://mustardamus.github.io/lehm/#configuration\">Configuration</a> or\n <a href=\"https://mustardamus.github.io/lehm/#metas\">Template Metas</a> section.\n </p>\n\n <h4 class=\"title is-4\">Create template directory</h4>\n\n <div class=\"columns\">\n <div class=\"column is-6\">\n <p>\n Lets learn the Template Tags by creating our first template, lets\n call it <code>js-component</code>.\n </p>\n <p>\n We create it in the default\n Templates Directory, which is in the Home folder of the current\n user, under the sub-directory <code>~/templates</code>. Again, you\n can <a href=\"https://mustardamus.github.io/lehm/#configuration\">configure it</a> if you want it to be\n in another path.\n </p>\n <p>\n Lets use the <code>lehm list</code> command to check if the newly\n created template was picked up.\n </p>\n </div>\n <div class=\"column is-6\">\n <pre><span>~ $</span> mkdir -p templates/js-component\n<span>~ $</span> cd templates/js-component\n<span>~/templates/js-component $</span> touch component.js\n<span>~/templates/js-component $</span> touch template.html\n<span>~/templates/js-component $</span> touch style.css\n<span>~/templates/js-component $</span> lehm list\nTemplates Path: ~/templates\n\njs-component\njs-component: component.js | style.css | template.html</pre>\n </div>\n </div>\n\n <h4 class=\"title is-4\">Comments and Variable Declarations</h4>\n\n <div class=\"columns\">\n <div class=\"column is-6\">\n <p>\n Lehm slightly extends Handlebars Comments. Comments are indicated\n by an <code>!</code> inside the delimiters. In generated files,\n the comments will be removed.\n </p>\n <p>\n If the comment does not include a <code>=</code> and/or\n <code>#</code> character, it will be a plain comment and not\n parsed.\n </p>\n\n <h5 class=\"title\">\n <code>variableName = default value</code>\n </h5>\n\n <p>\n You can assign a <code>default value</code> to a\n <code>variableName</code> by using the <code>=</code> character.\n When you create the template with <code>lehm create</code> you\n will be offered a <code>default value</code>, you either can hit\n enter to use it or overwrite it with another string. Note that the\n <code>default value</code> is always a string, with no need for\n <code>' '</code> or <code>\" \"</code>.\n </p>\n </div>\n <div class=\"column is-6\">\n <pre><span>~/templates/js-component $</span> cat component.js\n{{! this is a plain comment }}\n{{! componentName # Name of your component }}\n{{! namespace = components }}\n{{! initialComment = A JS Component # First line comment }}\n\n<span>~/templates/js-component $</span> cd\n<span>~ $</span> mkdir test-comp &amp;&amp; cd test-comp\n<span>~/test-comp $</span> lehm create js-component\n? Name of your component: test-comp\n? namespace (components): app\n? First line comment (A JS Component): neat :)</pre>\n </div>\n </div>\n\n <div class=\"columns\">\n <div class=\"column is-6\">\n <h5 class=\"title\">\n <code>variableName # useful description</code>\n </h5>\n\n <p>\n You can assign a <code>useful description</code> to\n <code>variableName</code> with the <code>#</code> character.\n When you create your template, the description will be shown in\n the prompts instead of the <code>variableName</code>.\n </p>\n </div>\n <div class=\"column is-6\">\n <h5 class=\"title\">\n <code>variableName = default # description</code>\n </h5>\n\n <p>\n A combination of both, <code>default value</code> and\n <code>useful description</code>, assigned to the\n <code>variableName</code>. The <code>description</code> must\n always come last.\n </p>\n </div>\n </div>\n\n <h4 class=\"title is-4\">\n Using, re-using and declaring Variables on the fly\n </h4>\n\n <div class=\"columns\">\n <div class=\"column is-6\">\n <p>\n The <code>component.js</code> template from above won't have any\n content, because we only have declared some variables but actually\n never used them. Lets try that.\n </p>\n <p>\n Variables are sitting between the delimiters, by default they are\n <code>{{ }}</code>, but they can be set to custom delimeters\n either using the <code>lehm config</code> command or by having a\n <code>.lehmrc</code> somewhere. See the the\n <a href=\"https://mustardamus.github.io/lehm/#configuration\">Configuration section</a> for details.\n Another way is to set per-template-delimiters with\n <a href=\"https://mustardamus.github.io/lehm/#metas\">Template Metas</a>.\n </p>\n <p>\n Variables don't need a declaration, just by using a variable will\n declared it, but without a default value or description. So use\n a good name for it to make it clear what the variable is about,\n like the <code>htmlContent</code> variable in the\n <code>template.html</code> example.\n </p>\n <p>\n Lehm parses all files in the template for variables before\n prompting for the values. That means that you can and must re-use\n variables across files. If in one file a variable has been\n declared without a default value or description, and in another a\n default value is set, it will be assigned to the variable.\n However, once the default value or description has been assigned,\n it can not be overwritten by a third declaration.\n </p>\n </div>\n <div class=\"column is-6\">\n <pre><span>~/templates/js-component $</span> cat component.js\n{{! this is a plain comment }}\n{{! componentName # Name of your component }}\n{{! namespace = components }}\n{{! initialComment = A JS Component # First line comment }}\n// {{ initialComment }}\n\n<span>~/templates/js-component $</span> cat template.html\n&lt;div class=\"{{ namespace }} {{ componentName }}\"&gt;\n {{ htmlContent }}\n&lt;/div&gt;\n\n<span>~/templates/js-component $</span> cd\n<span>~ $</span> rm -rf test-comp &amp;&amp; mkdir test-comp &amp;&amp; cd test-comp\n<span>~/test-comp $</span> lehm create js-component\n? Name of your component: test-comp\n? namespace (components): app\n? First line comment (A JS Component): neat :)\n? htmlContent: Pretty kewl\n<span>~/test-comp $</span> cat component.js\n// neat :)\n\n<span>~/test-comp $</span> cat template.html\n&lt;div class=\"app test-comp\"&gt;\n Pretty kewl\n&lt;/div&gt;</pre>\n </div>\n </div>\n\n <h4 class=\"title is-4\">\n Boolean Variables with <code>#if</code> and <code>#unless</code>\n </h4>\n\n <div class=\"columns\">\n <div class=\"column is-6\">\n <p>\n Lehm will parse Handlebars' <code>#if</code> and\n <code>#unless</code> block-statements and turn them into boolean\n variables that can either be <code>true</code> or\n <code>false</code>.\n </p>\n <p>\n In the prompt you can choose the <code>true</code> or\n <code>false</code> value from a list with your arrow keys.\n </p>\n <p>\n If you use <code>#if</code> in your template, the default value of\n the variable will be <code>true</code>. If you use\n <code>#unless</code> the default value will be <code>false</code>.\n </p>\n <p>\n You can use the <code>else</code> statement to cover both,\n <code>true</code> and <code>false</code> cases like shown in the\n example file <code>style.css</code>.\n </p>\n <p>\n You can assign a description to a boolean variable with the\n <code>#</code> character in a comment like you do with simple\n variables. Setting a default value would make no sense, just use\n either <code>#if</code> or <code>#unless</code>, depending if you\n want the default value to be <code>true</code> or\n <code>false</code>.\n </p>\n <p>\n Inside of the block-statements you can use variables normally, or\n nest other <code>#if</code> or <code>#unless</code>\n block-statements.\n </p>\n </div>\n <div class=\"column is-6\">\n <pre><span>~/templates/js-component $</span> cat style.css\n.{{ componentName }} {\n {{#if darkStyle }}\n background: black;\n color: white;\n {{else}}\n background: white;\n color: black;\n {{/if}}\n}\n\n<span>~/templates/js-component $</span> cd\n<span>~ $</span> rm -rf test-comp &amp;&amp; mkdir test-comp &amp;&amp; cd test-comp\n<span>~/test-comp $</span> lehm create js-component\n? Name of your component: test-comp\n? namespace (components): app\n? First line comment (A JS Component): neat :)\n? htmlContent: Pretty kewl\n? darkStyle (Use arrow keys)\n&#x276F; true\n false\n<span>~/test-comp $</span> cat style.css\n.test-comp {\n background: black;\n color: white;\n}</pre>\n </div>\n </div>\n\n <h4 class=\"title is-4\">\n Use variables inside file- and folder-names\n </h4>\n\n <div class=\"columns\">\n <div class=\"column is-6\">\n <p>\n File- and folder-names are parsed for template tags as well. So\n whenever you use the delimiters you declare a variable just like\n you would do in a file's content. This is very useful for\n creating named files and folders.\n </p>\n <p>\n You even can set the default value and the description of the\n variable. Just use the Handlebars comment <code>!</code> in\n combination with <code>=</code> and/or <code>#</code> in any\n file's content of the template like you've seen before.\n </p>\n </div>\n <div class=\"column is-6\">\n <pre><span>~/templates $</span> mkdir node-module &amp;&amp; cd node-module\n<span>~/templates/node-module $</span> mkdir \"{{ moduleName }}\"\n<span>~/templates/node-module $</span> cd \"{{ moduleName }}\"\n<span>~/templates/node-module/{{ moduleName }} $</span> echo \"{{! moduleName # Name of your Node Module }}\\n// :O\"&gt;\"{{ moduleName }}.js\"\n<span>~/templates/node-module/{{ moduleName }} $</span> ls\n{{ moduleName }}.js\n<span>~/templates/node-module/{{ moduleName }} $</span> cat \"{{ moduleName }}.js\"\n{{! moduleName # Name of your Node Module }}\n// :O\n\n<span>~/templates/node-module/{{ moduleName }} $</span> cd\n<span>~</span> lehm create node-module\n? Name of your Node Module: clay\n<span>~</span> cat clay/clay.js\n// :O</pre>\n </div>\n </div>\n </div>\n\n \n\n <div id=\"hooks\" class=\"content section\">\n <h3 class=\"title\">Template Hooks</h3>\n\n <div class=\"columns\">\n <div class=\"column is-6\">\n <p>\n With the <code>before</code> and <code>after</code> hook you can\n handle work that template parsing alone would not get done.\n </p>\n <p>\n These are functions that are stored in the same file as the\n template-metas, a <code>.js</code> file that has the same name\n as the parent folder. <code>node-module.js</code> in the example.\n </p>\n\n <h4 class=\"title is-4\"><code>before</code></h4>\n <p>\n The <code>before</code> hook is called before any file is\n generated, but after the template-files have been parsed for\n variables and the user has been prompted for values.\n </p>\n <p>\n Most of the time you want to extend Handlebars with custom\n helpers (<code>utils.Handlebars.registerHelper</code>) that you\n can use in your template-files. Another common task is to\n manipulate the <code>variables</code> that are passed into the\n template.\n </p>\n <p>\n The hook function is called with the following parameters:\n </p>\n <p>\n <code>srcPath</code> - The full path where the template-files are\n located.\n </p>\n <p>\n <code>distPath</code> - The full path where files will be\n generated.\n </p>\n <p>\n <code>variables</code> - An object with variable-names as keys,\n and the user entered input as values. Modify a value in this\n object, and it will be used in template-files.\n </p>\n <p>\n <code>utils</code> - An object with instances of the following\n tools:\n\n </p><ul>\n <li>\n <a href=\"http://handlebarsjs.com\">utils.Handlebars</a> -\n A slightly modified Handlebars.js. This\n version is aware of custom delimiters. Most of the time you\n want to use <code>utils.Handlebars.registerHelper</code>\n to register custom helpers you can use in your template-files.\n It has a extra function\n <code>utils.Handlebars.transform(str, data)</code> to\n transform template strings with custom delimiters (see\n example).\n </li>\n <li>\n <a href=\"https://github.com/SBoudrias/Inquirer.js\">\n utils.Inquirer\n </a> - With Inquirer.js you are able to prompt the user\n and execute different actions depending on the answers.\n </li>\n <li>\n <a href=\"https://github.com/shelljs/shelljs\">utils.Shell</a> -\n With Shell.js you are able to execute system commands. Useful\n for all kinds of stuff: download files, install dependencies,\n etc.\n </li>\n <li>\n <a href=\"https://github.com/jprichardson/node-fs-extra\">\n utils.Fs\n </a> - With fs-extra you can do any kind of action you can do\n with the normal Node.js' fs module, and a couple useful more.\n </li>\n <li>\n <a href=\"https://lodash.com/docs\">utils._</a> - With Lodash\n you receive a rather big toolbelt for all sorts of things to\n work with Arrays, Objects, Functions and Strings.\n </li>\n <li>\n <a href=\"https://github.com/chalk/chalk\">utils.Chalk</a> -\n With Chalk you can style terminal output. This is useful to\n show additional infos to the user nicely formatted.\n </li>\n </ul>\n \n <p>\n <code>cb</code> - The callback you need to call when everything is\n ready to proceed generating the files. Call this function without\n any parameter to continue. Call it with a error message, a String,\n to abort the generation process and show the message.\n </p>\n </div>\n <div class=\"column is-6\">\n <pre><span>~/templates/node-module $</span> cat node-module.js\nmodule.exports = {\n delimiters: '',\n\n before: function (srcPath, distPath, variables, utils, cb) {\n utils.Handlebars.registerHelper('loud', (val) =&gt; {\n return utils._.toUpper(val) + '!'\n })\n\n utils.Inquirer.prompt([{\n type: 'list',\n name: 'abort',\n message: 'Abort?',\n choices: ['No', 'Yes']\n }]).then((answers) =&gt; {\n if (answers.abort === 'Yes') {\n cb('Aborted by user')\n } else {\n cb()\n }\n })\n },\n\n after: function (srcPath, distPath, variables, utils) {\n utils.Shell.cd(distPath + '/' + variables.moduleName)\n utils.Shell.exec('npm init -y')\n utils.Shell.exec('npm install lodash --save')\n\n let str = utils.Handlebars.transform('')\n console.log(utils.Chalk.blue(str))\n }\n}\n\n<span>~/templates/node-module $</span> cd &amp;&amp; rm -rf clay\n<span>~ $</span> lehm create node-module\n? Name of your Node Module: clay\n? Abort? No\nWrote to ~/templates/clay/package.json:\n...\nclay@1.0.0 ~/templates/clay\n&#x2514;&#x2500;&#x2500; lodash@4.13.1\n...\nLETS ROCK!\n<span>~ $</span> ls clay\nnode_modules package.json clay.js\n</pre>\n \n <h4 class=\"title is-4\"><code>after</code></h4>\n <p>\n The <code>after</code> hook is called after all the files have\n been generated.\n </p>\n <p>\n Here you can finish the generation process, like installing\n dependencies or move files around or show any additional info\n that is useful to the user.\n </p>\n <p>\n It has has the same function parameters like the\n <code>before</code> hook, except there is no <code>cb</code> to\n indicate to continue. As far as Lehm is concerned, the process\n is done when all files are generated.\n </p>\n </div>\n </div>\n </div>\n\n <div id=\"helpers\" class=\"content section\">\n <h3 class=\"title\">Handlebar Helpers</h3>\n <p>\n Lehm extends Handlebars with these basic helpers, so you can use them\n in your template-files without the need for a <code>before</code>\n hook. But remember, you can add new helpers by using the\n <code>utils.Handlebars.registerHelper</code> method in the\n <code>before</code> hook.\n </p>\n\n <div class=\"columns\">\n <p class=\"column is-6\">\n <code>{{ lowerCase \"LOWERCASE\" }} = lowercase</code>\n </p>\n <p class=\"column is-6\">\n <code>{{ upperCase \"uppercase\" }} = UPPERCASE</code>\n </p>\n </div>\n <div class=\"columns\">\n <p class=\"column is-6\">\n <code>{{ snakeCase \"snake-case\" }} = snake_case</code>\n </p>\n <p class=\"column is-6\">\n <code>{{ camelCase \"camel_case\" }} = camelCase</code>\n </p>\n </div>\n <div class=\"columns\">\n <p class=\"column is-6\">\n <code>{{ kebabCase \"kebabCase\" }} = kebab-case</code>\n </p>\n <p class=\"column is-6\">\n <code>{{ capitalize \"capitalize\" }} = Capitalize</code>\n </p>\n </div>\n <div class=\"columns\">\n <p class=\"column is-6\">\n <code>{{ pluralize \"user\" }} = users</code>\n </p>\n <p class=\"column is-6\">\n <code>{{ singularize \"users\" }} = user</code>\n </p>\n </div>\n <div class=\"columns\">\n <div class=\"column is-12\">\n <code>{{ combine \"singularize,capitalize\" \"users\" }} = User</code>\n <p>\n With this helper you can combine multiple helpers that take a\n single String as parameter. That means you can combine any helper\n from above and any custom helper you may have written. The first\n argument is a comma seperated String with all helpers that you\n want to apply one after the other. The second argument is your\n String you want to transform (ie a variable in your\n template-files).\n </p>\n <p>\n If you write custom helpers you need to make sure that the\n variable is always the last parameter, just like in the combine\n helper above.\n </p>\n </div>\n </div>\n </div>\n\n <div id=\"configuration\" class=\"content section\">\n <h3 class=\"title\">Configuration</h3>\n\n <div class=\"columns\">\n <div class=\"column is-6\">\n <p>\n There are only two things you could configure with Lehm: the path\n where the templates are found, and which delimiters to use.\n Remember, you can overwrite the delimiters specifically for a template\n with the <a href=\"https://mustardamus.github.io/lehm/#metas\">Template Metas</a>.\n </p>\n <p>\n Out of the box Lehm looks in the path <code>~/templates</code> for\n templates, and <code>{{ }}</code> are used as default delimiters.\n </p>\n <p>\n You can permanently overwrite these defaults by creating a\n <code>.lehmrc</code> file in your users home folder. This is a JSON\n formatted file with the following fields:\n </p>\n <p>\n <code>templatesPath</code> - Full path where the global templates can\n be found.\n </p>\n <p>\n <code>handlebarsDelimiters</code> - A String to define the delimiters\n used by Handlebars.js. The delimiters must have a space in between\n them.\n </p>\n <p>\n For convenience you can use the <code>lehm config</code> command\n to set these values. It will save them to <code>~/.lehmrc</code>.\n Easy-peasy.\n </p>\n <p>\n If you use the <code>lehm</code> command, it will look for a\n <code>.lehmrc</code> in the current working directory before\n checking the home directory or using the defaults.\n </p>\n <p>\n That way you can have a project-specific configuration for Lehm.\n Use a relative path in <code>templatesPath</code> to store the\n templates in the same directory as your project.\n </p>\n <p>\n That's useful if you want to have global temlpates for scaffolding\n whole projects, but want to have components templates for a\n specific project stored in the same folder to keep it all\n together, and for other people to use.\n </p>\n </div>\n <div class=\"column is-6\">\n <pre><span>~ $</span> lehm list\nTemplates Path: /Users/mustardamus/templates\n\njs-component\njs-component: component.js | style.css | template.html\n\nnode-module\nnode-module: /.js\n<span>~ $</span> mkdir -p templates-alt/test\n<span>~ $</span> echo \"\"&gt;templates-alt/test/test.txt\n<span>~ $</span> lehm config\n? Templates Path: ~/templates-alt\n? Handlebars Delimiters: \nSaved.\n<span>~ $</span> cat .lehmrc\n{\n \"templatesPath\": \"/Users/mustardamus/templates-alt\",\n \"handlebarsDelimiters\": \"\"\n}\n\n<span>~ $</span> lehm list\nTemplates Path: /Users/mustardamus/templates-alt\n\ntest\ntest: test.txt\n<span>~ $</span> mkdir -p project/tpl/local/\n<span>~ $</span> cd project\n<span>~/project $</span> echo \":)\"&gt;tpl/local/only.txt\n<span>~/project $</span> cat .lehmrc\n{\n \"templatesPath\": \"./tpl\",\n \"handlebarsDelimiters\": \"\"\n}\n\n<span>~/project $</span> lehm list\nTemplates Path: ~/project/tpl\n\nlocal\nlocal: only.txt</pre>\n </div>\n </div>\n </div>\n\n </div>"
    }, {
        "title": "Making Sense of Everything with words2map",
        "url": "http://blog.yhat.com/posts/words2map.html",
        "excerpt": "We are now at a point in history when algorithms can learn, like people, about pretty much anything. Facebook is pursuing &#x201C;computer services that have better perception than people&#x201D;,&hellip;",
        "date_saved": "Sat Jul 30 02:13:28 EDT 2016",
        "html_file": "making_sense_of_everything_with_words2map.html",
        "png_file": "making_sense_of_everything_with_words2map.png",
        "md_file": "making_sense_of_everything_with_words2map.md",
        "content": "<div>\n <div class=\"row\">\n <br> \n\n\n<p>We are now at a point in history when algorithms can learn, like people, about\npretty much anything. Facebook is pursuing <a href=\"http://overlap.ai/facebook-computer-services-better-perception-than-people\">&#x201C;computer services that have better\nperception than people&#x201D;</a>, while researchers at Google aim to &#x201C;<a href=\"http://overlap.ai/google-solve-intelligence\">solve intelligence</a>&#x201D;.\nAt <a href=\"http://overlap.ai/words2map\">overlap.ai</a> we&#x2019;re building artificial intelligence to unite people through their\noverlapping passions, and here we introduce a framework we call <a href=\"http://overlap.ai/words2map-github\">words2map</a> for\nconsidering what our users love, like these personal passions of ours:\n<br></p>\n<p><img src=\"http://blog.yhat.com/static/img/words2map-1.png\"></p>\n<p>Let&#x2019;s explain <a href=\"http://overlap.ai/words2map-github\">the code</a> that made this pretty picture just from the raw text,\nwhich serves as the basis for our recommender system. When we set out to\nautomatically recommend groups and events to people that they may wish to\njoin, we didn&#x2019;t have any data to start with, and yet the <a href=\"http://overlap.ai/google-unreasonable-effectiveness-of-data\">unreasonable\neffectiveness</a> of data was not lost on us. So we decided to build from a\npre-trained machine learning model that has basically already seen all\nthere is to see: we set up a <a href=\"http://overlap.ai/word2vec-model\">word2vec model from Google trained on 100\nbillion words</a>&#x200A;&#x2014;&#x200A;just a few orders of magnitude bigger than Wikipedia.\nAnd indeed we found amazing insights embedded in the vectors, like:</p>\n<ul>\n<li><em>human + robot &#x2248; <strong>cyborg</strong></em></li>\n<li><em>electricity + silicon &#x2248; <strong>solar cells</strong></em></li>\n<li><em>virtual reality + reality &#x2248; <strong>augmented reality</strong></em></li>\n</ul>\n<p>Minus the futurism fetish above, you&#x2019;ll see the math above is fairly simple.\nWe call this &#x201C;adding vectors&#x201D;, while below you see what we are really doing is\naveraging each element across all the vectors. Let&#x2019;s dissect the guts here,\nsince deft applications of the vector algebra will take us far in terms of\nhacking intelligence into and out of the system:</p>\n<p><img alt=\"\" src=\"http://blog.yhat.com/static/img/adding-vectors.png\"></p>\n<p>The key point above is that new vectors are simply nearby existing vectors,\nsuch that <em>king - man + woman &#x2260; <strong>queen</strong></em> (at least not exactly). From this,\nit is clear that any new words can be introduced to the model simply by\nadding existing vectors for existing words. This idea is similar to how\nhumans add words to their own vocabulary with a dictionary: consider\nsome familiar words and combine their meanings to figure out new words.\nOur A.I. responds to unknown words by searching the web for them,\nextracting keywords from relevant websites, and adding vectors for\nthose keywords it knows, to produce a new vector representing the new thing.\nThis feels like it shouldn&#x2019;t work, like becoming a scientist just by reading\nWikipedia instead of doing real science experiments, but surprisingly its\nperformance is robust. Here is another cool map from <a href=\"http://overlap.ai/words2map-github\">words2map</a> (which you\ncan easily download and play with yourself) that demonstrates this:</p>\n<p><img src=\"http://blog.yhat.com/static/img/words2map-2.png\"></p>\n<p>Once each word is derived in 300 dimensions (from the word2vec model trained on\n100 billion words) we then applied t-SNE to embed them into 2D, as x and y\ncoordinates for data visualization, like seen above. This is indeed nice for\ndata visualization, while it&#x2019;s also very helpful in our pipeline because it\nremoves noise in the derived vectors, by forcing a new mapping based purely on\nrelative similarity. For this reason we will be using the low-dimensional\ncoordinates of each word in our recommender system.</p>\n<p>The above pipeline is strong enough for many applications, but in our case we\nwanted to go further to be able to uncover clusters of similar types of activities\nthat our users really enjoy (and also, to be able to quickly infer what types of\nthings you <em>don&#x2019;t</em> like). That way, we can recommend more or less of these things\nto you, and do so with high precision purely through A.I. At this point in our\nresearch we tried balltrees as a way to identify clusters:</p>\n<p><img src=\"http://blog.yhat.com/static/img/t-SNE-clusters.png\"></p>\n<p>Upon close examination, these clusters are not bad, but we felt they weren&#x2019;t\nas responsive to the topology of the underlying data as we&#x2019;d like. So we continued\nresearching and found a clustering algorithm that works really well for our\ncomplex distributions: HDBSCAN, i.e. &#x201C;hierarchical density-based spatial\nclustering of applications with noise&#x201D;. It sounds fancy, but it&#x2019;s very simple\nto work with, given that you only need to put in 2D coordinates.</p>\n<p>The full pipeline for all of this word vector hackery comes together like so:</p>\n<p><img alt=\"\" src=\"http://blog.yhat.com/static/img/words2map-pipeline.png\"></p>\n<p>We&#x2019;re very happy to make <a href=\"http://blog.yhat.com/posts/overlap.ai/words2map-github\">words2map available through github</a>, and have worked hard\nto make sure that almost anyone with a Mac or Linux terminal can quickly download\nand play with it by copying and pasting:</p>\n<pre><code>git clone https://github.com/overlap-ai/words2map.git\ncd words2map\n./install.sh\n</code></pre>\n<p>We welcome critical feedback, and anyone interested in joining us in advancing\nthe state of this art across machine learning + data visualization,\nto make it better and better for everyone.</p>\n<p>As of today, words2map is mapping every group that our users launch\nthrough our iPhone app&#x200A;&#x2014;&#x200A;which you can <a href=\"http://overlap.ai/download-overlap\">download now</a>. We&#x2019;ve tested it,\nand know it reliably derives reasonable vectors in an online way for any\ntopic that&#x2019;s learnable on the web&#x200A;&#x2014;&#x200A;i.e. just about every topic. Readers\nare invited to <a href=\"http://overlap.ai/words2map-github\">try out words2map</a> for any type of natural language processing,\nand share their maps using a <a href=\"http://overlap.ai/words2map-hashtag\">#words2map</a> hashtag.</p>\n<p>Readers are also invited to overlap with us over A.I. and data science&#x200B;,\nin New York City and beyond&#x200B;. This summer we&#x2019;re hosting coffee chats,\ndata hackathon&#x200B;s&#x200B;, and other fun stuff so we can connect, join forces,\nand hack&#x200B; cool stuff&#x200B;&#x200B; together&#x200B;.&#x200B; Just <a href=\"http://overlap.ai/\">download the app here</a> and soon you'll be\nnerding with us IRL.</p>\n<p>Big thanks to the data hipsters at <a href=\"http://overlap.ai/yhat\">Yhat</a> for helping us share something new on their blog,\nand for spearheading great open source data science tools like <a href=\"http://overlap.ai/yhat-rodeo\">Rodeo</a> and <a href=\"http://yhat.github.io/ggplot/\">ggplot</a>.\nAnd thanks to so many scientists, engineers, and leaders who have helped make\nall of this possible. We love you:</p>\n<p><img src=\"http://blog.yhat.com/static/img/we-love-you.png\"></p>\n\n </div>\n </div>"
    }, {
        "title": "Git Tutorial \u2013 The Ultimate Guide",
        "url": "https://www.javacodegeeks.com/2016/07/git-tutorial.html",
        "excerpt": "Git is, without any doubt, the most popular version control system. Ironically, there are other version control systems easier to learn and to use, but, despite that, Git is the favorite option for&hellip;",
        "date_saved": "Sat Jul 30 02:13:30 EDT 2016",
        "html_file": "git_tutorial_u2013_the_ultimate_guide.html",
        "png_file": "git_tutorial_u2013_the_ultimate_guide.png",
        "md_file": "git_tutorial_u2013_the_ultimate_guide.md",
        "content": "<div><div class=\"entry\"><p>Git is, without any doubt, the most popular version control system. Ironically, there are other version control systems easier to learn and to use, but, despite that, Git is the favorite option for developers, which is quite clarifying about the powerfulness of Git.</p><p>This guide will cover all the topics needed to know in order to use Git properly, from explaining what is it and how it differs from other tools, to its usage, covering also advanced topics and practices that can suppose an added value to the process of version controlling.</p><h2 id=\"section_1\">1. What is version control? What is Git?</h2><p>Version control is the management of the changes made within a system, that it has not to be software necessarily.</p><p>Even if you have never used before Git or similar tools, you will probably have ever carried out a version control. A very used and <em>bad</em> practice in software developing is, when the software has reached a stable situation, saving a local copy of it, identifying it as stable, and then following with the changes in other copy.</p><p>This is something that <strong>every</strong> software engineer has done before using specific tools version controlling, so don&#x2019;t feel bad if you have done it. Actually, this is much more better than having commented the code like:</p><pre class=\"brush:java\">/* First version\npublic void foo(int bar) {\n return bar + 1;\n}\n*/\n/* Second version\npublic void foo(int bar) {\n return bar - 1;\n}\n*/\npublic void foo(int bar) {\n return bar httpie_parse_url.sh links_data.json links_list.txt node_modules package.json parse_urls_json.sh 2;\n}</pre><p>Which should be declared illegal.</p><p>The <strong>version control systems (VCS)</strong> are designed for carrying out a proper management of the changes. These tools provide the following features:</p><ul><li>Reversibility.</li><li>Concurrency.</li><li>Annotation.</li></ul><p>The reversibility is the main capability of a VCS, allowing to return to any point of the history of the source code, for example, when a bug has been introduced and the code has to return to a stable point.</p><p>The concurrency allows to have several people making changes on the same project, facilitating the process of the integration of pieces of code developed by two or more developers.</p><p>The annotation is the feature that allows to add additional explanations and thoughts about the changes made, such us a resume of the changes made, the reason that has caused these changes, an overall description of the stability, etc.</p><p>With this, <strong>the VCSs solve one of the most common problems of software development: the fear for changing the software</strong>. You will be probably be familiar to the famous saying &#x201C;if something works, don&#x2019;t change it&#x201D;. Which is almost a joke, but, actually, is like we act many times. <strong>A VCS will help you to get rid of being scared about changing your code</strong>.</p><p>There are several models for the version control systems. The one we mentioned, the manual process, can be considered as a local version control system, since the changes are only saved locally.</p><p>Git is a <strong>distributed</strong> version control system (DVCS), also known as decentralized. This means that <strong>every developer has a full copy of the repository</strong>, which is hosted in the cloud.</p><p>We will see more in detail the features of DVCSs in the following chapter.</p><h2 id=\"section_2\">2. Git vs SVN (DVCS vs CVCS)</h2><p>Before the DVCSs burst into the version controlling world, the most popular VCS was, probably Apache Subversion (known also as SVN). This VCS was centralized (CVCS). A centralized VCS is a system designed to have <strong>a single full copy of the repository, hosted in some server, where the developers save the changes they made</strong>.</p><p>Of course, using a CVCS is better than having a local version control, which is incompatible with teamwork. But having a version control system that completely depends on a centralized server has an obvious implication: if the server, or the connection to it goes down, the developers won&#x2019;t be able to save the changes. Or even worse, if the central repository gets corrupted, and no backup exists, the history of the repository will be lost.</p><p><strong>CVCSs can also be slow</strong>. Recording a change in the repository means making effective the change in the remote repository, so, it relies on the connection speed to the server.</p><p>Returning to Git and DVCSs, with it, every developer has the full repository locally. So, the <strong>developers can save the changes whenever they want</strong>. If at certain moment the server hosting the repository is down, the developers can continue working without any problem. And the changes could be recorded into the shared repository later.</p><p>Another difference with CVCSs, is that DVCSs, specially Git, are <strong>much more faster</strong>, since the changes are made locally, and the disk access is faster than network access, at least in normal situation.</p><p>The differences between both systems could be summed up to the following: with a <strong>CVCS you are enforced to have a complete dependency on a remote server</strong> to carry out your version control, whereas <strong>with a DVCS the remote server is just an option</strong> to share the changes.</p><h2 id=\"section_3\">3. Download and install Git</h2><h3 id=\"section_3_1\">3.1. Linux</h3><p>As you probably have guessed, Git can be installed in Linux executing the following commands:</p><pre class=\"brush:bash\">sudo apt-get update\nsudo apt-get install git</pre><h3 id=\"section_3_2\">3.2. Windows</h3><p>Firstly, we have to download the last stable release from <a href=\"https://git-scm.com/downloads\">official page</a>.</p><p>Run the executable, and click &#x201C;next&#x201D; button until you get to the following step:</p><div class=\"wp-caption aligncenter\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/Windows-install-1.jpg\" width=\"499\"><p class=\"wp-caption-text\">1. Configuring Git in Windows to use it through Git Bash only.</p></div><p>Check the first option. The following options can be left as they come by default. You are about four or five &#x201C;next&#x201D; ago of having Git installed.</p><p>Now, if you open the context menu (right click), you will see two new options:</p><ul><li>&#x201C;Git GUI here&#x201D;.</li><li>&#x201C;Git Bash here&#x201D;.</li></ul><p>In this guide we will be using the bash. All the commands shown will be for their execution in this bash.</p><h2 id=\"section_4\">4. Git usage</h2><p>In this chapter, we will see how to use Git to start with our version controlling.</p><h3 id=\"section_4_1\">4.1. Creating a repository</h3><p>To begin using Git, we have first to create a repository, also known as &#x201C;repo&#x201D;. For that, in the directory where we want to have the repository, we have to execute:</p><pre class=\"brush:bash\">git init</pre><p>We have a Git repository! Note that a folder named <code>.git</code> has been created. The repository will be the directory where the <code>.git</code> folder is placed. This folder is the repository metadata, an embedded database. It&#x2019;s better not to touch anything inside it while you are not familiarized with Git.</p><h3 id=\"section_4_2\">4.2. Creating the history: commits</h3><p>Git constructs the history of the repository with commits. <strong>A commit is a full snapshot of the repository, that is saved in the database</strong>. Every state of the files that are committed, will be recoverable later at any moment.</p><p>When doing a commit, we have to choose which files are going to be committed; not all the repository has to be committed necessarily. This process is called <strong>staging</strong>, where files are <strong>added to the index</strong>. The Git index is <strong>where the data that is going to be saved in the commit is stored temporarily</strong>, until the commit is done.</p><p>Let&#x2019;s see how it works.</p><p>We are going to create a file and add some content to it, for example:</p><pre class=\"brush:bash\">echo 'My first commit!' &gt; README.txt</pre><p>Adding this file, the status of the repository has changed, since a new file has been created in the <strong>working directory</strong>. We can check for the status of the repository with the <code>status</code> option:</p><pre class=\"brush:bash\">git status</pre><p>Which, in this case, would generate the following output:</p><pre class=\"brush:bash\">On branch master\n\nInitial commit\n\nUntracked files:\n&#xA0; (use \"git add &lt;file&gt;...\" to include in what will be committed)\n\n&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; README.txt\n\nnothing added to commit but untracked files present (use \"git add\" to track)</pre><p>What Git is saying is &#x201C;<em>you have a new file in the repository directory, but this file is not yet selected to be committed</em>&#x201C;.</p><p>If we want to include this file the commit, remember that it has to be added to the index. This is made with the <code>add</code> command, as Git suggests in the output of <code>status</code> :</p><pre class=\"brush:bash\">git add README.txt</pre><p>Again, the status of the repository has changes:</p><pre class=\"brush:bash\">On branch master\n\nInitial commit\n\nChanges to be committed:\n&#xA0; (use \"git rm --cached &lt;file&gt;...\" to unstage)\n\n&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; new file:&#xA0;&#xA0; README.txt</pre><p>Now, we can do the commit!</p><pre class=\"brush:bash\">git commit</pre><p>Now, the default text editor will be shown, where we have to type the commit message, and then save. <strong>If we leave the message empty, the commit will be aborted</strong>.</p><p>Additionally, we can use the shorthand version with <code>-m</code> flag, specifying the commit message inline:</p><pre class=\"brush:bash\">git commit -m 'Commit message for first commit!'</pre><p>We can add all the files of the current directory, recursively, to the index, with <code>.</code>:</p><pre class=\"brush:bash\">git add .</pre><p>Note that the following:</p><pre class=\"brush:bash\">echo 'Second commit!' &gt; README.txt\ngit add README.txt\necho 'Or is it the third?' &gt; README.txt\ngit commit -m 'Another commit'</pre><p><strong>Would commit the file with <code>'Second commit!'</code> content, because it was the one added to the index, and then we changed the file of the working directory</strong>, not the one added to staging area. To commit the latest change, we would have to add again the file to the index, being the first added file overwritten.</p><p>Git identifies each commit uniquely using SHA1 hash function, based on the contents of the committed files. So, each commit is identified with a 40 character-long hexadecimal string, like the following, for example: <code>de5aeb426b3773ee3f1f25a85f471750d127edfe</code>. Take into account that the commit message, commit date, or any other variable rather than the committed files&#x2019; content (and size), are not included in the hash calculation.</p><p>So, for our first two commit, the history would be the following:</p><div class=\"wp-caption aligncenter\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/1-Git-history.jpg\" width=\"860\"><p class=\"wp-caption-text\">2. History of the repository, with two commits.</p></div><p>Git shortens the checksum of each commit to 7 characters (whenever it&#x2019;s possible), to make them more legible.</p><p>Each commit points to the commit it has been created from, being this called the &#x201C;ancestor&#x201D;.</p><p>Note that <code>HEAD</code> element. This is one of the most important element in Git. The <code>HEAD</code> is the element that points to the current point in the repository history. <strong>The contents of the working directory will be those that belong to the snapshot the <code>HEAD</code> is pointing to</strong>.</p><p>We will see this <code>HEAD</code> more in detail later.</p><h4>4.2.1. Tips for creating good commit messages</h4><p>The commit message content is more important that it may seem at first sight. Git allows to add any kind of explanation for any change we made, without touching the source code, and we should always take advantage of this.</p><p>For the message formatting, there&#x2019;s an unwritten rule known as the <strong>50/72 rule</strong>, which is so simple:</p><ul><li>One first line with a summary of no more than 50 characters.</li><li>Wrap the subsequent explanations in lines of no more than 72 characters.</li></ul><p>This is based on how Git formats the output when we are reviewing the history.</p><p>But, more important than this, is the content of the message itself. The first thing that comes to mind to write are the changes that have been made, which is not bad at all. But the commit object itself is a description of the changes that have been made in the source code. To make the commit messages useful, <strong>you should always include the reason that motivated the changes</strong>.</p><h3 id=\"section_4_3\">4.3. Viewing the history</h3><p>Of course, Git is able to show the history of the repository. For that, the <code>log</code> command is used:</p><pre class=\"brush:bash\">git log</pre><p>If you try it, you will see that the output is not very nice. The <code>log</code> command has many flags available to draw pretty graphs. Here&#x2019;s a suggestion for using this command through this guide, even if graphs are shown for each scenario:</p><pre class=\"brush:bash\">git log --all --graph --decorate --oneline</pre><p>If you want, you can omit the <code>--oneline</code> flag for showing the full information of each commit.</p><h3 id=\"section_4_4\">4.4. Independent development lines: branches</h3><p>Branching is probably the most powerful feature of Git. <strong>A branch represents an independent development path</strong>. The branches coexist in the same repository, but each one has its own history. In the previous section, we have worked with a branch, Git&#x2019;s default branch, which is named <code>master</code>.</p><p>Taking into account this, the proper way to express the history would be the following, considering&#xA0;the branches.</p><div class=\"wp-caption aligncenter\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/2-History-with-master.jpg\" width=\"860\"><p class=\"wp-caption-text\">3. History of the repository, showing the branch pointer.</p></div><p>Creating a branch with Git is so simple:</p><pre class=\"brush:bash\">git branch &lt;branch-name&gt;</pre><p>For example:</p><pre class=\"brush:bash\">git branch second-branch</pre><p>And that&#x2019;s it.</p><p>But, what is Git doing really when it creates a branch? It just creates a pointer with that branch name that points to the commit where the branch has been created:</p><div class=\"wp-caption aligncenter\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/3-Two-branches.jpg\" width=\"860\"><p class=\"wp-caption-text\">4. History of the repository with a new branch.</p></div><p>This is one of the most notable features of Git: the branch creation speed, almost instantaneous, regardless of the repository size.</p><p>To start working in that branch, we have to <code>checkout</code> it:</p><pre class=\"brush:bash\">git checkout second-branch</pre><p>Now, the commits will only exist in <code>second-branch</code>. Why? <strong>Because the <code>HEAD</code> now is pointing to <code>second-branch</code>, so, the history created from now will have an independent path from <code>master</code></strong>.</p><p>We can see it making a couple of commits being located in <code>second-branch</code>:</p><pre class=\"brush:bash\">echo 'The changes made in this branch...' &gt;&gt; README.txt \ngit add README.txt \ngit commit -m 'Start changes in second-branch'\necho '... Only exist in this branch' &gt;&gt; README.txt \ngit add README.txt \ngit commit -m 'End changes in second-branch'</pre><p>If we check for the content of the file we have being modifying, we will see the following:</p><pre class=\"brush:bash\">Second commit!\nThe changes made in this branch...\n... Only exist in this branch</pre><p>But, what if we return to <code>master</code>?</p><pre class=\"brush:bash\">git checkout master</pre><p>The content of the file will be:</p><pre class=\"brush:bash\">Second commit!</pre><p>This is because, after creating the history of <code>second-branch</code>, we have placed the <code>HEAD</code> pointing to <code>master</code>:</p><div class=\"wp-caption alignnone\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/Two-histories.jpg\" width=\"860\"><p class=\"wp-caption-text\">5. Independent history for second-branch.</p></div><h3 id=\"section_4_5\">4.5. Combining histories: merging branches</h3><p>In the previous subsection, we have seen how we can create different paths for our repository history. Now, we are going to see how to combine them, what for Git is calling <strong>merging</strong>.</p><p>Let&#x2019;s suppose that, after the changes made in <code>second-branch</code>, is ready to return to <code>master</code>. For that, we have to place the <code>HEAD</code> in the destination branch (<code>master</code>), and specify the branch that is going to be merged to this destination branch (<code>second-branch</code>), with <code>merge</code> command:</p><pre class=\"brush:bash\">git checkout master\ngit merge second-branch</pre><p>And Git will give the following output:</p><pre class=\"brush:bash\">Updating f043d98..0705117\nFast-forward\n&#xA0;README.txt | 2 ++\n&#xA0;1 file changed, 2 insertions(+)</pre><p>Now, the history of the&#xA0;<code>second-branch</code> has been merged to <code>master</code>, so, all the changes made in this first branch have been applied to the second.</p><p>In this case, the entire history of second-branch is now part of the history of master, having a graph like the following:</p><div class=\"wp-caption aligncenter\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/4-Fast-forward.jpg\" width=\"860\"><p class=\"wp-caption-text\">6. History after merging second-branch to master.</p></div><p>As you can see, no track of the life of <code>second-branch</code> has been saved, when you probably were expecting a nice tree.</p><p>This is because Git merged the branch using the <code>fast-forward</code> mode. Note that is telling it in the merge output, shown above. Why did Git do this? <strong>Because <code>master</code> and <code>second-branch</code> shared the common ancestor</strong>, <code>f043d98</code>.</p><p>When we are merging branches, <strong>is always advisable not to use the&#xA0;<code>fast-forward</code> mode</strong>. This is achieved passing <code>--no-ff</code> flag while merging:</p><pre class=\"brush:bash\">git merge --no-ff second-branch</pre><p>What does this really do? Well, it just creates an intermediate, third commit, between the <code>HEAD</code>, and the &#x201C;from&#x201D; branch&#x2019;s last commit.</p><p>After saving the commit message (of course, is editable), the branch will be merged, having the following history:</p><div class=\"wp-caption aligncenter\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/No-ff-merge.jpg\" width=\"860\"><p class=\"wp-caption-text\">7. History after merging second-branch to master, using no fast-forward mode.</p></div><p>Which is much more expressive, since the history is reflected as it is actually is. <strong>The no fast-forward mode should be always used</strong>.</p><p>A merge of a branch supposes the end of the life of this. So, it should be deleted:</p><pre class=\"brush:bash\">git branch -d second-branch</pre><p>Of course, in the future, you can create again a <code>second-branch</code> named branch.</p><h3 id=\"section_4_6\">4.6. Conflictive merges</h3><p>In the previous section we have seen an &#x201C;automatic&#x201D; merge, i.e., Git has been able to merge both histories. Why? Because of the previously mentioned <strong>common ancestor</strong>. That is, the branch is returning to the point it started from.</p><p>But, when the branch another branch borns from suffers changes, problems appear.</p><p>To understand this, let&#x2019;s construct a new history, which will have the following graph:</p><div class=\"wp-caption alignnone\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/Conflictive-histories.jpg\" width=\"860\"><p class=\"wp-caption-text\">8. Continuing the history of master, after the creation of second-branch.</p></div><p>With the following commands:</p><pre class=\"brush:bash\">echo 'one' &gt;&gt; file.txt\ngit add file.txt\ngit commit -m 'first'\n\necho 'two' &gt;&gt; file.txt\ngit add file.txt \ngit commit -m 'second'\n\ngit checkout -b second-branch\n\necho 'three (from second-branch)' &gt;&gt; file.txt \ngit add file.txt \ngit commit -m 'third from second branch'\n\ngit checkout master\n\necho 'three' &gt;&gt; file.txt \ngit add file.txt \ngit commit -m 'third'</pre><p>What will happen if we try to merge <code>second-branch</code> to <code>master</code>?</p><pre class=\"brush:bash\">git checkout master\ngit merge second-branch</pre><p>Git won&#x2019;t be able to do it:</p><pre class=\"brush:bash\">CONFLICT (content): Merge conflict in file.txt\nAutomatic merge failed; fix conflicts and then commit the result.</pre><p><strong> Git doesn&#x2019;t know how to do it, because the changes made in <code>second-branch</code> are not directly applicable to <code>master</code>, since it has changed from this first branch inception</strong>. What Git has done is to indicate in which parts exists these incompatibilities.</p><p>Note that we haven&#x2019;t used the <code>--no-ff</code> flag, since we now in advance that the fast-forward won&#x2019;t be possible.</p><p>If we check the <code>status</code>, we will see the following:</p><pre class=\"brush:bash\">On branch master\nYou have unmerged paths.\n&#xA0; (fix conflicts and run \"git commit\")\n\nUnmerged paths:\n&#xA0; (use \"git add &lt;file&gt;...\" to mark resolution)\n\n&#xA0;&#xA0; &#xA0;both modified:&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; file.txt</pre><p>Showing the conflictive files. If we open it, we will see that Git has added some strange lines:</p><pre class=\"brush:bash;highlight:[3,5,7]\">one\ntwo\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nthree\n=======\nthree (from second-branch)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; second-branch</pre><p>Git has indicated which are the incompatible changes. And how does it know? <strong>The incompatible changes are those that have been introduced into the &#x201C;to&#x201D; merging branch (<code>master</code>) since the creation of the &#x201C;from&#x201D; merging branch (<code>second-branch</code>)</strong>.</p><p>Now, we have to decide how to combine the changes. On the one hand, the changes introduced to the current <code>HEAD</code> are shown (between <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code> and <code>=======</code>), and, on the other, the branch we are trying to merge (between <code>=======</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt; second-branch</code>). To solve the conflict, there are three options:</p><ul><li>Use&#xA0;<code>HEAD</code> version.</li><li>Use <code>second-branch</code> version.</li><li>A combination of two versions.</li></ul><p>Regardless the option, the file should end without any of the metacharacters that Git has added to identify the conflicts.</p><p>Once the conflicts have been resolved, we have to add the file to the index and continue with the merge, with <code>commit</code> command:</p><pre class=\"brush:bash;highlight:[3,5,7]\">git add file.txt\ngit commit</pre><p>Once saved the commit, the merge will be done, having Git created a third commit for this merge, as with when we used the <code>--no-ff</code> in the previous section.</p><h4>4.6.1. Knowing in advance which version to stay with</h4><p>It may happen that we know beforehand which version we want to choose in case of conflicts. In these cases, we can tell Git which version use, to make it apply it directly.</p><p>To do this, we have to pass the <code>-X</code> option to <code>merge</code>, indicating which version use:</p><pre class=\"brush:bash;highlight:[3,5,7]\">git merge -X &lt;ours|theirs&gt; &lt;branch-name&gt;</pre><p>So, for using <code>HEAD</code> version, we would have to use <code>ours</code> option; instead, for using the version that is not <code>HEAD</code>&#x2018;s, <code>theirs</code> has to be passed.</p><p>That is, the following:</p><pre class=\"brush:bash;highlight:[3,5,7]\">git merge -X ours second-branch</pre><p>Would leave the file as is shown:</p><pre class=\"brush:bash;highlight:[3,5,7]\">one\ntwo\nthree</pre><p>And, the following:</p><pre class=\"brush:bash;highlight:[3,5,7]\">git merge -X theirs second-branch</pre><p>As it follows:</p><pre class=\"brush:bash;highlight:[3,5,7]\">one\ntwo\nthree (from second-branch)</pre><h3 id=\"section_4_7\">4.7. Checking differences</h3><p>Git allows to check the differences between distinct points in the history. This is done with <code>diff</code> option.</p><h4>4.7.1. Interpreting the differences</h4><p>Before seeing what differences we can look at, firstly we have to understand how the differences are shown.</p><p>Let&#x2019;s see a sample output of a difference between the same file:</p><pre class=\"brush:bash\">diff --git a/README.txt b/README.txt\nindex 31325b6..55e8d58 100644\n--- a/README.txt\n+++ b/README.txt\n@@ -1,2 +1,2 @@\n-This is\n-the original file\n+This file\n+has been modified</pre><p>Here, <code>a</code> is the a previous version of the file, and <code>b</code> the current version.</p><p>The third and fourth line identifies each letter with a <code>-</code> or <code>+</code> symbol.</p><p>That <code>@@ -1,2 +1,2 @@</code> is called &#x201C;hunk header&#x201D;. This identifies the chunks of code that actually have changed, not showing the common parts for both versions.</p><p>The format is the following:</p><pre class=\"brush:bash\">@@ &lt;previous&gt;&lt;from-line&gt;,&lt;number-of-lines&gt; &lt;current&gt;,&lt;from-line&gt;&lt;number-of-lines&gt;</pre><p>In this case:</p><ul><li>&#x201C;previous&#x201D;: identified with <code>-</code>, corresponding to <code>a</code>.</li><li>&#x201C;from-line&#x201D;: the line number from where the changes start.</li><li>&#x201C;number-of-lines&#x201D;: the number of lines shown.</li><li>&#x201C;current&#x201D;: identified with <code>+</code>, corresponding to <code>b</code>.</li></ul><p>Finally, which lines are subtracted, and which added, are shown. In this case, two lines have been subtracted from the line (those preceded with <code>-</code>), and other two have been added (preceded with <code>+</code>).<code></code></p><h4>4.7.2. Differences between working directory and last commit</h4><p>One common use is to check the differences between the working directory and the last commit. For this, is enough to execute:</p><pre class=\"brush:bash\">git diff</pre><p>Which will show the difference for every file. We can specify also specific files:</p><pre class=\"brush:bash\">git diff &lt;file1&gt; &lt;file2&gt;</pre><h4>4.7.3. Differences between exact points in history</h4><p>We can look for differences with:</p><ul><li>SHA1 id</li><li>Branch names</li><li><code>HEAD</code></li><li>Tags</li></ul><p>Being combinable between them.</p><p>The syntax is the following:</p><pre class=\"brush:bash\">git diff &lt;original&gt;..&lt;modified&gt;</pre><p>For example, the following would show the changes that have been applied to <code>dev</code> branch, compared to a <code>v1.0</code> tag:</p><pre class=\"brush:bash\">git diff v1.0..dev</pre><h3 id=\"section_4_8\">4.8. Tagging important points</h3><p>Tagging is one of the nicest features of Git, since allows to mark important points in the repository history, in a very easy way. Usually, tags are used to mark releases, not only for stable releases, but also for under-development or incomplete releases, such as:</p><ul><li>Alpha</li><li>Beta</li><li>Release candidate (rc)</li></ul><p>Creating a tag is so simple, we just have to situate the <code>HEAD</code> in the point we want to tag, and just specify the tag name with the <code>tag</code> option:</p><pre class=\"brush:bash\">git tag -a &lt;tag-name&gt;</pre><p>For example:</p><pre class=\"brush:bash\">git tag -a v0.1-beta1</pre><p>Then, we will be asked to type a message for the tag. <strong>Typically, the changes made from last tag are specified</strong>.</p><p>As when committing, we can specify the tag message inline, with <code>-m</code> flag:</p><pre class=\"brush:bash\">git tag -a v0.1 -m 'v0.1 stable release, changes from...'</pre><p>Take into account that the <strong>tag names cannot be repeated </strong>in a repository.</p><h3 id=\"section_4_9\">4.9. Undoing and deleting things</h3><p>Git also allows to undo and modify some things in the history. In this section we will see what can be done, and how.</p><h4>4.9.1. Modifying the last commit</h4><p>Is quite common to want to modify the last commit, for example, when just a line of code has to be added; or even to modify the update message, without changing any file.</p><p>For that, Git has the <code>--amend</code> flag for <code>commit</code> command:</p><pre class=\"brush:bash\">git commit --amend</pre><p>This is just the same as committing, but, instead of a new commit object, the last one of that branch will be overwritten.</p><h4>4.9.2. Discarding uncommitted changes</h4><p>This is for, after a commit, when we keep developing, we think that we have taken an incorrect path, and we want to reset the changes, returning to the last commit&#x2019;s state.</p><p>For this, the command used is <code>checkout</code>, as for moving between branches. But, when specifying a file, this gets reseted to the state of the last commit.</p><p>For example:</p><pre class=\"brush:bash\">echo 'one' &gt; test.txt\ngit add test.txt \ngit commit -m 'commit one'\necho 'two' &gt; test.txt \ngit checkout test.txt # The content of test.txt is now 'one'.</pre><h4>4.9.3. Deleting commits</h4><p>Usually, we want to delete commits when we don&#x2019;t want to leave any record of an embarrassing commit, or just for removing useless changes.</p><p>This is achieved moving the branch or <code>HEAD</code> pointers. Moving the pointers to previous commits makes the commits remaining ahead get &#x201C;lost&#x201D;, unlinked from the linked list. To move them, <code>reset</code> command is used.</p><p>There are two ways of making a reset: not touching the working directory (soft reset, <code>--soft</code> flag), or resetting it too (hard reset, <code>--hard</code> flag). That is, <strong>if you make a soft reset, the commit(s) will be removed, but the modifications saved in that/those commit(s) will remain; and a hard reset, won&#x2019;t leave change made in the commit(s)</strong>. If no flag is specified, the reset will be done softly.</p><p>Let&#x2019;s start resetting things. The following command would remove the last commit, i.e., the commit where <code>HEAD</code> is pointing to:</p><pre class=\"brush:bash\">git reset --hard HEAD~</pre><p>The <code>~</code> character is for indicating an ancestor. Used once, indicates the immediate parent; twice, the grandparent; and so on. But, instead of typing <code>~</code> <em>n</em> times, we can specify the <em>n</em> ancestors that we want to remove:</p><pre class=\"brush:bash\">git reset --hard HEAD~3</pre><p>Which would remove the last 3 commits.</p><p>You may have noticed that this may cause conflicts with those commits with more than one ancestor, i.e., the result of a not fast-forwarded merge. Well, it doesn&#x2019;t cause any problem: the followed parent using <code>HEAD~</code> is always the first one. But there&#x2019;s a way to decide which of the common parents follow: <code>^</code>, followed by the parent number. So, the following:</p><pre class=\"brush:bash\">git reset --hard HEAD~2^2</pre><p>Would remove the previous two commits, but taking the path of the second ancestor.</p><p>Even if it is possible to specify which ancestor path follow, <strong>is recommended to always use the syntax for first ancestor</strong> (only <code>~</code>) <strong>since it&#x2019;s easier, even if more commands would be required</strong> (since you would have to checkout the different branches to update <code>HEAD</code> position).</p><h4>4.9.4. Deleting tags</h4><p>Deleting tags is so simple:</p><pre class=\"brush:bash\">git tag -d &lt;tag-name&gt;</pre><h2 id=\"section_5\">5. Branching strategies</h2><p>Reached this point, you may have already asked your self: &#x201C;<em>Okay, branches are cool, but when should I create and merge them?</em>&#x201D;</p><p>When we want to carry out a version control, we need to know&#xA0;which strategy we are going to follow. <strong>Using Git without having a clear branching policy is a complete nonsense</strong>.</p><p>The branching workflow to follow <strong>depends, mainly, on how we want to maintain the code</strong>. In this section, we will see the two main branching strategies.</p><h3 id=\"section_5_1\">5.1. Long running branches</h3><p>This strategy is used when we want <strong>to maintain a single version of our software at the same time</strong>. That is, when we offer the last version of our software as available, instead of having many versions (which can be still available, but considered as old or unmaintained).</p><p>The key of this strategy is having a branch only for stable versions, where the releases are tagged, for which the default branch is used, <code>master</code>; and having other branches for development, where the features are developed, tested, and integrated.</p><p>In this strategy, the <code>master</code> branch is the production branch, so <strong>only tested, properly integrated, definitive versions should be here. Or, there can also be under-development or not fully tested versions, but they must be properly tagged</strong>.</p><p>The following graph shows an example of the history of a repository following this strategy:</p><div class=\"wp-caption aligncenter\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/Long-running-branches-1.jpg\" width=\"786\"><p class=\"wp-caption-text\">9. Example of a repository history using the long-running branches strategy.</p></div><p>Simple and clarifying. The production state is only modified for those changes that have been integrated with the development branch, where nothing happens if something breaks. The changes that have to be made for each feature are perfectly isolated, favoring the division of tasks among the teammates.</p><h3 id=\"section_5_2\">5.2. One version, one branch</h3><p>This workflow is thought <strong>for creating software that will be available and maintained for several versions</strong>. In other words, a release does not &#x201C;overwrite&#x201D; every previous releases, it would &#x201C;overwrite&#x201D; only the release of the branch the release has been made for.</p><p>To achieve this, each maintained version has to have its main version, but with a common development path for all of them.</p><p>This is done having a branch for every version (usually named <code>PROJECT_NAME_XX_STABLE</code> or similar) for the stable release, the &#x201C;master&#x201D; of each version; and having a main branch (and its sub-branches) where the development is made, for which default&#xA0;<code>master</code> branch can be used. When each feature is developed and tested, the <code>master</code> branch can be merged to every wanted stable version.</p><p>This branching strategy is based on the long-running, but, in this case, having many &#x201C;masters&#x201D; instead of a single one.</p><p>Take into account that each feature should have to be tested with each version of the project we want to apply this feature to. Consider using Continuous Integration when dealing with this strategy.</p><p>Let&#x2019;s see an example of the graph of a history for which this strategy has been applied.</p><div class=\"wp-caption aligncenter\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/One-version-one-branch-1.jpg\" width=\"860\"><p class=\"wp-caption-text\">10. Example of a repository history using the one version, one branch strategy.</p></div><p>As we can see, here, the&#xA0;<code>master</code> branch is used as the main development branch.</p><p>In the v3.0 version, we have simulated a bug for <code>feature1</code>. It does not have to be necessarily a bug appeared at integration time, it can be a bug that has been detected later. In any case, we are sure that bug of <code>feature1</code> only exists for v3.0 version. In these cases, we shouldn&#x2019;t fix these bugs in&#xA0;<code>master</code> branch, because it&#x2019;s not something that affects all the versions. We should create a branch from the affected version, fix the bug, and merge to the specific branch. And, if the future the same error persists for new releases, we can consider the option of merging it to <code>master</code>.</p><p>The main advantage of this strategy is that the common features follow a common path, and the specific changes can be perfectly isolated for the affected versions.</p><h3 id=\"section_5_3\">5.3. Regardless the branching strategy: one branch for each bug</h3><p>No matter the branching strategy you use, an advisable practice is to create an independent branch for every bug, as same as it has been reported in your bug tracker (because you use a bug tracking system, right?). Actually, we have seen this practice in the previous example, in the integration of the feature into the branch for v3.0 version. <strong>Having an independent branch for each bug allows to having the issues perfectly located and identified, having the changes fixed involves isolated from others</strong>.</p><p>Identifying this branch with the id number generated by the bug tracker for the given bug, naming the branches, for example, as <code>issue-X</code>, allows to have a track between the changes made for the fix of that bug, to the comments made in the corresponding ticket of the bug tracker, which is really helpful, since you can explain possible solutions for the bugs, attach images, etc.</p><h2 id=\"section_6\">6. Remote repositories</h2><p>Git, as we have seen in the introduction, is a distributed VCS. This means that, apart from the local, we can have a copy of the repository hosted in a remote server that, apart from making public the source code of the project, is used for collaborative development.</p><p>The most popular platform for Git repositories hosting is <a href=\"https://github.com/\">GitHub</a>. Unfortunately, GitHub does not offer private repositories in its free plan. If you need a hosting platform with unlimited private repositories, you can use <a href=\"https://bitbucket.org/\">Bitbucket</a>. And, if you are looking for hosting your repositories in your own server, the available option is <a href=\"https://about.gitlab.com/\">GitLab</a>.</p><p>For this section, we will need to use one of the options mentioned above.</p><h3 id=\"section_6_1\">6.1. Writing changes in the remote</h3><p>The first thing we need to do in the remote hosting is to create a repository, for which a URL with the following format will be created:</p><pre class=\"brush:bash\">https://hosting-service.com/username/repository-name</pre><p>Once having a remote repository, we have to link it with our local repo. This is made with the <code>remote add &lt;remote-name&gt; &lt;repo-url&gt;</code> command:</p><pre class=\"brush:bash\">git remote add origin https://hosting-service.com/username/repository-name</pre><p><code>origin</code> is which is named by default by Git, similarly to <code>master</code> branch, but it does not have to be necessarily.</p><p>Now, in our local repository, the remote repository is identified as <code>origin</code>. We can now start to &#x201C;send&#x201D; information to it, which is made with <code>push</code> option:</p><pre class=\"brush:bash\">git push [remote-name] [branch-name | --all] [--tags]</pre><p>Let&#x2019;s see some examples of how it works:</p><pre class=\"brush:bash\">git push origin --all # Updates the remote with all the local branches\ngit push origin master dev # Updates remote's mater and dev branches\ngit push origin --tags # Sends tags to remotes</pre><p>This is an example output of a successful master branch update:</p><pre class=\"brush:bash\">Counting objects: 3, done.\nWriting objects: 100% (3/3), 235 bytes | 0 bytes/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://hosting-service.com/username/repository-name.git\n&#xA0;* [new branch]&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; master -&gt; master</pre><p>What has Git internally with this?</p><p>Well, now, a directory <code>.git/refs/remotes</code> has been created, and, inside it, another directory, <code>origin</code> (because that&#x2019;s the name we have given to the remote). Here, <strong>Git creates a file for each branch exiting in the remote repository, with a reference to it. This reference is just the SHA1 id of the last commit of the given branch of the remote repository</strong>. This is used by Git to know if in remote repo are any changes that can be applied to the local repository. We will cover this in detail later in the following sections.</p><p><strong>Note</strong>: a repository can have as many remotes as we want. For example, we could have the remote of the same repository in both GitHub and Bitbucket:</p><pre class=\"brush:bash\">git remote add github https://github.com/username/repository-name\ngit remote add bitbucket https://bitbucket.org/username/repository-name</pre><h3 id=\"section_6_2\">6.2. Cloning a repository</h3><p>The cloning of a repository is actually made once, when we are going to start to work with a remote repository.</p><p>For cloning remote repositories there&#x2019;s no mystery, we just have to use the <code>clone</code> option, specifying the URL of the repository:</p><pre class=\"brush:bash\">git clone https://hosting-service.com/username/repository-name</pre><p>Which would create a local directory with the repository, with the reference to the remote it has been cloned from.</p><p>By default, when cloning a repository, only the default branch is created (<code>master</code>, generally). The way to create the other branches locally is making a checkout to them.</p><p>Remote branches can be shown with <code>branch</code> option with <code>-r</code> flag:</p><pre class=\"brush:bash\">git branch -r</pre><p>They will be shown with the format <code>&lt;remote-name&gt;/&lt;branch-name&gt;</code>. To create the local branch, is enough to make a checkout to <code>&lt;branch-name&gt;</code>.</p><h3 id=\"section_6_3\">6.3. Updating remote references: fetching</h3><p>Fetching the remote repository means <strong>updating the reference of a local branch, to put it even with the remote branch</strong>.</p><p>Let&#x2019;s consider a collaborative scenario where two developers are pushing changes to the same repo. In some moment, one of the developers wants to update its reference to <code>master</code>, to the last push of the other developer, as is shown in the following image.</p><div class=\"wp-caption aligncenter\"><img src=\"https://www.javacodegeeks.com/wp-content/uploads/2016/07/Remote-1.jpg\" width=\"860\"><p class=\"wp-caption-text\">11. Collaborative development with a remote repository.</p></div><p>The content of the <code>.git/refs/remotes/origin/master</code> file of Alice&#x2019;s repository would be, before any update, the following:</p><pre class=\"brush:bash\">5bfc81ce5f7a0b26b493be0c99f1966a1896c972</pre><p>Bob&#x2019;s, however, will be updated, since it has been him the last who has updated the remote repository:</p><pre class=\"brush:bash\">37db4f82e346665f6048cc9e4b7cd48c83c6ebcb</pre><p>Now, Alice wants to have in her local repository the changes made by Bob. For that, she has to <code>fetch</code> the <code>master</code> branch:</p><pre class=\"brush:bash\">git fetch origin master</pre><p>(Alternatively, she can fetch every branch with <code>--all</code> flag):</p><pre class=\"brush:bash\">git fetch --all</pre><p>The output of the fetch would be something similar to the following:</p><pre class=\"brush:bash\">remote: Counting objects: 3, done.\nremote: Total 3 (delta 0), reused 0 (delta 0)\nUnpacking objects: 100% (3/3), done.\nFrom https://hosting-service.com/alice/repository-name\n&#xA0;* branch&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; master&#xA0;&#xA0;&#xA0;&#xA0; -&gt; FETCH_HEAD\n&#xA0;&#xA0; 5bfc81c..37db4f8&#xA0; master&#xA0;&#xA0;&#xA0;&#xA0; -&gt; origin/master</pre><p>And, now, the content of Alice&#x2019;s <code>.git/refs/remotes/origin/master</code> file would be the same of remote&#x2019;s:</p><pre class=\"brush:bash\">37db4f82e346665f6048cc9e4b7cd48c83c6ebcb</pre><p>With this, Alice has <strong>updated the reference to remote&#x2019;s&#xA0;<code>master</code> branch, but the changes have not been applied in the repository</strong>.</p><p><code>fetch</code>&#xA0; does not apply changes directly to the local repository. Is the first of two steps that have to be made. The other step is to <strong>merge the remote branch, which has just been updated; with the local branch</strong>. This is just a merge as any other, but where the remote name has to be indicated:</p><pre class=\"brush:bash\">git merge origin/master</pre><p>Once the merge is finished, Alice will have the latest version of <code>master</code> branch.</p><p><strong>Note</strong>: in this merge, we have not applied the no-fast forward. In this cases, would not have many sense to apply it, since we are merging the intrinsically same branch, but that is located somewhere else.</p><h3 id=\"section_6_4\">6.4. Fetching and merging remotes at once: pulling</h3><p>Git has an option to apply remote changes at once, that is, fetching and merging the branch in one command. This option is <code>pull</code>.</p><p>Considering the exactly same scenario seen above, we could just execute:</p><pre class=\"brush:bash\">git pull origin master</pre><p>And would do the fetch, followed by the merge.</p><p>Use the way you fell more comfortable with. There&#x2019;s no <em>better</em> way; actually, they do exactly the same, but expressed differently.</p><h3 id=\"section_6_5\">6.5. Conflicts updating remote repository</h3><p>Let&#x2019;s return to the scenario shown above. What would happen if, without updating her local repository, Alice would create a commit and push it to the remote repository? Well, she would receive an ugly message from Git:</p><pre class=\"brush:bash\">&#xA0;! [rejected]&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; master -&gt; master (fetch first)\nerror: failed to push some refs to 'https://hosting-service.com/alice/repository-name.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.</pre><p>The push would be rejected because Alice <strong>has not continued the history of the repository from a point that has been already registered in the remote</strong>. So, before writing changes in the remote repository, she would need first to fetch &amp; merge, or pull, the remote with her changes.</p><h4>6.5.1. A bad way to resolve conflicts</h4><p>There&#x2019;s still another way, which can not be exactly considered as <em>resolving</em>. Is about forcing pushes with <code>--force</code> flag.</p><p>Forcing a push is about, basically, one thing: overwrite the remote repository branch (or the whole repository) with the one that is being pushed. So, in the above scenario, if Alice forces a push of her repository, the <code>37db4f8</code> commit will disappear. <strong>This would leave Bob working in something similar to a &#x201C;parallel universe&#x201D; that cannot coexist with the current reality of the project</strong>, since his work is based on a state that no longer exist in the project.</p><p>In conclusion, <strong>don&#x2019;t force pushes when you are working with other developers</strong>, at least if you don&#x2019;t want to have an intense argument with them.</p><p>If you Google for &#x201C;git force push&#x201D; in the images section, you will see some memes that perfectly explain graphically what a forced push is considered as.</p><h3 id=\"section_6_6\">6.6. Deleting things in remote repository</h3><p>As with forced pushes, things in remote repositories must be deleted with extreme precaution. <strong>Before deleting anything, every collaborator should be informed about it</strong>.</p><p>Actually, when we are pushing commits, branches, etc., we are pushing them to a ref destination, but we don&#x2019;t have to specify the destination explicitly.</p><p>The explicit syntax is the following:</p><pre class=\"brush:bash\">git push origin &lt;source&gt;:&lt;destination&gt;</pre><p>So, the way of deleting remote things is updating the refs to prior states, or pushing &#x201C;nothing&#x201D;.</p><p>Let&#x2019;s see how to do it for every case.</p><h4>6.6.1. Deleting commits</h4><p>This is just the same as deleting commits locally, for example, to delete the last two commits:</p><pre class=\"brush:bash\">git push origin HEAD~2:master --force</pre><p>If we are using <code>HEAD</code> to refer the commits to remove, we must ensure that it&#x2019;s located on the same branch as remote.</p><p>Note that these pushes have to be forced too.</p><h4>6.6.2. Deleting branches</h4><p>This is quite simple, is just about pushing &#x201C;nothing&#x201D;, as said before. The following would remove <code>dev</code> branch from remote repository:</p><pre class=\"brush:bash\">git push origin :dev</pre><p><strong>Remote branches should be removed when the local branch is removed</strong>.</p><h4>6.6.3. Deleting tags</h4><p>As same as with branches, we have to push &#x201C;nothing&#x201D;, e.g.:</p><pre class=\"brush:bash\">git push origin --tags :v1.0</pre><h2 id=\"section_7\">7. Patches</h2><p>You will have probably ever seen a software being updated with something called <em>patch</em>. A patch is just a file that describes the changes that have to be made over a program, indicating which code lines have to be removed, and which have to be added. With Git, a path is just the output of a&#xA0;<code>diff</code> saved into a file.</p><p>Take into account that a patch is an update, but an update does not have to be a patch necessarily. <strong>Patches are thought for hotfixes</strong>, or also critical features that have to be fixed or implemented <em>right now</em>, not for being the common update and deploying strategy.</p><h3 id=\"section_7_1\">7.1. Creating patches</h3><p>As said, a patch for Git is just the output of a <code>diff</code>. So, we would have to redirect that output to a file:</p><pre class=\"brush:bash\">git diff &lt;expression&gt; &gt; &lt;patch-name&gt;.patch # The extension is not important.</pre><p>For example:</p><pre class=\"brush:bash\">git diff master..issue-1 &gt; issue-1-fix.patch</pre><p><strong>Note</strong>: a patch cannot be modified. <strong>If a patch file suffers any modification, Git will mark it as corrupt and it won&#x2019;t apply it.</strong></p><h3 id=\"section_7_2\">7.2. Applying patches</h3><p>The patches are applied with <code>apply</code> command. It&#x2019;s as simple as specifying the path to the path file:</p><pre class=\"brush:bash\">git apply &lt;patch-file&gt;</pre><p>If patching goes well, no message will be displayed (if you haven&#x2019;t used the <code>--verbose</code> flag). If the patch is not applicable, Git will show which are the files causing problems.</p><p>Is quite common to get errors due to just whitespace differences. These errors can be ignored with <code>--ignore-space-change</code> and <code>--ignore-whitespace</code> flags.</p><h2 id=\"section_8\">8. Cherry picking</h2><p>There may be some scenarios in where we are interested in porting into a branch just a specific set of changes made on another branch, instead of merging it.</p><p>To do this, Git allows to cherry pick commits from branches, with the <code>cherry-pick</code> command. The mechanism is the same as merging, but, in this case, specifying the commit SHA1 id, for example:</p><pre class=\"brush:bash\">git cherry-pick -x aba6c1b # Several commits can be cherry picked.</pre><p>A cherry pick creates a new commit, with the same message as the original&#x2019;s one. The <code>-x</code> option is for adding to that commit message a line indicating that is a cherry pick, and from which commit has been picked:</p><blockquote><p>(cherry picked from commit aba6c1bf9a0a7d6d9ccceeab2b5dfc64f6c115c2)</p></blockquote><p><strong>The cherry picking should not be a recurrent practice in your workflow, since it does not leave a track in the history</strong>, just a line indicating where has been picked from.</p><h2 id=\"section_9\">9. Hooks</h2><p>Git hooks are <strong>custom scripts that are triggered when important events occur</strong>, for example, a commit or a merge.</p><p>Let&#x2019;s say that you want to notify someone with an email when your production branch is updated, or to run a test suite; but in an automated way. The hooks can do this for you.</p><p>The scripts are located in <code>.git/hooks</code> directory. By default, some sample hooks are provided. Each hook has to have a concrete name, that we will see later, with no extension; and has to be marked as executable. You can use other scripting languages such us PHP or Python, apart from shell code.</p><p>There are two kind of hooks: client-side, and server-side.</p><h3 id=\"section_9_1\">9.1. Client-side hooks</h3><p>The client-side hooks are those that are fired when the local repository suffers changes. These are the most common:</p><ul><li><code>pre-commit</code>: invoked by <code>git commit</code> command, before the commit is saved. A commit can be aborted with this hook exiting with a non-zero status.</li><li><code>post-commit</code>: invoked by <code>git commit</code> too, but this time when the commit has been saved. At this point, the commit cannot be aborted.</li><li><code>post-merge</code>: as same as with <code>post-commit</code>, but being this one fired by <code>git merge</code>.</li><li><code>pre-push</code>: fired by <code>git push</code>, before the remote any object has been transferred.</li></ul><p>The following script shows an example applicable for both&#xA0;<code>post-commit</code> and&#xA0;<code>post-commit</code> hooks, for notification purposes:</p><pre class=\"brush:bash\">#!/bin/sh\n\nbranch=$(git rev-parse --abbrev-ref HEAD)\n\nif [ \"$branch\" = \"master\" ]; then\n&#xA0;&#xA0;&#xA0; echo \"Notifying release to everyone...\"\n&#xA0;&#xA0;&#xA0; # Send the notification...\nfi</pre><p>To use it for both cases, you would have to create both hooks.</p><h3 id=\"section_9_2\">9.2. Server-side hooks</h3><p>These hooks reside in the server where the remote repository is hosted. As it is obvious, if you are using services such as GitHub for hosting your repositories, you won&#x2019;t be permitted to execute arbitrary code. In GitHub&#x2019;s case, you have available third-party services, but never hooks with your own code. For using executing your code, you will have to have your own server for hosting.</p><p>The most used hooks are:</p><ul><li><code>pre-receive</code>: this is fired after someone makes a push, and before the references get updated in the remote. So, with this hook, you can deny any push.</li><li><code>update</code>: almost exact to the previous one, but this is executed for each reference pushed. That is, if 5 references have been pushed, this hook will be executed 5 times, unlike <code>pre-receive</code>, which is executed for the push as a whole.</li><li><code>post-receive</code>: this one is executed once the push has been successful, so it can be used for notification purposes also.</li></ul><h3 id=\"section_9_3\">9.3. Hooks are not included in the history</h3><p>The hooks only exist in the local repository. If you make a push in a repository that has custom hooks, these will not be sent to the remote. So, if every developer should share the same hooks, they would have to be included in the working directory, and installed them manually.</p><h2 id=\"section_10\">10. An approach to Continuous Integration</h2><p>Continuous Integration (CI) is a software engineering practice that is closely related to version controlling, and it&#x2019;s worth mentioning it, at least to know it conceptually.</p><p><strong>CI consists on making several integrations of the code </strong>(at least, once a day)<strong>, in a completely automated way, with the aim of finding errors in early phases of the development</strong>.</p><p>The concept is almost the same of the hooks, but in a much more <strong>scalable and maintainable way</strong>.</p><p>Here are some of the actions that are performed in each integration:</p><ul><li>Test execution: unit, acceptance, integration, regression, and a large etc.</li><li>Quality Assurance, with software metrics checking: cyclomatic complexity, code coverage, and another large etc.</li><li>Software coding standard compliance checking.</li></ul><p>Continuous Integration allows to perform those actions, and more, automatically, without the need of having to execute it manually, <strong>just with a push or commit</strong>. Interesting, isn&#x2019;t it?</p><p>Probably, the most used CI options are the following:</p><ul><li>Travis CI: cloud-based, which allows to fire integrations with pushes, without the need of having your own server.</li><li>Jenkins: self-hosted, for which a server is needed, but which is completely configurable. Compatible with many build tools, like Ant, Gradle, Maven, Phing, Grunt&#x2026;</li></ul><h2 id=\"section_11\">11. Conclusion</h2><p>Version control systems help to fulfill one of the dreams of every developer: having identified each point of the entire history of a project, being able to return to any point at any time. The VCS we have seen in this guide is Git, the preferred by the software developer community.</p><p>After reading this guide, if it&#x2019;s your first time with Git, you will probably be thinking how couldn&#x2019;t you live without Git until today.</p><h2 id=\"section_12\">12. Resources</h2><p>The best resource possible is Pro Git (2nd edition), by Scott Chacon and Ben Straub, which covers almost every topic about Git. You can read it free at <a href=\"https://git-scm.com/book/en/v2\">Git official page</a>.</p></div> </div>"
    }, {
        "title": "Why Blockchain Matters",
        "url": "http://www.datamation.com/data-center/why-blockchain-matters.html",
        "excerpt": "by Bernard Golden If your familiarity with Bitcoin and Blockchain is limited to having heard about the trial of Silk Road&#x2019;s Ross Ulbricht, you can be forgiven -- but your knowledge is out of&hellip;",
        "date_saved": "Sat Jul 30 02:13:31 EDT 2016",
        "html_file": "why_blockchain_matters.html",
        "png_file": "why_blockchain_matters.png",
        "md_file": "why_blockchain_matters.md",
        "content": "<div><div class=\"litcontent\">\n\t\t\t\t \t<p class=\"normal\"><em><strong>by Bernard Golden</strong></em></p>\n\n\n<p class=\"normal\">If your familiarity with Bitcoin and Blockchain is limited to having heard about <a href=\"https://www.wired.com/2015/05/silk-road-creator-ross-ulbricht-sentenced-life-prison/\">the trial of Silk Road&#x2019;s Ross Ulbricht</a>, you can be forgiven -- but your knowledge is out of date. Today, Bitcoin and especially Blockchain are moving into the mainstream, with <a href=\"http://www.onlinecasinoreports.com/articles/swiss-government-to-experiment-with-bitcoin.php\">governments</a> and <a href=\"https://securityintelligence.com/how-banks-are-leveraging-bitcoin-and-blockchain-technologies/\">financial institutions </a>launching <a href=\"http://www.coindesk.com/dutch-central-bank-preparing-boldest-blockchain-experiment-yet/\">experiments</a> and <a href=\"http://www.coindesk.com/sweden-blockchain-smart-contracts-land-registry/\">prototypes</a> to understand how they can take advantage of the unique characteristics of the technology.</p>\n<p class=\"normal\"><strong>Why Blockchain?</strong></p>\n<p class=\"normal\">The obvious question is why they&#x2019;re all so interested in blockchain. The answers vary, naturally, but they seem drawn by two opportunities: cost reduction and efficiency, and innovation. While the outcomes can be very different -- after all, cost reduction and efficiency typically focus on improving the as-is state of affairs while innovation disrupts or displaces the existing order of things.</p>\n<p class=\"normal\">Intriguingly, two characteristics of blockchain underlie both motivations: decentralization and immutability.</p>\n<p class=\"normalCxSpMiddle\">&#x25CF;&#xA0; Decentralization: Unlike most existing transactional systems, which require a centralized system of record run by a trusted authority, blockchain distributes the system of record (typically referred to as a distributed ledger) across multiple parties; conceptually, this ledger can run on thousands of nodes spread across the world. Decentralization avoids the problem of a centralized entity reaping outsized rewards due to its gatekeeper role as well as preventing it from committing fraud.</p>\n<p class=\"normalCxSpMiddle\">&#x25CF;&#xA0; Immutability: Transactional systems built on database technology have the ability to alter records after initial entry. While updates that reflect changing status are important (e.g., changing a person&#x2019;s marital status from single to married), updates also provide the mechanism for fraud. Blockchain operates differently than a centralized database -- instead of altering a single data entity, it attaches a new transaction that refers back to the last record. Thus, a person getting married would be represented by a married transaction that would point back to the person&#x2019;s previous status of single -- and if the status were already married, the new marriage transaction would be rejected (in many nations that prohibit multiple marriages).</p>\n<p class=\"normal\">These characteristics support both efficiency and innovation. Distributed ledgers hold the potential of streamlining many financial institution payment mechanisms. International payment transfers, for example, are notorious for requiring chains of financial institutions to transfer money from one to another -- sometimes five or six just to send money from, say, Nigeria to Singapore. A blockchain would require a single payment to be sent from one institution to another, with no need for intermediary institutions that, in essence, vouch that the preceding institution can be trusted by the downstream one. Since every one of the intermediary institutions requires payment for its vouching service, this could cut payment transfer fees significantly. It would, as another benefit, reduce the time required for an international payment from days to minutes. Consequently, banks all over the world <a href=\"https://bitcoinmagazine.com/articles/santander-becomes-first-u-k-bank-to-introduce-blockchain-technology-for-international-payments-1464795902\">are evaluating how they can apply blockchain technology to this type of transaction</a>.</p>\n<p class=\"normal\"><strong>Blockchain Experimentation</strong></p>\n<p class=\"normal\">From the innovation side, the <a href=\"http://www.coindesk.com/sweden-blockchain-smart-contracts-land-registry/\">Swedish Land Registry is testing a system</a> to place real estate transactions on the blockchain via a mechanism called a smart contract. Anyone who has ever bought or sold a house has dealt with the overwhelming piles of paper that constitute a transaction, any of which can be mislaid and interrupt the deal&#x2019;s progress. And has also dealt with the expense of title insurance. A blockchain-based system that captures all relevant documents electronically -- and has the current transaction point back to the previous transaction where the current seller was the buyer -- would improve the system immensely, and make possible new kinds of transactions.</p>\n<p class=\"normal\">You probably are asking yourself what a smart contract is. Simply stated, a smart contract is a transaction -- i.e., a contract -- that is represented in code rather than in a written document. In the example of the Land Registry, the real estate transaction, including money transfer, document exchange, and title search, would be defined and executed by a code-based smart contract.</p>\n<p class=\"normal\"><strong><em>Want to understand smart contracts better? Jumpstart your knowledge by <a href=\"https://bernardgolden.leadpages.co/smartcontracts/\">downloading this free white paper</a></em></strong><strong><em> that describes the four benefits and three challenges of smart contracts. </em></strong></p>\n<p class=\"normal\"><strong>Blockchain Vendors</strong></p>\n<p class=\"normal\">And just as IT users have jumped into blockchain technology with enthusiasm, so too have vendors. IBM is participating in the <a href=\"https://www.hyperledger.org/\">Linux Foundation&#x2019;s Hyperledger</a> project and helped jumpstart it by donating blockchain code. Hyperledger has attracted a large group of sponsors and participants that are interested in building blockchain-based smart contract applications. It also offers <a href=\"https://www-03.ibm.com/press/us/en/pressrelease/49029.wss\">support for blockchain environments on its SoftLayer cloud infrastructure</a>.</p>\n<p class=\"normal\">Microsoft is also engaged with blockchain technology via its <a href=\"https://decentralize.today/with-bletchley-microsoft-brings-cloud-computing-to-the-blockchain-35c36bb52cc0#.a2xq553cn\">Project Bletchley</a>. Bletchley is even more ambitious than Hyperledger, in that it provides a cloud-based execution environment akin to IBM&#x2019;s. But Bletchley goes well past that, in that it offers:</p>\n<p class=\"normalCxSpMiddle\">&#x25CF;&#xA0; A smart contracts marketplace, facilitating easy adoption of the technology</p>\n<p class=\"normalCxSpMiddle\">&#x25CF;&#xA0; A blockchain middleware offering, with connectors to identity management and security functions</p>\n<p class=\"normalCxSpMiddle\">&#x25CF;&#xA0; Something called Cryptlets, which can best be thought of as off-blockchain encrypted functionality necessary to help smart contracts operate. For example, if you have a futures contract that needs to get current pricing for, say, wheat, an obvious problem is where to obtain the price. A cryptlet could provide the price from an encrypted execution engine that would prevent fraud from occurring (since manipulating the wheat price would be an obvious way to defraud one of the parties to the wheat contract).</p>\n<p class=\"normal\">The obvious conclusion you should draw is that if you&#x2019;re not actively studying and experimenting with blockchain, you should get started. The technology has rapidly moved from the quasi-criminal status it was associated with for a number of years to an important initiative by mainstream institutions.</p>\n<p class=\"normal\">I believe that blockchain will become the business equivalent of cloud computing -- a convenient, less expensive offering that disrupts an existing ecosystem. And, just like cloud computing, the earlier you get started the faster you&#x2019;ll start to see results.</p>\n\n<p><em><strong>About the author: Tech visionary Bernard Golden was named by Wired.com as one of the ten most influential people in cloud computing. He is the author/co-author of five books on open source, virtualization, and cloud computing. He is also the author of a <a href=\"https://bernardgolden.leadpages.co/smartcontracts/\">free whitepaper about smart contracts</a>.&#xA0;</strong> </em></p>\t\t \t\t</div>\n\t\t\t\t\t\t\t\t\n\t\t\t\t</div>"
    }, {
        "title": "Build a Swarm Cluster",
        "url": "https://opsnotice.xyz/build-swarm-cluster/",
        "excerpt": "The version 1.12 of Docker has been released few days ago. Among the changes, Docker-Swarm get embedded directly into the Engine that allow easier Swarm deployment. I&#x2019;m going to show that to&hellip;",
        "date_saved": "Sat Jul 30 02:13:33 EDT 2016",
        "html_file": "build_a_swarm_cluster.html",
        "png_file": "build_a_swarm_cluster.png",
        "md_file": "build_a_swarm_cluster.md",
        "content": "<div><header>\n \n \n \n </header>\n\n <section class=\"post-content\">\n <p><img src=\"https://opsnotice.xyz/content/images/2016/07/swarm-cluster.png\" alt=\"\"></p>\n\n<p>The version 1.12 of Docker has been released few days ago. Among the changes, Docker-Swarm get embedded directly into the Engine that allow easier Swarm deployment. I&#x2019;m going to show that to you. </p>\n\n<blockquote>\n <p>Docker Swarm is native clustering for Docker. It turns a pool of Docker hosts into a single, virtual Docker host.</p>\n</blockquote>\n\n<p>Earlier stages, it was quite hard to deploy a Swarm cluster, you had to generate some certs, use a service discovery, configure each node&#x2026; This time is over ! <br>\nToday, we are going to make a Swarm cluster in two commands !</p>\n\n<p>In this post, I&#x2019;ll show you how to make a small cluster between two virtual machines but you can easily repeat operations to make a larger cluster.</p>\n\n<p>I&#x2019;ll use two Ubuntu 16.04 Xenial VM and install Docker 1.12 on each.</p>\n\n<pre><code class=\"language-langage-bash\">export DEBIAN_FRONTEND=noninteractive \nsudo apt-get -y update \nsudo apt-get install apt-transport-https ca-certificates \nsudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D \nsudo echo \"deb https://apt.dockerproject.org/repo ubuntu-xenial experimental\" &gt; /etc/apt/sources.list.d/docker.list \nsudo apt-get -y update \nsudo apt-get purge lxc-docker \nsudo apt-cache policy docker-engine \nsudo apt-get -y install linux-image-extra-$(uname -r) \nsudo apt-get -y install apparmor \nsudo apt-get -y install docker-engine \nsudo service docker start \ndocker -v \n</code></pre>\n\n<p>When your VMs have Docker 1.12 installed on it, we are going to create our Swarm cluster. Launch this command on the VM you want use for Master : </p>\n\n<pre><code class=\"language-langage-docker\">docker swarm init \n</code></pre>\n\n<p>This command will init the cluster and give you, at end, the command to launch on your nodes to join us to the cluster. Launch it on other nodes : </p>\n\n<pre><code class=\"language-langage-docker\">docker swarm join --secret &lt;secret&gt; \\ \n--ca-hash sha256:&lt;hash&gt; \\\nIP_Master_Swarm:2377 \n</code></pre>\n\n<p>With this second command, your node joins directly your Swarm cluster (in two commands, no jokes). You have definitely a Docker Swarm Cluster ! Docker Swarm is too easy since 1.12</p>\n\n<p>For going a bit deeper in Swarm, we&#x2019;ll deploy a service and expose it on our nodes (port 8001/tcp) who will be load-balanced in the cluster. In our example, we&#x2019;re going to use Nginx, but you can use any other image of your choice.</p>\n\n<p>We launch these commands on the Master : </p>\n\n<pre><code class=\"language-langage-docker\">docker network create -d overlay nginx-network \ndocker service create --name nginx --network nginx-network --replicas 5 -p 8001:80/tcp nginx \n</code></pre>\n\n<p><img src=\"https://opsnotice.xyz/content/images/2016/07/nginx-swarm-task.png\" alt=\"\"></p>\n\n<p>With this command, we have created a private network for Nginx and a service (new Docker concept) who expose the port 8001 off our Swarm&#x2019;s nodes and load-balance it to 5 Nginx containers.</p>\n\n<p>After, we can easily scale up our service : </p>\n\n<pre><code class=\"language-langage-docker\">docker service scale nginx=40 \n</code></pre>\n\n<p><img src=\"https://opsnotice.xyz/content/images/2016/07/nginx-scale-40.png\" alt=\"\"></p>\n\n<p>If you want to read more about the new Swarm, <a href=\"https://docs.docker.com/engine/swarm/\">let&#x2019;s RTFM</a>.</p>\n\n<p>For me, this update makes really easier the deployment of a Swarm Cluster from scratch. This is a good point for Docker Inc in the big war or container orchestration (Kubernetes, Mesos, Rancher&#x2026;).</p>\n </section>\n\n </div>"
    }, {
        "title": "Hello, Blog! - An advanced setup of Ghost and Docker made simple",
        "url": "http://coderunner.io/hello-blog-an-advanced-setup-of-ghost-and-docker-made-simple/",
        "excerpt": "So you want to setup a nice new blog with a streamlined development workflow? Great, so did I! After spending some time ironing out a setup that works for me, I thought I'd share it. If you want a&hellip;",
        "date_saved": "Sat Jul 30 02:13:34 EDT 2016",
        "html_file": "hello_blog__an_advanced_setup_of_ghost_and_docker_made_simple.html",
        "png_file": "hello_blog__an_advanced_setup_of_ghost_and_docker_made_simple.png",
        "md_file": "hello_blog__an_advanced_setup_of_ghost_and_docker_made_simple.md",
        "content": "<div><section class=\"post-content\">\n <p>So you want to setup a nice new blog with a streamlined development workflow? Great, so did I! After spending some time ironing out a setup that works for me, I thought I'd share it.</p>\n\n<p><img src=\"http://coderunner.io/content/images/2015/12/ghost-docker.png\" alt=\"Ghost &amp; Docker logos\"></p>\n\n<p>If you want a simple, back-to-basics blogging platform, then <a href=\"https://ghost.org/\">Ghost</a> is a good choice. It is focussed on the content and making it look nice right out the box, and it is powering what you're reading now.</p>\n\n<p>I'll detail my workflow step-by-step so that it you want to do something similar you can follow along, just replace all references to <code>coderunner.io</code> with your own domain :) I'm still making refinements, so leave a comment below with any suggestions!</p>\n\n<p>I have split this up into three parts:</p>\n\n\n\n<h2 id=\"thegoal\">The Goal </h2>\n\n<p>What we're shooting for:</p>\n\n<ul>\n<li>Ability to bring up/down the whole stack with a single command (we'll use <a href=\"https://docs.docker.com/compose/\">Docker Compose</a> for that)</li>\n<li>Front our blog with a <a href=\"https://en.wikipedia.org/wiki/Reverse_proxy\">reverse proxy</a>, because we will be hosting it on a VPS and may want to have other blogs/apps on the same box</li>\n<li>Simple to clone our environment for development (or to migrate to a different host in the future)</li>\n<li>Let us create content first on our local environment and then sync it with our public host</li>\n<li>Automated backup (and restore) to somewhere like <a href=\"https://www.dropbox.com\">Dropbox</a></li>\n</ul>\n\n<p>Sounds good? Let's get started!</p>\n\n<h2 id=\"partisettingupadockerisedinstallationofghostwithmariadb\">Part I: Setting up a Dockerised installation of Ghost with MariaDB</h2>\n\n<h4 id=\"guide\">Guide</h4>\n\n\n\n<h4 id=\"overview\">Overview</h4>\n\n<p>Ghost can be setup with <a href=\"https://www.sqlite.org/\">sqlite3</a> (default) or <a href=\"https://www.mysql.com/\">MySql</a>/<a href=\"https://mariadb.org/\">MariaDB</a>. I decided to use MariaDB so I have a fully featured RDBMS, and as it will fit in nicely to our modular Docker setup. </p>\n\n<p>In this post we'll setup Ghost running in a Docker container, linked to a MariaDB container and fronted by <a href=\"https://www.nginx.com/resources/wiki/\">Nginx</a>. To wire it all together, we'll use Docker Compose.</p>\n\n<blockquote>\n <p>Before getting started, you should have <a href=\"https://docs.docker.com/engine/installation/\">Docker installed</a> and <a href=\"https://docs.docker.com/compose/install/\">Docker Compose setup</a>. We'll set everything up locally and then deploy to a VPS in the next post. </p>\n</blockquote>\n\n<h2 id=\"directorystructure\">Directory Structure</h2>\n\n<p>So that it is clear up-front, this is the directory structure we'll be putting together: </p>\n\n<pre><code class=\"language-bash\">.\n|-- data-coderunner.io\n| |-- config.js #[gist](https://gist.github.com/bennetimo/6ddb288bf645abf76b38/ed4f50cd4acda83f540e12bc6b7bb3267ea18d93)\n| |-- Dockerfile #[gist](https://gist.github.com/bennetimo/0ab18d783557438c6145)\n| `-- env_coderunner.io\n`-- docker-compose.yml #[gist](https://gist.github.com/bennetimo/91ca871c3aaa2e7a148a)\n\n1 directory, 4 files \n</code></pre>\n\n<p>We'll build up each one as we go, but I've added the gists so you can see the final result if you want.</p>\n\n<h2 id=\"creatingadataonlycontainer\">Creating a Data Only container</h2>\n\n<p>The power of Docker comes from composing together single purpose containers to create your application. To fully embrace this we'll create a <a href=\"http://container42.com/2013/12/16/persistent-volumes-with-docker-container-as-volume-pattern/\">data only container</a> just to hold our data, and nothing more. Then we will be able to easily link the data volumes into any containers that need to access it, whether that's our Ghost container, backup container, or something else.</p>\n\n<p>Here is the <code>Dockerfile</code> for our data container, which lives in the sub-directory <code>data-coderunner.io</code>. </p>\n\n<pre><code class=\"language-docker\">FROM ghost \nMAINTAINER Tim Bennett &lt;tim@coderunner.io&gt;\n\n# Create required volumes\nVOLUME [\"/var/lib/mysql\", \"/var/lib/ghost\"]\n\nENTRYPOINT [\"/bin/bash\"] \n</code></pre>\n\n<p>It's pretty uninteresting, we just inform Docker that we want to mount the mysql and ghost directories. I'm basing off the Ghost image so it reuses the same layers as the Ghost container that we'll need later, to alleviate <a href=\"http://container42.com/2014/11/18/data-only-container-madness/\">container madness</a>.</p>\n\n<blockquote>\n <p>As of Docker 1.9.0 there is a new <a href=\"https://docs.docker.com/engine/reference/commandline/volume_create/\">Volumes API</a> which it would be nice to use here, but it is <a href=\"https://github.com/docker/compose/issues/2110\">not yet supported in Docker Compose</a>. </p>\n</blockquote>\n\n<h2 id=\"builditwithdockercompose\">Build it with Docker Compose</h2>\n\n<p>Now we want to build our data container, but instead of doing it manually we'll do it with <a href=\"https://docs.docker.com/compose/\">Docker Compose</a>, by creating a <code>docker-compose.yml</code> file at the top level of the directory:</p>\n\n<pre><code class=\"language-yaml\">data-coderunner.io: \n build: ./data-coderunner.io\n container_name: \"data-coderunner.io\"\n</code></pre>\n\n<p>In this file we'll be declaratively listing all of the components that make up our stack and how they link together. </p>\n\n<p>The <a href=\"https://docs.docker.com/compose/compose-file/#build\">build</a> directive will create our data container, and we name it so we can refer back to it later.</p>\n\n<blockquote>\n <p>It's also possible to setup the data-container <a href=\"http://stackoverflow.com/questions/32908621/how-can-i-create-a-data-container-only-using-docker-compose-yml\">directly</a> in Docker-Compose, but I prefer this approach</p>\n</blockquote>\n\n<h2 id=\"setupourmariadbcontainer\">Setup our MariaDB container</h2>\n\n<p>There's an officially supported <a href=\"https://hub.docker.com/_/mariadb/\">image</a> for MariaDB which makes our lives easy. </p>\n\n<p>All we need to do is add it to our docker-compose.yml: </p>\n\n<pre><code class=\"language-yaml\">mariadb: \n image: mariadb\n container_name: \"mariadb\"\n env_file: ./data-coderunner.io/env_coderunner.io\n environment:\n - TERM=xterm\n ports:\n - \"127.0.0.1:3306:3306\"\n volumes_from:\n - data-coderunner.io\n</code></pre>\n\n<p>There's a few things going on here. <code>volumes-from</code> references the data container we just created, so that MariaDB will be using the <code>/var/lib/mysql</code> mount point that we setup.</p>\n\n<p>The ports mapping will bind port 3306 on our host to 3306 in the container, and we bind it for the loopback interface only. Otherwise we would be able to connect directly to the database container from the outside, but we want our only outside entry point to be our proxy that we'll create shortly. </p>\n\n<p>We also specified an <code>env_file</code> with our db configuration: </p>\n\n<pre><code class=\"language-bash\"># MariaDB configuration\nMYSQL_ROOT_PASSWORD=&lt;REDACTED&gt; \nMYSQL_USER=tim \nMYSQL_PASSWORD=&lt;REDACTED&gt; \nMYSQL_DATABASE=blog \n</code></pre>\n\n<p>Finally, I'm setting the TERM environment variable so I can use the mysql tool to connect to our database, if needed.</p>\n\n<h2 id=\"setupghost\">Setup Ghost</h2>\n\n<p>Next up we need to actually add Ghost, and we have an <a href=\"https://hub.docker.com/_/ghost/\">official image</a> for that too, awesome!</p>\n\n<pre><code class=\"language-yaml\">blog-coderunner.io: \n image: ghost\n container_name: \"blog-coderunner.io\"\n volumes:\n - ./data-coderunner.io/config.js:/var/lib/ghost/config.js\n volumes_from:\n - data-coderunner.io\n env_file: ./data-coderunner.io/env_coderunner.io\n links:\n - mariadb:mysql\n</code></pre>\n\n<p>The only new things here are <code>volumes</code> and <code>links</code>, so let's just take a look at those.</p>\n\n<p>Ghost uses a config.js file for <a href=\"http://support.ghost.org/config/\">configuration</a>, and the Docker image will create one that can then be modified as needed. But we want everything to be dynamic, and pick up the fact that we're using MariaDB instead of sqlite automagically!</p>\n\n<p>So, we'll use our own config.js that figures everything out using the environment variables from the containers. Here is the <a href=\"https://gist.github.com/bennetimo/6ddb288bf645abf76b38\">gist</a>. Then, in the <code>volumes</code> section, we mount it straight to where Ghost is expecting it in the container.</p>\n\n<p>Now we just need to add this to our <code>env_coderunner.io</code> file: </p>\n\n<pre><code class=\"language-bash\"># Ghost configuration\nURL=http://coderunner.io \n</code></pre>\n\n<p>This is picked up in the config.js to configure the URL for Ghost.</p>\n\n<p>The <code>links</code> entry tells Docker to create a tunnel between our containers by adding a <code>mysql</code> entry to the <code>/etc/hosts</code> file; Now our blog container can talk to our mysql container.</p>\n\n<p>At this point we could fire up our blog, but we wouldn't be able to access it from our local machine as we're not exposing the ports. We will go one better than exposing the Ghost port directly, and setup <a href=\"https://www.nginx.com/resources/wiki/\">nginx</a>.</p>\n\n<h2 id=\"putitallbehindnginx\">Put it all behind nginx</h2>\n\n<p>By setting everything up behind an nginx reverse proxy, we can have multiple services (applications, other blogs etc) running on a single box and have nginx handling traffic routing between them. We could set this up manually, but there is already an awesome out-the-box Docker setup in <a href=\"https://hub.docker.com/r/jwilder/nginx-proxy/\">jwilder/nginx-proxy</a>. </p>\n\n<p>Now we're really starting to see the magic and power of Docker. We're building our application by sticking together components like making a house out of lego bricks! </p>\n\n<div class=\"attributed-image\"> \n<img src=\"https://farm9.static.flickr.com/8507/8505316460_78d0abaf5b_b.jpg\" alt=\"FlickrFriday: Keep it Simple. by elPadawan, on Flickr\" title=\"\"> \n<a href=\"https://www.flickr.com/photos/elpadawan/8505316460/\">Photo</a> \n by elPadawan / <a href=\"http://creativecommons.org/licenses/by/2.0/\">CC BY</a>\n</div>\n\n<p>So we add this to our <code>docker-compose.yml</code>:</p>\n\n<pre><code class=\"language-yaml\">nginx: \n image: jwilder/nginx-proxy\n container_name: \"nginx\"\n ports: \n - \"80:80\"\n volumes:\n - /var/run/docker.sock:/tmp/docker.sock\n</code></pre>\n\n<p>And that's all we need to create a fully-fledged reverse proxy! Now we just need to tell it the hostname that will map to our blog, by adding an environment variable to the blog container: </p>\n\n<pre><code class=\"language-yaml\">environment: \n - VIRTUAL_HOST=coderunner.io\n</code></pre>\n\n<h2 id=\"startitup\">Start it up!</h2>\n\n<p>In the main blog directory:</p>\n\n<pre><code>docker-compose up \n</code></pre>\n\n<p>And we're running!</p>\n\n<blockquote>\n <p>On the very first launch the Ghost container might try to connect to MariaDB before it's finished setting up the database. To avoid it you can start MariaDB separately first with <code>docker-compose up -d mariadb</code>, or by using my <a href=\"https://hub.docker.com/r/bennetimo/ghost-wait-mysql/\">modified image</a>. See <a href=\"https://github.com/docker/compose/issues/374\">here</a> for more info.</p>\n</blockquote>\n\n<p>At the moment everything is on our local machine, so we can add an entry to <code>/etc/hosts</code> to simulate the domain setup.</p>\n\n<pre><code class=\"language-bash\">localhost coderunner.io \n</code></pre>\n\n<blockquote>\n <p>If you're using <a href=\"https://docs.docker.com/machine/\">docker-machine</a> then you want to use the IP of the virtual machine which you can find with <code>docker-machine ip default</code></p>\n</blockquote>\n\n<p>Now we can fire up a browser and visit <a href=\"http://coderunner.io\">http://coderunner.io</a>, and we're greeted with Ghost:</p>\n\n<p><img src=\"http://coderunner.io/content/images/2015/12/Screen-Shot-2015-12-20-at-13-37-25.png\" alt=\"ghost-welcome-page\"></p>\n\n<p>We now have a Ghost blog running, linked to a MariaDB container, and fronted by an Nginx reverse proxy, all running in Docker containers. Nice! </p>\n\n<p>But, at the moment we're just running locally. In the <a href=\"http://coderunner.io/deploying-ghost-on-digital-ocean-with-docker-compose\">next post</a>, we'll move this to a VPS on DigitalOcean so we're publicly accessible.</p>\n </section>\n\n </div>"
    }, {
        "title": "Moving to Docker: NGINX reverse proxy with SSL termination",
        "url": "https://www.timofejew.com/moving-to-docker-nginx/",
        "excerpt": "Now that I have Ghost running in a Docker container, it's time to move the NGINX reverse proxy from the host environment into a Docker container as well. I'll be pretty much using the same techniques&hellip;",
        "date_saved": "Sat Jul 30 02:13:36 EDT 2016",
        "html_file": "moving_to_docker_nginx_reverse_proxy_with_ssl_termination.html",
        "png_file": "moving_to_docker_nginx_reverse_proxy_with_ssl_termination.png",
        "md_file": "moving_to_docker_nginx_reverse_proxy_with_ssl_termination.md",
        "content": "<div><section class=\"post-content\">\n <p>Now that I have <a href=\"https://ghost.org\">Ghost</a> <a href=\"https://www.timofejew.com/moving-to-docker-ghost/\">running</a> in a <a href=\"https://www.docker.com\">Docker</a> container, it's time to move the <a href=\"http://nginx.org\">NGINX</a> <a href=\"https://en.wikipedia.org/wiki/Reverse_proxy\">reverse proxy</a> from the host environment into a Docker container as well. I'll be pretty much using the same techniques as I wrote in the <a href=\"https://www.timofejew.com/nginx-ghost-stopping-hotlink-images/\">image hot linking article</a>, updated slightly to incorporate the latest <a href=\"https://en.wikipedia.org/wiki/Transport_Layer_Security\">TLS</a> <a href=\"https://weakdh.org/sysadmin.html\">security configuration</a>.</p>\n\n<h2 id=\"research\">Research</h2>\n\n<p>Since I wanted to use Docker <a href=\"https://docs.docker.com/userguide/dockerlinks/\">container linking</a> to automate the linkage of the containers, I thought I'd do some poking around the Internet to see what others have done. That, and see what the latest-and-greatest NGINX configurations for security and reverse proxying are.</p>\n\n<h3 id=\"security\">Security</h3>\n\n<p>The first thing I stumbled across was a server-side <a href=\"https://mozilla.github.io/server-side-tls/ssl-config-generator/\">TLS configuration generator</a> from the Mozilla foundation. What's really slick about this is that it has codified best practices for different servers, and for different levels of browser compatibility. Really nice.</p>\n\n<p>I also discovered an <a href=\"https://www.ssllabs.com/ssltest/index.html\">online SSL server test</a> from <a href=\"https://www.ssllabs.com/index.html\">Qualsys SSL Labs</a>. Once you have your server up and running, you can test your configuration with their tool, and it will supply recommendations for improving your configuration. For what it's worth, this site gets an A+ rating (as of the published date).</p>\n\n\n\n<p>I found a <a href=\"http://jasonwilder.com/blog/2014/03/25/automated-nginx-reverse-proxy-for-docker/\">great resource</a> about linking containers at <a href=\"http://jasonwilder.com\">Jason Wilder's blog</a>. It covers my use case exactly. What he's done is to create a Docker image, <a href=\"http://github.com/jwilder/nginx-proxy\">jwilder/nginx-proxy</a> that uses an underlying tool he created, <a href=\"http://github.com/jwilder/docker-gen\">jwilder/docker-gen</a>, which in turn uses the Docker API to monitor container events, and automatically generate new configurations on the fly as containers start and stop. It's all seriously slick, and looks like it would make a very robust production environment.</p>\n\n<p>However, it's a bit involved from a configuration point of view (it's using a template language to generate NGINX config files), and relies on Docker's API (which is still not 100% mature yet). For now, I'm going to keep it simple and use basic Docker linking. If the Ghost container starts to regularly fall over (which it doesn't in practice), I'll consider diving in and using his system.</p>\n\n<h2 id=\"shuttingdowninstallation\">Shutting down installation</h2>\n\n<p>First things first, I stopped NGINX:</p>\n\n<pre><code class=\"language-bash\">sudo service nginx stop \n</code></pre>\n\n<p>And then removed all traces of NGINX:</p>\n\n<pre><code class=\"language-bash\">sudo apt-get remove -y nginx \nsudo apt-get --purge -y autoremove \n</code></pre>\n\n<p>Now I'm committed...</p>\n\n<h2 id=\"systempreparation\">System preparation</h2>\n\n<p>Assuming you're following along in this series, you'll have already installed <code>docker</code> on your system. If you haven't, please see the <a href=\"https://www.timofejew.com/moving-to-docker-ghost/\">first part</a> of this series for instructions.</p>\n\n<h2 id=\"dockerimagecreation\">Docker image creation</h2>\n\n<p>Since a couple of the machines on <code>timofejew.com</code> will be running reverse proxies, I decided that what I would do is create a separate image for each proxy. In this case, it's tuned to run as a front-end to my Ghost blog. Here's the <code>Dockerfile</code> I created (I'm substituting <code>example.com</code>):</p>\n\n<pre><code># Create proxy container for www.example.com\n#\n# docker build -t proxy .\n\nFROM nginx\n\nMAINTAINER Peter Timofejew &lt;peter@timofejew.com&gt;\n\n# Set timezone\nRUN echo \"America/Toronto\" &gt; /etc/timezone \\ \n &amp;&amp; dpkg-reconfigure -f noninteractive tzdata\n\n# Install wget and install/updates certificates\nRUN apt-get update \\ \n &amp;&amp; apt-get install -y -q --no-install-recommends \\\n ca-certificates \\\n wget \\\n &amp;&amp; apt-get clean \\\n &amp;&amp; rm -r /var/lib/apt/lists/*\n\n# Add main NGINX config\nCOPY nginx.conf /etc/nginx/\n\n# Add DH params (generated with openssl dhparam -out dhparams.pem 2048)\nCOPY ssl/dhparams.pem /etc/ssl/private/\n\n# Add www certificates\nCOPY ssl/ssl-ca-certs-startssl.pem /etc/ssl/certs/ \nCOPY ssl/ssl-cert-chain-www-example-com.pem /etc/ssl/certs/ \nCOPY ssl/ssl-cert-www-example-com.crt /etc/ssl/private/ \nCOPY ssl/ssl-cert-www-example-com.key /etc/ssl/private/\n\n# Add virtual hosts\nCOPY vhosts/ /etc/nginx/conf.d/\n\n# Add static content\nCOPY html/ /usr/share/nginx/html/ \n</code></pre>\n\n<p>As you can see from examining the <code>Dockerfile</code>, there are three directories with auxiliary files:</p>\n\n<ol>\n<li><code>ssl/</code> - this contains the <a href=\"https://en.wikipedia.org/wiki/Certificate_authority\">CA certificate</a>, a <a href=\"http://security.stackexchange.com/questions/59566/ssl-certificate-chain-verification\">chained SSL certificate</a> for the host, the SSL key for the host, and a <a href=\"https://weakdh.org/sysadmin.html\">DH parameter</a> file. </li>\n<li><code>vhosts/</code> - in here are two config files for the two virtual hosts the container will handle (<code>example.com</code> and <code>www.example.com</code>). </li>\n<li><code>html/</code> - static content served up by the container.</li>\n</ol>\n\n<p><img src=\"https://www.timofejew.com/content/images/2015/09/Executie-Decision.jpg\" alt=\"Business Cat!\"></p>\n\n<p>I made an executive decision to not mount the host's filesystem, but to keep all the data in the container itself. Here's my rationale:</p>\n\n<ul>\n<li>The TLS information is relatively static. This will have to be updated in a year's time, but re-generating the image and launching a new container isn't that big of a deal, and it will be a good excuse to upgrade to the latest version of NGINX and OpenSSL (which will automatically happen when the container is rebuilt).</li>\n<li>The virtual hosts configuration is also pretty much static. I may modify this, but I doubt it. It's stable.</li>\n<li>Same with the static data. I may on occasion add something else to it, but it's probably better that I add it to the <code>git</code> repo, and then rebuild the image.</li>\n<li>And lastly, this is not a general purpose proxy. If it was, I'd probably mount the keys, configuration, and static content from the host filesystem.</li>\n</ul>\n\n<p>With that in mind, here's some of the key files:</p>\n\n<h3 id=\"nginxconf\">nginx.conf</h3>\n\n<pre><code class=\"language-nginx\">user www-data; \nworker_processes 1;\n\nerror_log /var/log/nginx/error.log warn; \npid /var/run/nginx.pid;\n\nevents { \n worker_connections 1024;\n}\n\nhttp { \n include /etc/nginx/mime.types;\n default_type application/octet-stream;\n\n log_format main '$remote_addr - $remote_user [$time_local] \"$request\" '\n '$status $body_bytes_sent \"$http_referer\" '\n '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n access_log /var/log/nginx/access.log main;\n\n sendfile on;\n #tcp_nopush on;\n\n keepalive_timeout 65;\n\n gzip on;\n gzip_disable \"msie6\";\n gzip_vary on;\n gzip_proxied any;\n gzip_comp_level 6;\n gzip_buffers 16 8k;\n gzip_http_version 1.1;\n gzip_types text/plain text/css application/json application/x-javascript\n text/xml application/xml application/xml+rss text/javascript;\n\n # SSL intermediate configuration from https://mozilla.github.io/server-side-tls/ssl-config-generator/\n ssl_certificate /etc/ssl/certs/ssl-cert-chain-www-example-com.pem;\n ssl_certificate_key /etc/ssl/private/ssl-cert-www-example-com.key;\n ssl_dhparam /etc/ssl/private/dhparams.pem;\n ssl_session_timeout 1d;\n ssl_session_cache shared:SSL:50m;\n\n ssl_protocols TLSv1 TLSv1.1 TLSv1.2;\n ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA';\n ssl_ecdh_curve secp384r1;\n ssl_prefer_server_ciphers on;\n\n # HSTS (ngx_http_headers_module is required) (15768000 seconds = 6 months)\n add_header Strict-Transport-Security \"max-age=15768000; includeSubdomains\";\n\n # OCSP Stapling ---\n # fetch OCSP records from URL in ssl_certificate and cache them\n ssl_stapling on;\n ssl_stapling_verify on;\n ssl_trusted_certificate /etc/ssl/certs/ssl-ca-certs-startssl.pem;\n\n include /etc/nginx/conf.d/*.conf;\n}\n</code></pre>\n\n<p>Although it may be more common to put the SSL configuration in a server configuration block, I decided to move it to the main http block since this is a proxy only for two virtual hosts that share the same SSL certificate. Otherwise, the only potentially odd thing I've done is set the <code>user</code> to <code>www-data</code>. This was done as the host OS is Ubuntu, and the default user id generated by the base NGINX image shares the same id as <code>sshd</code>, which disturbed me. At least <code>www-data</code> is web related, and has no write permissions anywhere in the container, let alone the host system.</p>\n\n<h3 id=\"vhostdefaultconf\">vhost/default.conf</h3>\n\n<pre><code class=\"language-nginx\">server { \n listen 80 default_server;\n listen [::]:80 default_server;\n listen 443 default_server ssl;\n listen [::]:443 default_server ssl;\n\n server_name example.com;\n\n root /usr/share/nginx/html;\n index index.html;\n\n # force TLS\n if ($scheme = http) {\n return 301 https://$server_name$request_uri;\n }\n\n # try to get files from $root, otherwise redirect to Ghost\n location / {\n try_files $uri $uri/ =404;\n error_page 403 = @ghost;\n error_page 404 = @ghost;\n }\n\n # ghost blog\n location @ghost {\n rewrite ^ $scheme://www.example.com$uri redirect;\n }\n}\n</code></pre>\n\n<p>I go into more detail for this and the other vhost configuration in the article <a href=\"https://www.timofejew.com/nginx-ghost-stopping-hotlink-images/\">NGINX, Ghost, and host-linked images</a>.</p>\n\n<h3 id=\"vhostwwwconf\">vhost/www.conf</h3>\n\n<pre><code class=\"language-nginx\">server { \n listen 80;\n listen [::]:80;\n listen 443 ssl;\n listen [::]:443 ssl;\n\n server_name www.example.com;\n\n root /usr/share/nginx/html;\n index index.html;\n\n # force TLS\n if ($scheme = http) {\n return 301 https://$server_name$request_uri;\n }\n\n # prevent hotlinking\n # test http://www.htaccesstools.com/test-hotlink-protection/\n location ~* \\.(gif|png|jpe?g)$ {\n valid_referers none blocked www.example.com;\n if ($invalid_referer) {\n return 444;\n }\n try_files $uri @ghost;\n }\n\n # try to get files from $root, otherwise redirect to Ghost\n location / {\n try_files $uri @ghost;\n }\n\n # proxy to Ghost (assumes 'ghost' is in /etc/hosts)\n location @ghost {\n proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n proxy_set_header Host $http_host;\n proxy_set_header X-Forwarded-Proto $scheme;\n proxy_pass http://ghost:2368;\n }\n}\n</code></pre>\n\n<p>The main difference between this config and the one in the hot-linking article, is that <code>proxy_pass</code> parameter references a host named <code>ghost</code>. This is some magic supplied by the Docker <code>run</code> <code>--link</code> argument. What happens is that the container name of a linked container is put into <code>/etc/hosts</code> of the NGINX container. I'll describe this more in the next section.</p>\n\n<h2 id=\"deployment\">Deployment</h2>\n\n<p>To stand this up on <code>www.timofejew.com</code>, I ended up created two images. An image called <code>proxy</code> (built from the <code>Dockerfile</code> above), and another called <code>blog</code>, which is built from the following <code>Dockerfile</code>:</p>\n\n<pre><code># Create Ghost container for www.example.com\n#\n# docker build -t blog .\n\nFROM ptimof/ghost\n\nMAINTAINER Peter Timofejew &lt;peter@timofejew.com&gt;\n\n# Set timezone\nRUN echo \"America/Toronto\" &gt; /etc/timezone \\ \n &amp;&amp; dpkg-reconfigure -f noninteractive tzdata\n\n# install Postfix for forwarding\nRUN apt-get update \\ \n &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y \\\n postfix \\\n libsasl2-modules \\\n &amp;&amp; apt-get autoremove -y \\\n &amp;&amp; apt-get clean\n\n# Add in real values for example.com\nENV GHOST_URL https://www.example.com \nENV MAIL_FROM '\"Webmaster\" &lt;webmaster@www.example.com&gt;' \nENV MAIL_HOST localhost\n\n# Set up Postfix\nRUN sed -i -e \"/^mydestination/s/^.*$/mydestination = /\" /etc/postfix/main.cf \\ \n &amp;&amp; sed -i -e \"/^relayhost/s/^/#/\" /etc/postfix/main.cf \\\n &amp;&amp; sed -i -e \"/^#myorigin/s/^#//\" /etc/postfix/main.cf \\\n &amp;&amp; sed -i -e \"/^myhostname/s/^.*$/myhostname = www.example.com/\" /etc/postfix/main.cf \\\n &amp;&amp; echo \"www.example.com\" &gt; /etc/mailname \\\n &amp;&amp; echo \"inet_protocols = ipv4\" &gt;&gt; /etc/postfix/main.cf \\\n &amp;&amp; echo \"relayhost = [mail.example.com]:587\" &gt;&gt; /etc/postfix/main.cf \\\n &amp;&amp; echo \"smtp_tls_security_level = verify\" &gt;&gt; /etc/postfix/main.cf \\\n &amp;&amp; echo \"smtp_tls_CAfile = /etc/ssl/certs/ca-certificates.crt\" &gt;&gt; /etc/postfix/main.cf \\\n &amp;&amp; echo \"smtp_sasl_auth_enable = yes\" &gt;&gt; /etc/postfix/main.cf \\\n &amp;&amp; echo \"smtp_sasl_password_maps = hash:/etc/postfix/relay_password\" &gt;&gt; /etc/postfix/main.cf \\\n &amp;&amp; echo \"smtp_sasl_tls_security_options =\" &gt;&gt; /etc/postfix/main.cf \\\n &amp;&amp; echo \"[mail.example.com]:587 www-host@example.com:secret-password\" &gt; /etc/postfix/relay_password \\\n &amp;&amp; chmod 600 /etc/postfix/relay_password \\\n &amp;&amp; postmap /etc/postfix/relay_password\n\n# Add startup shell\nCOPY ghost.sh /\n\n# Run in production\nENTRYPOINT [ \"/ghost.sh\" ] \nCMD [ \"npm\", \"start\", \"--production\" ] \n</code></pre>\n\n<p>You'll notice that's a bit different from the first article in this series, <a href=\"https://www.timofejew.com/moving-to-docker-ghost/\">Moving to Docker: Ghost</a>. Mainly, I wanted to move the configuration from a file on the host, to burning it right into a Docker image, enable <a href=\"https://www.timofejew.com/adventures-in-emailing/\">authenticated email to my email server</a>, and setting up the default command to run in production mode.</p>\n\n<p>And the <code>ghost.sh</code> script is fairly simple:</p>\n\n<pre><code class=\"language-bash\">#!/bin/bash\n\n# start postfix\nservice postfix start\n\n# start ghost\nexec /entrypoint.sh $@ \n</code></pre>\n\n<p>Once the two images were built, I ran them as follows:</p>\n\n<pre><code class=\"language-bash\">docker run --name ghost -v /var/lib/ghost:/var/lib/ghost \\ \n --restart=always -m 150m --memory-swap 300m \\\n --log-driver=json-file --log-opt max-size=1m \\\n -h www.example.com \\\n -d blog\ndocker run --name proxy --link ghost \\ \n -p 80:80 -p 443:443 --restart=always -m 75m \\\n --log-driver=json-file --log-opt max-size=10m \\\n -d proxy\n</code></pre>\n\n<p>This will create two <em>containers</em>, <code>ghost</code> and <code>proxy</code><sup id=\"fnref:1\"><a href=\"https://www.timofejew.com/moving-to-docker-nginx/#fn:1\" rel=\"footnote\">1</a></sup>, and pass the config information from <code>ghost</code> to <code>proxy</code> via the link.</p>\n\n<p>What's neat about the <code>--link</code> command, is that it will push a bunch of environment variables that describe <code>ghost</code> into <code>proxy</code>, as well as creating a host entry for <code>ghost</code> into <code>/etc/hosts</code> in <code>proxy</code>. That's how the <code>proxy_pass</code> line of <code>/etc/nginx/conf.d/www.conf</code> knows what IP to forward requests to. Interestingly, the port is available to <code>proxy</code> via an environment variable, but since NGINX config files can't directly access environment variables, I've hardcoded it to <code>2368</code> (which is what the <code>ghost</code> container exposes).</p>\n\n<p>And this is where Jason Wilder's scheme really shines. It will update the NGINX config to reflect the Ghost container's config exactly, no matter how it's launched. If I was running a more complex setup than a simple two container config, I'd certainly seriously consider using his technique.</p>\n\n<h2 id=\"finalthoughts\">Final thoughts</h2>\n\n<p>What's really cool about using container linking is that the <code>ghost</code> container is only listening on it's own virtual interface - no port collisions if I decide to run another <code>ghost</code> container. And this virtual interface is automatically exported to the <code>proxy</code> container when it links it.</p>\n\n<p>There are two downsides, though, to using this technique:</p>\n\n<ol>\n<li>If the <code>ghost</code> container restarts, it may not have the same IP address. The <code>proxy</code> container will get an updated <code>/etc/hosts</code> entry, but NGINX won't be aware of it, and will need to be reloaded. This is one nice benefit of using Jason Wilder's technique: it will detect the change and kick NGINX. At the moment, I'm betting that the <code>ghost</code> container will not restart. </li>\n<li>Container linking is not compatible with <code>--net=host</code>. There is a performance hit to the <code>proxy</code> container because it needs to go through a couple of network stacks (the host IP stack, bridging to the Docker network, and then the container's stack). If <code>timofejew.com</code> was a high-volume site on the Internet, I wouldn't use linking. But I'm not worried...</li>\n</ol>\n\n<p>And finally, logs. I'm not running any log analysis, so the logs from both containers are just being rotated after they hit a size limit. This is good enough for debugging any problems, and will stop the logs from growing out of control.</p>\n\n<h2 id=\"nextsteps\">Next steps</h2>\n\n<p>Now that the services on <code>www.timofejew.com</code> are containerized, there's a couple things I'd like to try:</p>\n\n<ul>\n<li>Develop a backup/restore technique for dealing with Docker volumes. Once this is done, <code>www.timofejew.com</code> will be \"stateless\", in the broad sense of the term: I should be able to destroy and re-create the machine without data loss.</li>\n<li>Experiment with using <a href=\"https://en.wikipedia.org/wiki/CoreOS\">CoreOS</a> as the host operating system. If I'm moving to all services in containers, this seems to be the way to go. But I'll need to mess about with it for a bit first.</li>\n</ul>\n\n\n\n\n </section>\n\n </div>"
    }, {
        "title": "Now with Built-in Orchestration!",
        "url": "https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/",
        "excerpt": "Three&#xA0;years ago, Docker made an esoteric Linux kernel technology called containerization simple and accessible to everyone.&#xA0;Today, we are doing the same for container orchestration.&hellip;",
        "date_saved": "Sat Jul 30 02:13:37 EDT 2016",
        "html_file": "now_with_builtin_orchestration.html",
        "png_file": "now_with_builtin_orchestration.png",
        "md_file": "now_with_builtin_orchestration.md",
        "content": "<div><div class=\"entry-content\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<p>Three&#xA0;years ago, Docker made an esoteric Linux kernel technology called containerization simple and accessible to everyone.&#xA0;Today, we are doing the same for container orchestration.</p>\n<p>Container orchestration is what is needed to transition from deploying containers individually on a single host, to deploying complex multi-container apps on many machines. It requires a distributed platform, independent from infrastructure, that stays online through the entire lifetime of your application, surviving hardware failure and software updates. Orchestration is at the same stage today as containerization was 3 years ago.&#xA0;There are two options: either you need an army of technology experts to cobble together a complex ad hoc system, or you have to rely on a company with a lot of experts to take care of everything for you as long as you buy all hardware, services, support, software from them.&#xA0;There is a word for that, it&#x2019;s called lock-in.</p>\n<p>Docker users have been sharing with us that neither option is acceptable. Instead, you need a platform that makes orchestration usable by everyone, without locking you in. Container orchestration would be easier to implement, more portable, secure, resilient, and faster if it was built into the platform.</p>\n<p>Starting with Docker 1.12, we have added features to the core Docker Engine to make multi-host and multi-container orchestration easy. We&#x2019;ve added new API objects, like Service and Node, that will let you use the Docker API to deploy and manage apps on a group of Docker Engines called a swarm. With Docker 1.12, the best way to orchestrate Docker is Docker!</p>\n<a href=\"https://www.docker.com/getdocker\"><img title=\"\" src=\"http://img.scoop.it/q5ZcSVcr06OuoljzvFbsvrnTzqrqzN7Y9aBZTaXoQ8Q=\" alt=\"\"></a>&#xA0;\n<p>The Docker 1.12 design is based on four principles:</p>\n<ul class=\"indented\">\n<li><span>Simple</span><span>&#xA0;Yet Powerful</span>&#xA0;&#x2013; Orchestration is a central part of modern distributed applications; it&#x2019;s so central that we have seamlessly built it into our core Docker Engine. Our approach to orchestration follows our philosophy about containers: no setup, only a small number of simple concepts to learn, and an &#x201C;it just works&#x201D; user experience.</li>\n<li><span>Resilient</span>&#xA0;&#x2013; Machines fail all the time. Modern systems should expect these failures to occur regularly and adapt without any application downtime that&#x2019;s why a zero single-point-of-failure design is a must.</li>\n<li><span>Secure</span>&#xA0;&#x2013; Security should be the default.&#xA0;Barriers to strong security &#x2014; certificate generation, having to understand PKI &#x2014; should be removed. But advanced users should still be able to control and audit every aspect of certificate signing and issuance.</li>\n<li><span>Optional Features and Backward Compatibility</span>&#xA0;&#x2013; With millions of users, preserving backwards compatibility is a must for Docker Engine.&#xA0;All new features are optional, and you don&#x2019;t incur any overhead (memory, cpu) if you don&#x2019;t use them. Orchestration in Docker Engine aligns with our platform&#x2019;s batteries included but swappable approach allowing users to continue using any third-party orchestrator that is built on Docker Engine.</li>\n</ul>\n\n\n<p>Let&#x2019;s take a look at how the new features in Docker 1.12 work.</p>\n\n<h3>Creating Swarms with One Decentralized Building Block</h3>\n<p>It all starts with creating a swarm&#x2013;a self-healing group of engines&#x2013;which for the bootstrap node is as simple as:</p>\n<p>docker swarm init</p>\n<p>Under the hood this creates a <a href=\"https://raft.github.io/raft.pdf\">Raft</a>&#xA0;consensus group of one node. This first node has the role of manager, meaning it accepts commands and schedule tasks. As you join more nodes to the swarm, they will by default be workers, which simply execute containers dispatched by the manager. You can optionally add additional manager nodes.&#xA0;The manager nodes will be part of the Raft consensus group. We use an optimized Raft store in which reads are serviced directly from memory&#xA0;which makes scheduling performance fast.</p>\n\n<h3>Creating and Scaling Services</h3>\n<p>Just as you run a single container with docker run, you can now start a replicated, distributed, load balanced process on a swarm of Engines with docker service:</p>\n<p>docker service create &#x2013;name frontend &#x2013;replicas 5 -p 80:80/tcp nginx:latest</p>\n<p>This command declares a desired state on your swarm&#xA0;of 5 nginx containers, reachable as a single, internally load balanced service on port 80 of any node in your swarm. Internally, we make this work using <a href=\"http://www.linuxvirtualserver.org/software/ipvs.html\">Linux IPVS</a>,&#xA0;an in-kernel Layer 4 multi-protocol load balancer that&#x2019;s been in the Linux kernel for more than 15 years. With IPVS routing packets inside the kernel, swarm&#x2019;s routing mesh delivers high performance container-aware load-balancing.</p>\n<p>When you create services, you can optionally create replicated or global services. Replicated services mean any number of containers that you define will be spread across the available hosts.&#xA0;Global services, by contrast, schedule one instance the same container on every host in the swarm.</p>\n<p>Let&#x2019;s turn to how Docker provides resiliency.&#xA0;Swarm mode enabled engines are self-healing, meaning that they are aware of the application you defined and will continuously check and reconcile the environment when things go awry.&#xA0;For example, if you unplug one of the machines running an nginx instance, a new container will come up on another node.&#xA0;Unplug&#xA0;the network switch for half the machines in your swarm, and the other half will take over, redistributing the containers amongst themselves. For updates, you now have flexibility in how you re-deploy services once you make a change. You can set a rolling or parallel update of the containers on your swarm.</p>\n<p>Want to scale up to 100 instances? It&#x2019;s as simple as:</p>\n<p>docker service scale frontend=100</p>\n<p>A typical two-tier (web+db) application would be created like this:</p>\n<p>docker network create -d overlay mynet<br>\ndocker service create &#x2013;name frontend &#x2013;replicas 5 -p 80:80/tcp \\<br>\n&#x2013;network mynet mywebapp<br>\ndocker service create &#x2013;name redis &#x2013;network mynet redis:latest</p>\n<p>This is the basic architecture of this application:</p>\n<a href=\"https://www.docker.com/getdocker\"><img title=\"\" src=\"http://img.scoop.it/bdEKWLMnOnahqAX3rCPOQrnTzqrqzN7Y9aBZTaXoQ8Q=\" alt=\"\"></a>&#xA0;\n<h3>Security</h3>\n<p>A core principle for Docker 1.12 is creating a zero configuration, secure-by-default, out of the box experience for the Docker platform. One of the major hurdles that administrators often face with deploying applications into production is running them securely, Docker 1.12 allows an administrator to follow the exact same steps setting up a demo cluster that they would to setup a secure production cluster.</p>\n<p>Security is not something you can bolt-on after the fact. That is why Docker 1.12 comes with mutually authenticated TLS, providing authentication, authorization and encryption to the communications of every node participating in the swarm, out of the box.</p>\n<p>When starting your first manager, Docker Engine will generate a new Certificate Authority (CA) and a set of initial certificates for you. After this initial step, every node joining the swarm will automatically be issued a new certificate with a randomly generated ID, and their current role in the swarm&#xA0;(manager or worker). These&#xA0;certificates will be used as their cryptographically secure node identity for the lifetime of their participation in this swarm, and will be used by the managers to ensure secure dissemination of tasks and other updates.</p>\n<a href=\"https://www.docker.com/getdocker\"><img title=\"\" src=\"http://img.scoop.it/7M1ysO8rHO8VyFkrebWxT7nTzqrqzN7Y9aBZTaXoQ8Q=\" alt=\"\"></a>&#xA0;\n<p>One of the biggest barriers of adoption of TLS has always been the difficulty of creating, configuring and maintaining the necessary Public Key Infrastructure&#xA0;(PKI). With Docker 1.12, everything not only gets setup and configured with safe defaults for you, but we also automated one of the most painful parts of dealing with TLS certificates: certificate rotation.</p>\n<p>Under the hood, every node participating in the swarm&#xA0;is constantly refreshing its certificates, ensuring that potentially leaked or compromised certificates are no longer valid. The frequency with which certificates are rotated can be configured by the user, and set as low as every 30 minutes.</p>\n<p>If you would like to use your own Certificate Authority, we also support an external-CA mode, where the managers in the swarm simply relay the Certificate Signing Requests of the nodes attempting to join the cluster to a remote URL.</p>\n\n<h3>Bundles</h3>\n<p>Docker 1.12 introduces a new file format called a <a href=\"https://github.com/docker/docker/blob/master/experimental/docker-stacks-and-bundles.md\">Distributed Application Bundle</a>&#xA0;(experimental build only).&#xA0;Bundle&#xA0;is a new abstraction on top of service&#xA0;focused on the full stack application.</p>\n<p>A Docker Bundle file is a declarative specification of a set of services that mandates:</p>\n<ul class=\"indented\">\n<li>What specific image revision to run</li>\n<li>What networks to create</li>\n<li>How containers in those services must be networked to run</li>\n</ul>\n<p>Bundle files are fully portable and are perfect deployment artifacts for software delivery pipelines because they let you ship fully spec&#x2019;ed and versioned multi-container Docker apps.</p>\n<p>The bundle file spec is simple and open, and you can create bundles however you want. To get you started, Docker Compose has experimental support for creating bundle files&#xA0;and with Docker 1.12 and swarm mode enabled, you can deploy the bundle files.</p>\n<p>Bundles are an efficient mechanism for moving multi-service apps from developer laptops through CI to production. It&#x2019;s experimental, and we&#x2019;re looking for feedback from the community.</p>\n\n<h3>Under the hood of Docker 1.12</h3>\n<p>When you take a look under the hood, Docker 1.12 uses a number of other interesting technologies.&#xA0;Inter-node communication is done using <a href=\"http://www.grpc.io/\">gRPC</a>, which gives us HTTP/2 benefits like connection multiplexing and header compression. Our data structures are transmitted efficiently thanks to <a href=\"https://developers.google.com/protocol-buffers/\">protobufs</a>.<br>\n<span class=\"embed-youtube\"><iframe class=\"youtube-player\" type=\"text/html\" width=\"1140\" height=\"672\" src=\"https://www.youtube.com/embed/F7hoq0KwHD4?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent\"></iframe></span><br>\nCheck out these additional resources on Docker 1.12:</p>\n\n\n<div class=\"tm-click-to-tweet\">\n\n<p><a href=\"https://twitter.com/share?text=%23Docker+1.12%3A+Now+with+built-in+orchestration%21+%23swarmmode&amp;via=docker&amp;related=docker&amp;url=https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/\" target=\"_blank\" class=\"tm-ctt-btn\">Click To Tweet</a>\n</p>\n</div>\n\n\n<h3><b>Learn More about Docker</b></h3>\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"tags\"><a href=\"https://blog.docker.com/tag/docker-1-12/\" rel=\"tag\">docker 1.12</a>, <a href=\"https://blog.docker.com/tag/docker-orchestration/\" rel=\"tag\">Docker orchestration</a>, <a href=\"https://blog.docker.com/tag/dockercon/\" rel=\"tag\">dockercon</a>, <a href=\"https://blog.docker.com/tag/orchestration/\" rel=\"tag\">orchestration</a></span>\n\t\t\t\t\t\t\t\t</div>\t\n\t\t\t\t\n\t\t\t</div>"
    }, {
        "title": "A Word is Worth a Thousand Vectors",
        "url": "http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/",
        "excerpt": "Standard natural language processing (NLP) is a messy and difficult affair. It requires teaching a computer about English-specific word ambiguities as well as the hierarchical, sparse nature of words&hellip;",
        "date_saved": "Sat Jul 30 02:13:39 EDT 2016",
        "html_file": "a_word_is_worth_a_thousand_vectors.html",
        "png_file": "a_word_is_worth_a_thousand_vectors.png",
        "md_file": "a_word_is_worth_a_thousand_vectors.md",
        "content": "<div><section class=\"text-content\">\n <p>Standard natural language processing (NLP) is a messy and difficult affair. It requires teaching a computer about English-specific word ambiguities as well as the hierarchical, sparse nature of words in sentences. At Stitch Fix, word vectors help computers learn from the raw text in customer notes. Our systems, composed of machines and human experts, need to recommend the maternity line when she says she&#x2019;s in her &#x2018;third trimester&#x2019;, identify a medical professional when she writes that she &#x2018;used to wear scrubs to work&#x2019;, and distill &#x2018;taking a trip&#x2019; into a Fix for vacation clothing.</p>\n\n<p>While we&#x2019;re not totally &#x201C;there&#x201D; yet with the holy grail to NLP, word vectors (also referred to as distributed representations) are an amazing tool that sweeps away some of the issues of dealing with human language. The machines work in tandem with the stylists as a support mechanism to help identify and summarize textual information from the customers. The human experts will make the final call on what actions will be taken. The goal of this post is to be a motivating introduction to word vectors and demonstrate their real-world utility.</p>\n\n<p>The following example set the natural language community afire<a name=\"footnote1-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote1\">1</a></sup> back in 2013:</p>\n\n\n\n<p>In this example, a human posed a question to a computer: what is <code class=\"highlighter-rouge\">king - man + woman</code>? This is similar to an SAT-style analogy (<code class=\"highlighter-rouge\">man</code> is to <code class=\"highlighter-rouge\">woman</code> as <code class=\"highlighter-rouge\">king</code> is to what?). And a computer solved this equation and answered: <code class=\"highlighter-rouge\">queen</code>. Under the hood, the machine gets that the biggest difference between the words for man and woman is gender. Add that gender difference to <code class=\"highlighter-rouge\">king</code>, and you get <code class=\"highlighter-rouge\">queen</code>.</p>\n\n<p><strong>This is astonishing because we&#x2019;ve never explicitly taught the machine anything about gender!</strong></p>\n\n<p>In fact, we&#x2019;ve never handed the computer anything like a dictionary, a thesaurus, or a network of word relationships. We haven&#x2019;t even tried to break apart a sentence into its constituent parts of speech<a name=\"footnote2-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote2\">2</a></sup>. We&#x2019;ve simply fed a mountain of text into an algorithm called <a href=\"https://code.google.com/p/word2vec/\">word2vec</a> and expected it to learn from context. Word by word, it tries to predict the other surrounding words in a sentence. Or rather, it internally represents words as vectors, and given a word vector, it tries to predict the other word vectors in the nearby text<a name=\"footnote3-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote3\">3</a></sup>.</p>\n\n<p>The algorithm eventually sees so many examples that it can infer the gender of a single word, that both the The Times and The Sun are newspapers, that The Matrix is a sci-fi movie, and that the style of an article of clothing might be boho or edgy. That word vectors represent much of the information available in a dictionary definition is a convenient and almost miraculous side effect of trying to predict the context of a word.</p>\n\n<p>Internally high dimensional vectors stand in for the words, and some of those dimensions are encoding gender properties. Each axis of a vector encodes a property, and the magnitude along that axis represents the relevance of that property to the word<a name=\"footnote4-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote4\">4</a></sup>. If the gender axis is more positive, then it&#x2019;s more feminine; more negative, more masculine.</p>\n\n<p><img src=\"http://multithreaded.stitchfix.com/assets/images/blog/vectors.gif\" alt=\"Vectors\"></p>\n\n<p>Applied appropriately,<strong>word vectors are dramatically more meaningful and more flexible than current techniques</strong><a name=\"footnote5-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote5\">5</a></sup> and let computers peer into text in a fundamentally new way. It&#x2019;s surprisingly easy to get started using libraries like <a href=\"http://radimrehurek.com/2014/02/word2vec-tutorial/\">gensim</a> (in Python) or <a href=\"http://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html#word2vec\">Spark</a> (in Scala &amp; Python) &#x2013; all you need to know is how to add, subtract, and multiply vectors!</p>\n\n<p>Let&#x2019;s review the new abilities that word vectors grant us.</p>\n\n<h2 id=\"similar-words-are-nearby-vectors\">Similar words are nearby vectors</h2>\n<p>Similar words are nearby vectors in a vector space. This is a powerful convention since it lets us wipe away a lot of the noise and nuance in vocabulary. For example, let&#x2019;s use <a href=\"https://radimrehurek.com/gensim/\">gensim</a> to find a list of words similar to <code class=\"highlighter-rouge\">vacation</code> using the freebase skipgram data<a name=\"footnote6-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote6\">6</a></sup>:</p>\n\n<div class=\"highlighter-rouge\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">gensim.models</span> <span class=\"kn\">import</span> <span class=\"n\">Word2Vec</span>\n<span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"s\">\"freebase-vectors-skipgram1000-en.bin.gz\"</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Word2Vec</span><span class=\"o\">.</span><span class=\"n\">load_word2vec_format</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">most_similar</span><span class=\"p\">(</span><span class=\"s\">'vacation'</span><span class=\"p\">)</span>\n\n<span class=\"c\"># [('trip', 0.7234684228897095),</span>\n<span class=\"c\"># ('honeymoon', 0.6447688341140747),</span>\n<span class=\"c\"># ('beach', 0.6249285936355591),</span>\n<span class=\"c\"># ('vacations', 0.5868890285491943),</span>\n<span class=\"c\"># ('wedding', 0.5541957020759583),</span>\n<span class=\"c\"># ('resort', 0.5231006145477295),</span>\n<span class=\"c\"># ('traveling', 0.5194448232650757),</span>\n<span class=\"c\"># ('vacation.', 0.5068142414093018),</span>\n<span class=\"c\"># ('vacationing', 0.5013546943664551)]</span>\n</code></pre>\n</div>\n\n<p>We&#x2019;ve calculated the vectors most similar to the vector for <code class=\"highlighter-rouge\">vacation</code>, and then looked up what words those vectors represent. As we read the list, we note that these words aren&#x2019;t just similar in vector space, but that they make sense intuitively too.</p>\n\n<p>In this case, we&#x2019;ve looked for vectors that are nearby to the word <code class=\"highlighter-rouge\">vacation</code> by measuring the similarity (usually cosine similarity) to the root word and sorting by that.</p>\n\n\n\n\n\n\n\n\n\n\n\n<div id=\"legend\">\n<p class=\"l1\">Destinations</p>\n<p class=\"l2\">Vacation</p>\n<p class=\"l3\">Season</p>\n<p class=\"l4\">Holidays</p>\n<p class=\"l5\">Wedding</p>\n<p class=\"l6\">Month</p>\n</div>\n\n\n<p>Above is an interactive visualization of the words nearest to vacation. The more similar a word to it&#x2019;s genre, the larger the radius of the marker. Hover over the bubbles to reveal the words they represent<a name=\"footnote7-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote7\">7</a></sup>.</p>\n\n<p>And these words aren&#x2019;t just nearby; they&#x2019;re also in several clusters. So we can determine that the words most similar to <code class=\"highlighter-rouge\">vacation</code> come in a variety of flavors: one cluster might be <code class=\"highlighter-rouge\">wedding</code>-related, but another might relate to destinations like <code class=\"highlighter-rouge\">Belize</code>.</p>\n\n<p>Of course our human stylists understand when a client says &#x201C;I&#x2019;m going to Belize in March&#x201D; that she has an upcoming vacation. But the computer can potentially tag this as a &#x2018;vacation&#x2019; fix because the word vector for <code class=\"highlighter-rouge\">Belize</code> is similar to that for <code class=\"highlighter-rouge\">vacation</code>. We can then make sure that the Fixes our customers get are vacation-appropriate!</p>\n\n\n<p>We have the ability to search semantically by adding and subtracting word vectors<a name=\"footnote8-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote8\">8</a></sup>. This empowers us to creatively add and subtract concepts and ideas. Let&#x2019;s start with a style we know a customer liked, <code class=\"highlighter-rouge\">item_3469</code>:</p>\n\n<p><img src=\"http://multithreaded.stitchfix.com/assets/images/blog/vectors_image1.png\" alt=\"Vectors\"></p>\n\n<p>Our customer recently became pregnant, so let&#x2019;s try and find something like <code class=\"highlighter-rouge\">item_3469</code> but along the <code class=\"highlighter-rouge\">pregnant</code> dimension:</p>\n\n<div class=\"highlighter-rouge\"><pre class=\"highlight\"><code><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">most_similar</span><span class=\"p\">(</span><span class=\"s\">'ITEM_3469'</span><span class=\"p\">,</span> <span class=\"s\">'pregnant'</span><span class=\"p\">)</span>\n<span class=\"n\">matches</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">filter</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"s\">'ITEM_'</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">matches</span><span class=\"p\">))</span>\n\n<span class=\"c\"># ['ITEM_13792',</span>\n<span class=\"c\"># 'ITEM_11275',</span>\n<span class=\"c\"># 'ITEM_11868']</span>\n</code></pre>\n</div>\n\n<p>Of course the item IDs aren&#x2019;t immediately informative, but the pictures let us know that we&#x2019;ve done well:</p>\n\n<p><img src=\"http://multithreaded.stitchfix.com/assets/images/blog/vectors_images.png\" alt=\"\"></p>\n\n<p>The first two are items have prominent black &amp; white stripes like <code class=\"highlighter-rouge\">item_3469</code> but have the added property that they&#x2019;re great maternity-wear. The last item changes the pattern away from stripes but is still a loose blouse that&#x2019;s great for an expectant mother. Here we&#x2019;ve simply added the word vector for <code class=\"highlighter-rouge\">pregnant</code> to the word vector for <code class=\"highlighter-rouge\">item_3469</code>, and looked up the word vectors most similar to that result<a name=\"footnote9-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote9\">9</a></sup>.</p>\n\n<p>Our stylists tailor each Fix to their clients, and this prototype system may free them to mix and match artistic concepts about style, size and fit to creatively search for new items.</p>\n\n<h2 id=\"summarizing-sentences--documents\">Summarizing sentences &amp; documents</h2>\n<p>At Stitch Fix, we work hard to craft a uniquely-styled Fix for each of our customers. At every stage of a Fix we collect feedback: what would you like in your next Fix? What did you think of the items we sent you? What worked? What didn&#x2019;t?</p>\n\n<p>The spectrum of responses is myriad, but vectorizing those sentences<a name=\"footnote10-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote10\">10</a></sup> allows us to begin systematically categorizing those documents:</p>\n\n<div class=\"highlighter-rouge\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">gensim.models</span> <span class=\"kn\">import</span> <span class=\"n\">Doc2Vec</span>\n<span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"s\">\"word_vectors_blog_post_v01_notes\"</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Doc2Vec</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">most_similar</span><span class=\"p\">(</span><span class=\"s\">'pregnant'</span><span class=\"p\">)</span>\n<span class=\"n\">matches</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">filter</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"s\">'SENT_'</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">matches</span><span class=\"p\">))</span>\n\n<span class=\"c\"># ['...I am currently 23 weeks pregnant...',</span>\n<span class=\"c\"># '...I'm now 10 weeks pregnant...',</span>\n<span class=\"c\"># '...not showing too much yet...',</span>\n<span class=\"c\"># '...15 weeks now. Baby bump...',</span>\n<span class=\"c\"># '...6 weeks post partum!...',</span>\n<span class=\"c\"># '...12 weeks postpartum and am nursing...',</span>\n<span class=\"c\"># '...I have my baby shower that...',</span>\n<span class=\"c\"># '...am still breastfeeding...',</span>\n<span class=\"c\"># '...I would love an outfit for a baby shower...']</span>\n</code></pre>\n</div>\n\n<p>In this example we calculate which sentences are closest to the word <code class=\"highlighter-rouge\">pregnant</code>. This list also skips over many literal matches of <code class=\"highlighter-rouge\">pregnant</code> in order to demonstrate the more advanced capabilities. We&#x2019;ve also censored sentences to keep out personally identifying text. Also note that the last sentence is a false positive: while similar to the word pregnant, she&#x2019;s unlikely to be interested in maternity clothing.</p>\n\n<p>This allows us to understand not just what words mean, but condense our client comments, notes, and requests in a quantifiable way. We can for example categorize our sentences by first calculating the similarity between a sentence and a word:</p>\n\n<div class=\"highlighter-rouge\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">get_vector</span><span class=\"p\">(</span><span class=\"n\">word</span><span class=\"p\">):</span>\n <span class=\"k\">return</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">syn0norm</span><span class=\"p\">[</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">]</span>\n<span class=\"k\">def</span> <span class=\"nf\">calculate_similarity</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">,</span> <span class=\"n\">word</span><span class=\"p\">):</span>\n <span class=\"n\">vec_a</span> <span class=\"o\">=</span> <span class=\"n\">get_vector</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span>\n <span class=\"n\">vec_b</span> <span class=\"o\">=</span> <span class=\"n\">get_vector</span><span class=\"p\">(</span><span class=\"n\">word</span><span class=\"p\">)</span>\n <span class=\"n\">sim</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">vec_a</span><span class=\"p\">,</span> <span class=\"n\">vec_b</span><span class=\"p\">)</span>\n <span class=\"k\">return</span> <span class=\"n\">sim</span>\n<span class=\"n\">calculate_similarity</span><span class=\"p\">(</span><span class=\"s\">'SENT_47973, '</span><span class=\"n\">casual</span><span class=\"s\">')</span><span class=\"s\"># 0.308</span></code></pre>\n</div>\n\n<p>We calculated the overlap between a sentence with label <code class=\"highlighter-rouge\">SENT_47973</code> and the word <code class=\"highlighter-rouge\">casual</code>. The sentence is previously trained from this customer text: &#x2018;I need some weekend wear. Comfy but stylish.&#x2019; The similarity to <code class=\"highlighter-rouge\">casual</code> is about 0.308, which is pretty high.</p>\n\n<p>Having built a function that computes the similarity between a sentence and a word, we can build a table of customer comments and their similarities to a given topic:</p>\n\n<table>\n <thead>\n <tr>\n <th>raw text snippets</th>\n <th>&#x2018;broken&#x2019;</th>\n <th>&#x2018;casual&#x2019;</th>\n <th>&#x2018;pregnant&#x2019;</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <td>&#x2018;&#x2026; unfortunately the lining <strong>ripped</strong> after wearing if twice &#x2026;&#x2019;</td>\n <td>0.281</td>\n <td>0.082</td>\n <td>0.062</td>\n </tr>\n <tr>\n <td>&#x2018;&#x2026; I need some weekend wear. <strong>Comfy</strong> but stylish.&#x2019;</td>\n <td>0.096</td>\n <td>0.308</td>\n <td>0.191</td>\n </tr>\n <tr>\n <td>&#x2018;&#x2026; 12 weeks <strong>postpartum</strong> and am nursing &#x2026;&#x2019;</td>\n <td>0.158</td>\n <td>0.110</td>\n <td>0.378</td>\n </tr>\n </tbody>\n</table>\n\n<p><br>\nA table like this around helps us quickly answer how many people are looking for comfortable clothes or finding defects in the clothing we send them.</p>\n\n<h2 id=\"what-we-didnt-mention\">What we didn&#x2019;t mention</h2>\n<p>While word vectorization is an elegant way to solve many practical text processing problems, it does have a few shortcomings and considerations:</p>\n\n<ol>\n <li>\n <p><strong>Word vectorization requires a lot of text.</strong> You can <a href=\"https://code.google.com/p/word2vec/#Pre-trained_word_and_phrase_vectors\">download pretrained word vectors</a> yourself, but if you have a highly specialized vocabulary then you&#x2019;ll need to train your own word vectors and have a lot of example text. Typically this means hundreds of millions of words, which is the equivalent of 1,000 books, 500,000 comments, or 4,000,000 tweets.</p>\n </li>\n <li>\n <p><strong>Cleaning the text.</strong> You&#x2019;ll need to clean the words of punctuation and normalize Unicode<a name=\"footnote11-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote11\">11</a></sup> characters, which can take significant manual effort. In this case, there are a few tools that can help like <a href=\"https://github.com/LuminosoInsight/python-ftfy\">FTFY</a>, <a href=\"http://honnibal.github.io/spaCy/\">SpaCy</a>, <a href=\"http://www.nltk.org/\">NLTK</a>, and the <a href=\"http://nlp.stanford.edu/software/corenlp.shtml\">Stanford Core NLP</a>. SpaCy even comes with word vector support built-in.</p>\n </li>\n <li>\n <p><strong>Memory &amp; performance.</strong> The training of vectors requires a high-memory and high-performance multicore machine. Training can take several hours to several days but shouldn&#x2019;t need frequent retraining. If you use pretrained vectors, then this isn&#x2019;t an issue.</p>\n </li>\n <li>\n <p><strong>Databases.</strong> Modern SQL systems aren&#x2019;t well-suited to performing the vector addition, subtraction and multiplication searching in vector space requires. There are a few libraries that will help you quickly find the most similar items<a name=\"footnote12-return\"></a><sup><a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote12\">12</a></sup>: <a href=\"https://github.com/spotify/annoy\">annoy</a>, <a href=\"http://scikit-learn.org/dev/modules/neighbors.html#ball-tree\">ball trees</a>, <a href=\"http://scikit-learn.org/dev/modules/neighbors.html#mathematical-description-of-locality-sensitive-hashing\">locality-sensitive hashing</a> (LSH) or <a href=\"http://www.cs.ubc.ca/research/flann/\">FLANN</a>.</p>\n </li>\n <li>\n <p><strong>False-positives &amp; exactness.</strong> Despite the impressive results that come with word vectorization, no NLP technique is perfect. Take care that your system is robust to results that a computer deems relevant but an expert human wouldn&#x2019;t.</p>\n </li>\n</ol>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The goal of this post was to convince you that <strong>word vectors give us a simple and flexible platform for understanding text.</strong> We&#x2019;ve covered a few diverse examples that should help build your confidence in developing and deploying NLP systems and what problems they can solve. While most coverage of word vectors has been from a scientific angle, or demonstrating toy examples, we at Stitch Fix think this technology is ripe for industrial application.</p>\n\n<p>In fact, Stitch Fix is the perfect testbed for these kinds of new technologies: with expert stylists in the loop, we can move rapidly on new and prototypical algorithms without worrying too much about edge and corner cases. The creative world of fashion is one of the few domains left that computers don&#x2019;t understand. If you&#x2019;re interested in helping us break down that wall, <a href=\"http://technology.stitchfix.com/jobs/index.html\">apply</a>!</p>\n\n<h2 id=\"further-reading\">Further reading</h2>\n<p>There are a few miscellaneous topics that we didn&#x2019;t have room to cover or were too peripheral:</p>\n\n<ol>\n <li>\n <p>There&#x2019;s an excellent nuts and bolts <a href=\"http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf\">explanation and derivation</a> of the word2vec algorithm. There&#x2019;s a similarly useful <a href=\"http://nbviewer.ipython.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb\">iPython Notebook version</a> too.</p>\n </li>\n <li>\n <p>Translating word-by-word English into Spanish is <a href=\"http://arxiv.org/pdf/1309.4168.pdf\">equivalent to matrix rotations</a>. This means that all of the basic linear algebra operators (addition, subtraction, dot products, and matrix rotations) have meaningful functions on human language.</p>\n </li>\n <li>\n <p>Word vectors can also be used to find the <a href=\"https://github.com/dhammack/Word2VecExample\">odd word out</a>.</p>\n </li>\n <li>\n <p>Interestingly, the same skip-gram algorithm can be <a href=\"https://sites.google.com/site/bryanperozzi/projects/deepwalk\">applied to a social graph</a> instead of sentence structure. The authors equate a sequence of social network graph visits (a random walk) to a sequence of words (a sentence in word2vec) to generate a dense summary vector.</p>\n </li>\n <li>\n <p>A brief but very visual overview of distributed representations is available <a href=\"http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\">here</a>.</p>\n </li>\n <li>\n <p>Intriguingly, the word2vec algorithm can be reinterpreted as a <a href=\"https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf\">matrix factorization method using point-wise mutual information</a>. This theoretical breakthrough cleanly connects older and faster but more memory-intensive techniques with word2vec&#x2019;s streaming algorithm approach.</p>\n </li>\n</ol>\n\n\n\n<p><span>\n<a name=\"footnote1\"></a>\n<sup>1</sup>\nSee also the original papers, and the subsequently bombastic <a href=\"http://www.wired.com/2014/12/googlers-quest-teach-machines-understand-emotions/\">media frenzy</a>, the race to understand why word2vec works <a href=\"https://levyomer.wordpress.com/2014/09/10/neural-word-embeddings-as-implicit-matrix-factorization/\">so well</a>, some academic drama on <a href=\"https://docs.google.com/a/stitchfix.com/document/d/1ydIujJ7ETSZ688RGfU5IMJJsbxAi-kRl8czSwpti15s/edit#heading=h.66rkmh7nd17u\">GloVe vs word2vec</a>, and a <a href=\"https://www.youtube.com/watch?v=wTp3P2UnTfQ\">nice introduction</a> to the algorithms behind word2vec from my friend <a href=\"http://radimrehurek.com/\">Radim &#x158;eh&#x16F;&#x159;ek</a>. <a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote1-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote2\"></a>\n<sup>2</sup>\nAlthough see <a href=\"https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/\">Omer Levy and Yoav Goldberg&#x2019;s post</a> for an interesting approach that has the word2vec context defined by parsing the sentence structure. Doing this introduces a more functional similarity between words (see this <a href=\"http://irsrv2.cs.biu.ac.il:9998/?word=hogwarts\">demo</a>). For example, <em>Hogwarts</em> in word2vec is similar to <em>dementors</em> and <em>dumbledore</em>, as they&#x2019;re all from Harry Potter, while parsing context gives <em>sunnydale</em> and <em>colinwood</em> as they&#x2019;re similarly prestigious schools.\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote2-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote3\"></a>\n<sup>3</sup>\nThis is describing the &#x2018;skip-gram&#x2019; mode of word2vec where the target word is asked to predict the surrounding context. Interestingly, we can also get similar results by doing the reverse: using the surrounding text to predict a word in the middle! This model, called continuous bag-of-words (CBOW), loses word order and so we lose a bit of grammatical information since that&#x2019;s very sensitive to the position of a word in a sentence. This means CBOW-trained word vectors tend to do worse in a <em>syntactic</em> sense: the resulting vectors more poorly encode whether a word is an adjective or a verb, or a noun.\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote3-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote4\"></a>\n<sup>4</sup>\nMore generally, a linear combination of axes encodes the properties. We can attempt to rotate into the correct basis by using PCA (as long as we only include a few nearby words) or visualize that space using t-SNE (although we lose the concept of a single axis encoding structure).\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote4-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote5\"></a>\n<sup>5</sup>\nCompare word vectors to sentiment analysis, which effectively distills everything into one dimension of &#x2018;happy or sad&#x2019;, or document labeling efforts like Latent Dirichlet Allocations that sort words into a few types. In either case, we can only ask these simpler models to categorize new documents into a few predetermined groups. With word vectors we can encapsulate far more diversity without having to build a labeled training text (and thus with less effort.)\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote5-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote6\"></a>\n<sup>6</sup>\nYou can download this file freely from <a href=\"https://code.google.com/p/word2vec/#Pre-trained_word_and_phrase_vectors\">here</a>.\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote6-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote7\"></a>\n<sup>7</sup>\nThis is using an advanced visualization technique called <a href=\"http://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne\">t-SNE</a>. This allows us to project down to 2D while still trying to maintain the local structure. This helps pop up the several word clusters that are near to the word <code class=\"highlighter-rouge\">vacation</code>.\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote7-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote8\"></a>\n<sup>8</sup>\nCheck out this live demo with just wikipedia words <a href=\"http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/#wikisim\">here</a>.\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote8-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote9\"></a>\n<sup>9</sup>\nWe&#x2019;ve used cosine similarity to find the nearest items, but, we could&#x2019;ve chosen the 3COSMUL method. This combines vectors multiplicatively instead of additively and seems to <a href=\"https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf\">get better results</a> (pdf warning!). This stays truer to cosine distance and in general prevents one word from dominating in any one dimension.\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote9-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote10\"></a>\n<sup>10</sup>\nYou can easily make a vector for a whole sentence by following the <a href=\"http://radimrehurek.com/2014/12/doc2vec-tutorial/\">Doc2Vec</a> tutorial (also called <a href=\"http://cs.stanford.edu/~quocle/paragraph_vector.pdf\">paragraph vector</a>) in <a href=\"https://radimrehurek.com/gensim/\">gensim</a>, or by clustering words using the <a href=\"http://eng.kifi.com/from-word2vec-to-doc2vec-an-approach-driven-by-chinese-restaurant-process/\">Chinese Restaurant Process</a>.\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote10-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote11\"></a>\n<sup>11</sup>\nIf you&#x2019;re using Python 2, this is a great reason to reduce Unicode headaches and switch to Python 3.\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote11-return\">&#x2190;</a>\n</span></p>\n\n<p><span>\n<a name=\"footnote12\"></a>\n<sup>12</sup>\nSee a comparison of these techniques <a href=\"http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/\">here</a>. My recommendation is using LSH if you need a pure Python solution, and annoy if you need a solution that is memory light.\n<a href=\"http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote12-return\">&#x2190;</a>\n</span></p>\n\n </section>\n </div>"
    }, {
        "title": "Building an SDR from scratch",
        "url": "http://electronics.kitchen/misc/freesrp/",
        "excerpt": "The FreeSRP is an affordable SDR device I decided to design because there were no existing options that filled the gap between the $300 and relatively narrow bandwidth and low resolution HackRF and&hellip;",
        "date_saved": "Sat Jul 30 02:13:40 EDT 2016",
        "html_file": "building_an_sdr_from_scratch.html",
        "png_file": "building_an_sdr_from_scratch.png",
        "md_file": "building_an_sdr_from_scratch.md",
        "content": "<div><div id=\"post-content\" class=\"container\">\n\n<p>The FreeSRP is an affordable SDR device I decided to design because there were no existing options that filled the gap between the $300 and relatively narrow bandwidth and low resolution <a href=\"https://greatscottgadgets.com/hackrf/\">HackRF</a> and the better performing <a href=\"https://www.ettus.com/product/details/UB200-KIT\">USRP</a>s you can get for about $800. Of course it will be fully open source, but some parts still need tidying up before I publish them.</p>\n<p>The FreeSRP is based on the <a href=\"http://www.analog.com/en/products/rf-microwave/integrated-transceivers-transmitters-receivers/wideband-transceivers-ic/ad9364.html\">Analog Devices AD9364</a> transceiver. Some key specifications are:</p>\n<ul>\n<li>56 MHz bandwidth</li>\n<li>70 MHz to 6 GHz center frequency</li>\n<li>Full duplex operation</li>\n</ul>\n<p>I am aware there are more alternatives now, such as the LimeSDR. However, I still think the FreeSRP is a useful tool and certainly has been a rewarding learning experience.</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/hab-vs-freesrp.png\" alt=\"HAB vs FreeSRP\">\n<span class=\"img-caption\">First High altitude ballon PCB vs. FreeSRP</span></p>\n<p>I started this project about 2 years ago, in the summer of 2014 (when I was 16). At that time, I had not done any serious hardware design apart from some (low speed) circuit boards for my <a href=\"http://electronics.kitchen/nearspace\">High Altitude Balloon</a>. Therefore, I knew the FreeSRP's hardware would be challenging in almost every aspect: wide parallel buses running at relatively high speeds (100 MHz), USB 3.0 and RF signal paths requiring good performance at up to 6 GHz, complex power requirements with more than 7 rails at different voltages... Using modern ICs, and wanting to build a compact SDR system, I also needed to deal with high-density packages such as BGA or fine pitch QFN.</p>\n<p>With my limited design experience I knew this was an ambitious project, but I could not even imagine what sorts of implications these requirements would have. So I started right away, knowing only that I wanted to use the AD9364 as my frontend/transceiver, and that I'd need an FPGA bridging the transceiver's interface to a separate USB 3.0 controller. I quickly decided on using a Xilinx Artix 7 FPGA based on cost, and a Cypress EZ-USB FX3 for the USB 3.0 interface, as it was the only option \"easy\" to implement at the time.</p>\n<p>Reading the datasheets and looking at reference designs helped me understand further requirements and I gradually expanded the design until I had finished the <a href=\"https://s3.amazonaws.com/freesrp/prototype-r2/freesrp-sch.pdf\">schematics</a> a couple of weeks later.</p>\n<p>I used Altium Designer for designing the FreeSRP. While it's not open-source, it was the most intuitive PCB package I found, and it offered many advanced features that would make my life a lot easier when routing controlled impedance traces, parallel buses, etc. When I finalize the design I might remake the whole thing in KiCad, which is open source.</p>\n<h2>From initial design to functioning prototype</h2><p><img src=\"http://electronics.kitchen/misc/freesrp/res/r0-layout.png\" alt=\"Almost-final PCB layout for the first revision\">\n<span class=\"img-caption\">Almost-final PCB layout for the first revision</span></p>\n<p>So I finished the schematics -- now PCB layout had to be done. Especially for the initial prototype, cost was the most important constraint. I figured I could just barely do the layout I needed on a four-layer PCB using the <a href=\"https://oshpark.com\">OSH Park</a> fabrication service, the cheapest one I have found for low volume (three pieces) manufacturing. Apart from having four layers only, OSH Park's process offers quite good specifications: 5 mil traces with 5 mil spacing, 10 mil via holes, and the well-characterized Isola FR408 PCB substrate (important to ensure RF signal integrity at high frequencies).</p>\n<p>Optimal component placement probably is the most important PCB design aspect, so I made sure everything was placed in a way that would minimize the length of connections. I also wanted to make the device as compact as possible to cut board costs and increase portability. Having placed about half of the components and knowing more or less how I'd do the rest, I started routing some connections.</p>\n<p>Basically, I started on one side of the signal path (the USB interface) and built up the design by gradually adding parts following the signal path until I arrived at the RF interface and was done. Components not in this path (mostly the voltage regulators) were added as I saw that PCB space was available to be filled.</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/screenshot-partial.jpg\" alt=\"Partially placed and routed board\">\n<span class=\"img-caption\">Partially placed and routed board (only top layer visible)</span></p>\n<p>While the basic structure of the design stayed the same, I did move around, remove and remake lots of parts of the design several times until I got them \"right\".</p>\n<p>Fanning out all of the balls on the BGA packages was probably the most painful aspect of the whole design -- mainly due to only having 4 layers.</p>\n<p>After some more weeks I was finally done. The design passed all rule checks, and I had carefully gone over it making sure nothing funny would get into production. This was one of my main worries, as repairing a four layer board with hundreds of SMD components after soldering would be virtually impossible.</p>\n<h3>Manufacturing the first prototype</h3><p>After much hesitation I ordered three boards from OSH Park, which arrived sometime in January 2015. Yay!</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/r0-bare.png\" alt=\"Bare PCB\">\n<span class=\"img-caption\">First FreeSRP PCBs ever!</span></p>\n<p>I had also ordered a Kapton stencil to apply solder paste. Even <a href=\"http://electronics.kitchen/reflow\">my reflow controller</a> and oven were ready to be used. Now I needed to order the ca. 300 different components.</p>\n<p>As the FreeSRP uses a PCB with double-sided load, I'd first place and solder all the components on the bottom layer. During the design phase I had made sure to only use small components: the bottom would get soldered again while soldering the upper side, but the surface tension of molten solder easily keeps small components in place even when upside down.</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/r0-stencil.jpg\" alt=\"PCB with top side stencil\">\n<span class=\"img-caption\">PCB with stencil on the top side, after applying solder paste. Bottom already soldered.</span></p>\n<h4>Partial assembly</h4><p>Having three PCBs allowed me to first partially assemble the prototype. I did one board with only the voltage regulators installed and found an issue with the 1.8V regulator. This was not a significant problem, however, as I could replace it with my lab power supply. Also, the 1.3V regulators the transceiver needs had a (non-fixable) layout problem -- so at least the RF frontend part of my design would have to wait until the second revision for validation.</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/r0-first-power.jpg\" alt=\"First time I powered up the partially assembled board\">\n<span class=\"img-caption\">First time I powered up the partially assembled board</span></p>\n<p>The second board I'd assemble would contain the whole digital section of the SDR: USB interface and FPGA. It was time for hand-placing my first BGA components.</p>\n<p>After an extremely tense (...expensive ICs, extremely critical alignment, no possibility to repair...) three hours of component placement, I very carefully placed the PCB in the oven. And -- I did not really know what to expect -- the soldering process went fine.</p>\n<h4>First power-up</h4><p>I was really afraid of powering up my board. I knew the power supplies worked, but I could still potentially fry my painstaikingly hand-placed, expensive components by turning on my power supply.</p>\n<p>I do not know of any optimal way to power up a previously untested design. What I ended up doing was just setting a very low current limit on my power supply, increasing it until either stuff seemed to be working, not releasing any magic smoke, or it seemed to draw too much current.</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/r0-testing-fpga.jpg\" alt=\"PCB with digital sections assembled\">\n<span class=\"img-caption\">Partially assembled board with USB controller and FPGA</span></p>\n<p>Luckily, current draw seemed normal, and the power LED was on. Neither the FPGA nor the USB 3.0 controller were warm to the touch. Looked fine! So I plugged it into a USB port on my computer, and the Cypress chip was recognized. The Xilinx tools also connected to the FPGA via JTAG, so everything seemed to be working.</p>\n<p>(More design mistakes: The footprint for the USB 3.0 connector had an error that only allowed the USB 2.0 lanes to be accessible. Not a big deal for testing basic functionality, but I'd not be able to test USB 3.0 throughput.)</p>\n<h3>Second revision</h3><p>The second revision fixed all issues that I managed to find in the first version:</p>\n<ul>\n<li>USB 3.0 footprint</li>\n<li>1.8V regulator</li>\n<li>1.3V regulators</li>\n<li>LED indicators</li>\n</ul>\n<p>That meant the digital/processing part of the system should be fully functional, and that I could assemble and test the transceiver/front-end.</p>\n<p>I first assembled everything but the transceiver, so that I could start writing software and test USB 3.0 transfers. That went very well, and I made a lot of progress writing the firmware for the USB controller and learning the Verilog hardware description language to start implementing the parallel interface to the USB controller (I had never programmed an FPGA before starting this project).</p>\n<p>Developing the FPGA design was one of the hardest parts of this project:</p>\n<ul>\n<li>I found it was quite difficult to find beginner-friendly documentation on using the tools and using the included IP blocks.</li>\n<li>Error messages from the <a href=\"http://www.xilinx.com/products/design-tools/vivado.html\">Vivado Design Suite</a> were often very difficult to understand for me, and using pre-made IP resulted in overwhelming numbers of warnings, that rendered them pretty much useless. It is very likely that I'm just using the tools in an incorrect way, but the lack of approachable documentation made properly understanding them harder than I expected. Also it takes several minutes to report seemingly trivial mistakes such as syntax errors.</li>\n</ul>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/vivado-errors.png\" alt=\"Screenshot showing Vivado warnings\">\n<span class=\"img-caption\">So many warnings it's not fun anymore</span></p>\n<ul>\n<li>Synthesizing and implementing your design every time you make a change is not practical, as it takes a very long time to do even for a small design (more than 15 minutes). You need to simulate as much as possible, which makes the learning curve for FPGA design even steeper.</li>\n<li>Debugging a design running on the FPGA is difficult without using the Xilinx-provided Integrated Logic Analyzer (ILA). The ILA, however, cost hundreds of dollars before it was made free in 2016 (so I could not take advantage of it until a late stage in the design, and often just resorted to using the onboard LEDs or routing signals to GPIO pins as to probe them with my oscilloscope).</li>\n</ul>\n<p>Also, it took me until this time to understand the FPGAs clocking resources, so I realized I'd made some important design mistakes by not routing some of the clock signals from the transceiver to clock-capable inputs on the FPGA.</p>\n<p>After having played around with the digital part of the design I decided it was time to try to assemble a new board, now including the transceiver. So, I placed by hand the 300 components, soldering the bottom side first. But when soldering the top side it happened: the controller on my reflow oven stopped working, and left the oven switched on. I lost the temperature readout, and could only switch the oven on and off manually. I tried to leave the oven on, hoping it would still solder fine, but worrying I would overshoot my intended peak temperature.</p>\n<p>And it did not work: Even though the board looked great, there were shorts on main power rails, and powering it up, the FPGA was getting hot. I tried to fix it probing around, removing things, but nothing helped. It was completely fried. This took me some time getting over, as I had just lost about 400$ in parts. It was definitely not encouraging.</p>\n<p>Nevertheless, I still wanted to finish this project, so I reordered some parts and tried again a week later. I was incredibly nervous during the whole assembly and soldering process (especially because I was using the last one of the three PCBs I had), but the manufacturing process worked this time (as it had every time but last time).</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/r1-dead-xcvr.jpg\" alt=\"Fully assembled FreeSRP\">\n<span class=\"img-caption\">Fully assembled FreeSRP in acrylic enclosure</span></p>\n<p>Everything that had worked on the previous, partially built prototype worked on this one, as well. But all my attempts to communicate with the transceiver over its SPI configuration port failed: it was not responding. I also noticed that it was getting hot (maybe to about 60&#xB0;C) after just a few seconds of being powered on. This was very worrying, as the device should have been in a \"sleep\" state, dissipating very little power -- so I figured the problem probably was in hardware.</p>\n<p>After checking all power rails, I could still not find any possible issues. So I went back to the schematics and started checking all the support circuitry for the transceiver. It took me some time, but then I found this:</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/shit.png\" alt=\"Something awful\">\n<span class=\"img-caption\">Oh no...</span></p>\n<p>One 698&#x3A9; and one 546&#x3A9; valued resistor (in series, so 1234&#x3A9; total) instead of the 14.3k&#x3A9; resistor specified by the datasheet!</p>\n<p>Asking on Analog Devices' support forum, I was told to try replacing the resistors with the correct one. I did that, and the part indeed did not warm up as much as before, but still, I was unable to get it to work. It was safe to assume the chip was dead. <strong>:(</strong></p>\n<p>At this point, I considered giving up on this project, acknowledging the fact that it was way too ambitious for the limited electronics knowledge I had. But I continued... concentrating on finishing as much of the software/FPGA design as possible using the partially working hardware.</p>\n<p>Finally, I managed to write an FPGA design that incorporated Analog Devices' driver for the AD9364 (which handles calibration and set up for the transceiver), and generated test signals which could be introduced into the whole signal chain: This way I could work on the interface between USB controller and FPGA and continue writing the C++ library I had started to control the SDR from the computer.</p>\n<p>I also extended the <a href=\"http://sdr.osmocom.org/trac/wiki/GrOsmoSDR\">osmocom gr-osmosdr GNURadio block</a>, making my SDR fully compatible with GNURadio (<a href=\"https://github.com/FreeSRP/gr-osmosdr\">my fork is available here</a>)!</p>\n<h3>Third revision</h3><p><img src=\"http://electronics.kitchen/misc/freesrp/res/r2.jpg\" alt=\"FreeSRP, third revision\">\n<span class=\"img-caption\">Latest version of the FreeSRP</span></p>\n<p>Fixed in this revision:</p>\n<ul>\n<li>Clock inputs into FPGA clock cabaple pins</li>\n<li>14.3k&#x3A9; resistor</li>\n<li>Replaced transceiver oscillator with a crystal to simplify the reference clock circuitry (eliminating potential for further issues)</li>\n</ul>\n<p>This, again, fixed all issues I'd discovered up to now. I also had done more revisions of the board and spent way more money than I initially planned for... even though in hindsight, having only three revisions for a fully functioning, moderately complex design, with no previous experience working on something like this, probably is not <em>that</em> bad.</p>\n<p>One major change I did was switching from the four-layer OSH Park process to a six layer stackup. This multiplied the cost of the prototype boards, but allowed me to dramatically increase clearance between many critical signal traces, allowing the parallel buses to finally run at the maximum required clock speeds very reliably.</p>\n<p>I also got myself some very nice stainless steel stencils, which were super easy to use and produced much better results than the Kapton ones:</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/steel-stencil.jpg\" alt=\"PCB and stencil in stencilling jig\">\n<span class=\"img-caption\">PCB and steel stencil placed in stencilling jig</span></p>\n<p>I could verify that the transceiver was working very easily, as I had already developed all the software and integrated the driver provided by Analog Devices for the AD9364 into my design. It did not take me long to receive my first samples using GNURadio!</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/950mhz.png\" alt=\"FFT spectrum\">\n<span class=\"img-caption\">Spectrum around 950 MHz, at the full 56 MHz bandwidth</span></p>\n<p>Finally, all the hard work and failed attempts had paid off. And, some more weeks later I could verify the transmitter worked as well (again, fully compatible with GNURadio, with full duplex working as well -- not at the full bandwidth, though). However, in this revision there is still a problem with the amplifier at the RF output, and the signal is extremely weak.</p>\n<p>Still, at this point, I basically have a fully functioning SDR.</p>\n<p>There still is a lot that needs to be done, though. For example, I still need to properly characterize the receiver and transmitter performance. Also, I'm thinking about doing a small production run -- this means fixing the last few hardware issues, optimizing the design, making sure it is manufacturable...</p>\n<h2>Available software and compatibility</h2><p>To interface with the FreeSRP, I have written a small C++ library, <a href=\"https://github.com/FreeSRP/libfreesrp\">libfreesrp</a>. It uses <a href=\"http://www.libusb.org/\">libusb</a> and is very easy to use as it is quite simple itself. Initializing the SDR is as simple as instantiating a <code>FreeSRP</code> object:</p>\n<pre><code class=\"language-c++\">FreeSRP::FreeSRP srp;\ncout &lt;&lt; \"version: \" &lt;&lt; srp.version() &lt;&lt; endl;\n</code></pre>\n<p>Setting the center frequency and bandwidth:</p>\n<pre><code class=\"language-c++\">srp.send_cmd(srp.make_command(SET_RX_LO_FREQ, 2.4e9));\nsrp.send_cmd(srp.make_command(SET_RX_SAMP_FREQ, 1e8));\n</code></pre>\n<p>You can then start the receiver and get a sample:</p>\n<pre><code class=\"language-c++\">srp.start_rx();\n\nsample s;\nbool success = srp.get_rx_sample(s);\n</code></pre>\n<p>There are also some simple utilities. <code>freesrp-ctl</code> can be used to load a bitstream onto the FPGA and perform various other configurations (such as setting bandwidth and frequency) using a simple interactive command-line interface.</p>\n<p><code>freesrp-rec</code> enables the receiver and writes the received samples to a file or to <code>stdout</code>. This is useful because you can pipe samples into programs like <a href=\"http://www.baudline.com/\">baudline</a>, in real time and at full bandwidth. For example:</p>\n<pre><code>freesrp-rec -f433.5e6 -b10e6 -g42 -o- | baudline -reset -stdin -channels 2 -quadrature -format le16 -samplerate 10000000\n</code></pre><p>Using libfreesrp, I have extended <a href=\"http://sdr.osmocom.org/trac/wiki/GrOsmoSDR\">gr-osmosdr</a> to provide support for the FreeSRP in GNURadio. You now only need to specify <code>freesrp</code> in the osmocom Source and Sink blocks' device arguments. Here's an 802.15.4 transceiver made in GNURadio Companion, using some <a href=\"https://github.com/bastibl/gr-ieee802-15-4\">gr-ieee802-15-4</a> blocks:</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/gnuradio-802-15-4.png\" alt=\"GNURadio companion flowchart\">\n<span class=\"img-caption\">GNURadio Companion flowchart</span></p>\n<p>Conveniently, the FreeSRP now also works with all <code>gr-osmosdr</code> based utilities, such as <code>grgsm_scanner</code>. <code>grgsm_scanner</code> is a little program that will scan different frequencies looking for GSM base stations, reporting what it finds:</p>\n<p><img src=\"http://electronics.kitchen/misc/freesrp/res/grgsm-scanner.png\" alt=\"GNURadio companion flowchart\">\n<span class=\"img-caption\">If you look up the codes you will know roughly where I live</span></p>\n<p><a href=\"http://gqrx.dk\">Gqrx</a> uses the osmocom block, too, and also works flawlessly with the FreeSRP.</p>\n<p>Recently I came across <a href=\"http://luaradio.io\">luaradio</a>. Should be easy to add FreeSRP support to that as well!</p>\n<h2>Conclusion</h2><p>I have discovered that taking on large projects, even when not knowing how to tackle problems that might arise, is a very effective way of learning for me. It's just important to be persistent, as I've seen that almost any problem can be solved on your own -- which is incredibly rewarding -- even if you get stuck and seem to not make progress for a while.</p>\n<p>And now I get to play with my SDR!</p>\n<p>You can also check out the the <a href=\"http://electronics.kitchen/freesrp\">project page</a> or the <a href=\"http://electronics.kitchen/docs/freesrp/\">documentation</a>. There's also <a href=\"https://www.reddit.com/r/electronics/comments/4uic5r/sdr_platform_ive_been_working_on/\">a reddit thread</a>.</p>\n<p><span id=\"copyright\">Copyright &#xA9; 2016, Lukas Lao Beyer</span></p></div></div>"
    }, {
        "title": "Sup",
        "url": "https://pressly.github.io/sup/",
        "excerpt": "Installation $ go get -u github.com/pressly/sup/cmd/sup Usage $ sup [OPTIONS] NETWORK COMMAND [...] Options Option Description -f Supfile Custom path to Supfile -e, --env=[] Set environment variables&hellip;",
        "date_saved": "Sat Jul 30 02:13:41 EDT 2016",
        "html_file": "sup.html",
        "png_file": "sup.png",
        "md_file": "sup.md",
        "content": "<div><main>\n <h2 id=\"toc_2\">Installation</h2>\n <div><pre><code class=\"language-yaml\">$ go get -u github.com/pressly/sup/cmd/sup</code></pre></div>\n\n <h2 id=\"toc_3\">Usage</h2>\n <div><pre><code class=\"language-yaml\">$ sup [OPTIONS] NETWORK COMMAND [...]</code></pre></div>\n </main>\n\n <aside>\n <h3 id=\"toc_4\">Options</h3>\n\n <table>\n <thead>\n <tr>\n <th>Option</th>\n <th>Description</th>\n </tr>\n </thead>\n\n <tbody>\n <tr>\n <td><code>-f Supfile</code></td>\n <td>Custom path to Supfile</td>\n </tr>\n <tr>\n <td><code>-e</code>, <code>--env=[]</code></td>\n <td>Set environment variables</td>\n </tr>\n <tr>\n <td><code>--only REGEXP</code></td>\n <td>Filter hosts matching regexp</td>\n </tr>\n <tr>\n <td><code>--except REGEXP</code></td>\n <td>Filter out hosts matching regexp</td>\n </tr>\n <tr>\n <td><code>--debug</code>, <code>-D</code></td>\n <td>Enable debug/verbose mode</td>\n </tr>\n <tr>\n <td><code>--disable-prefix</code></td>\n <td>Disable hostname prefix</td>\n </tr>\n <tr>\n <td><code>--help</code>, <code>-h</code></td>\n <td>Show help/usage</td>\n </tr>\n <tr>\n <td><code>--version</code>, <code>-v</code></td>\n <td>Print version</td>\n </tr>\n </tbody>\n </table>\n\n <h2 id=\"toc_5\">Network</h2>\n\n <p>A group of hosts.</p>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n\nnetworks:\n production:\n hosts:\n - api1.example.com\n - api2.example.com\n - api3.example.com\n staging:\n # fetch dynamic list of hosts\n inventory: curl http://example.com/latest/meta-data/hostname</code></pre></div>\n\n<p><code>$ sup production COMMAND</code> will run COMMAND on <code>api1</code>, <code>api2</code> and <code>api3</code> hosts in parallel.</p>\n\n<h2 id=\"toc_6\">Command</h2>\n\n<p>A shell command(s) to be run remotely.</p>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n\ncommands:\n restart:\n desc: Restart example Docker container\n run: sudo docker restart example\n tail-logs:\n desc: Watch tail of Docker logs from all hosts\n run: sudo docker logs --tail=20 -f example</code></pre></div>\n\n<p><code>$ sup staging restart</code> will restart all staging Docker containers in parallel.</p>\n\n<p><code>$ sup production tail-logs</code> will tail Docker logs from all production containers in parallel.</p>\n\n<h3 id=\"toc_7\">Serial command (a.k.a. Rolling Update)</h3>\n\n<p><code>serial: N</code> constraints a command to be run on <code>N</code> hosts at a time at maximum. Rolling Update for free!</p>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n\ncommands:\n restart:\n desc: Restart example Docker container\n run: sudo docker restart example\n serial: 2</code></pre></div>\n\n<p><code>$ sup production restart</code> will restart all Docker containers, two at a time at maximum.</p>\n\n<h3 id=\"toc_8\">Once command (one host only)</h3>\n\n<p><code>once: true</code> constraints a command to be run only on one host. Useful for one-time tasks.</p>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n\ncommands:\n build:\n desc: Build Docker image and push to registry\n run: sudo docker build -t image:latest . &amp;&amp; sudo docker push image:latest\n once: true # one host only\n pull:\n desc: Pull latest Docker image from registry\n run: sudo docker pull image:latest</code></pre></div>\n\n<p><code>$ sup production build pull</code> will build Docker image on one production host only and spread it to all hosts.</p>\n\n<h3 id=\"toc_9\">Local command</h3>\n\n<p>Runs command always on localhost.</p>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n\ncommands:\n prepare:\n desc: Prepare to upload\n local: npm run build</code></pre></div>\n\n<h3 id=\"toc_10\">Upload command</h3>\n\n<p>Uploads files/directories to all remote hosts. Uses <code>tar</code> under the hood.</p>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n\ncommands:\n upload:\n desc: Upload dist files to all hosts\n upload:\n - src: ./dist\n dst: /tmp/</code></pre></div>\n\n<h3 id=\"toc_11\">Interactive Bash on all hosts</h3>\n\n<p>Do you want to interact with multiple hosts at once? Sure!</p>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n\ncommands:\n bash:\n desc: Interactive Bash on all hosts\n stdin: true\n run: bash</code></pre></div>\n\n<div><pre><code class=\"language-bash\">$ sup production bash\n#\n# type in commands and see output from all hosts!\n# ^C</code></pre></div>\n\n<p>Passing prepared commands to all hosts:\n</p><div><pre><code class=\"language-bash\">$ echo 'sudo apt-get update -y' | sup production bash</code></pre></div>\n\n\n\n<div><pre><code class=\"language-bash\">$ sup production bash &lt;&lt;&lt; 'sudo apt-get update -y'</code></pre></div>\n\n\n\n<div><pre><code class=\"language-bash\">$ cat &lt;&lt;EOF | sup production bash\nsudo apt-get update -y\ndate\nuname -a\nEOF\n</code></pre></div>\n\n<h3 id=\"toc_14\">Interactive Docker Exec on all hosts</h3>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n\ncommands:\n exec:\n desc: Exec into Docker container on all hosts\n stdin: true\n run: sudo docker exec -i $CONTAINER bash</code></pre></div>\n\n<div><pre><code class=\"language-bash\">$ sup production exec\nps aux\nstrace -p 1 # trace system calls and signals on all your production hosts</code></pre></div>\n\n<h2 id=\"toc_15\">Target</h2>\n\n<p>Target is an alias for multiple commands. Each command will be run on all hosts in parallel,\n<code>sup</code> will check return status from all hosts, and run subsequent commands on success only\n(thus any error on any host will interrupt the process).</p>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n\ntargets:\n deploy:\n - build\n - pull\n - migrate-db-up\n - stop-rm-run\n - health\n - slack-notify\n - airbrake-notify</code></pre></div>\n\n<p><code>$ sup production deploy</code></p>\n\n<p>is equivalent to</p>\n\n<p><code>$ sup production build pull migrate-db-up stop-rm-run health slack-notify airbrake-notify</code></p>\n\n<h2 id=\"toc_16\">Supfile</h2>\n\n<p>See <a href=\"https://pressly.github.io/sup/example/Supfile\">example Supfile</a>.</p>\n\n<h3 id=\"toc_17\">Basic structure</h3>\n\n<div><pre><code class=\"language-yaml\"># Supfile\n---\nversion: 0.4\n\n# Global environment variables\nenv:\n NAME: api\n IMAGE: example/api\n\nnetworks:\n local:\n hosts:\n - localhost\n staging:\n hosts:\n - stg1.example.com\n production:\n hosts:\n - api1.example.com\n - api2.example.com\n\ncommands:\n echo:\n desc: Print some env vars\n run: echo $NAME $IMAGE $SUP_NETWORK\n date:\n desc: Print OS name and current date/time\n run: uname -a; date\n\ntargets:\n all:\n - echo\n - date</code></pre></div>\n\n<h3 id=\"toc_18\">Default environment variables available in Supfile</h3>\n\n<ul>\n<li><code>$SUP_HOST</code> - Current host.</li>\n<li><code>$SUP_NETWORK</code> - Current network.</li>\n<li><code>$SUP_USER</code> - User who invoked sup command.</li>\n<li><code>$SUP_TIME</code> - Date/time of sup command invocation.</li>\n<li><code>$SUP_ENV</code> - Environment variables provided on sup command invocation. You can pass <code>$SUP_ENV</code> to another <code>sup</code> or <code>docker</code> commands in your Supfile.</li>\n</ul>\n\n<h2 id=\"toc_19\">Running sup from Supfile</h2>\n\n<p>Supfile doesn't let you import another Supfile. Instead, it lets you run <code>sup</code> sub-process from inside your Supfile. This is how you can structure larger projects:</p>\n\n<div><pre><code class=\"language-yaml\">./Supfile\n./database/Supfile\n./services/scheduler/Supfile</code></pre></div>\n\n<p>Top-level Supfile calls <code>sup</code> with Supfiles from sub-projects:</p>\n\n<div><pre><code class=\"language-yaml\">yaml\n restart-scheduler:\n desc: Restart scheduler\n run: &gt;\n sup -f ./services/scheduler/Supfile $SUP_ENV $SUP_NETWORK restart\n db-up:\n desc: Migrate database\n run: &gt;\n sup -f ./database/Supfile $SUP_ENV $SUP_NETWORK up\n</code></pre></div>\n\n<h2 id=\"toc_20\">Development</h2>\n\n<div><pre><code class=\"language-yaml\"># fork it, hack it..\n\n$ make build\n\n# create new Pull Request</code></pre></div>\n\n <p>We'll be happy to review &amp; accept new Pull Requests!</p>\n </aside>\n\n </div>"
    }, {
        "title": "Docker Built-in Orchestration Ready for Production",
        "url": "https://blog.docker.com/2016/07/docker-built-in-orchestration-ready-for-production-docker-1-12-goes-ga/",
        "excerpt": "We wanted to thank everyone in the community for helping us achieve this great milestone of making Docker 1.12 generally available for production environments. Docker 1.12 adds the largest and most&hellip;",
        "date_saved": "Sat Jul 30 02:13:43 EDT 2016",
        "html_file": "docker_builtin_orchestration_ready_for_production.html",
        "png_file": "docker_builtin_orchestration_ready_for_production.png",
        "md_file": "docker_builtin_orchestration_ready_for_production.md",
        "content": "<div><div class=\"entry-content\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<p>We wanted to thank everyone in the community for helping us achieve this great milestone of making Docker 1.12 generally available for production environments. Docker 1.12 adds the largest and most sophisticated set of features into a single release since the beginning of the Docker project. Dozens of engineers, both Docker employees and external contributors, have made substantial contributions to every aspect of 1.12 orchestration including core algorithms, integration into the Docker Engine, documentation and testing.</p>\n<p>We&#x2019;re very grateful to the community, which has helped us with feedback, bug reports and new ideas. We couldn&#x2019;t have done it without the help in particular of the tens of thousands of <a href=\"https://blog.docker.com/2016/07/ocker-for-mac-and-windows-production-ready\">Docker for Mac and Windows</a> beta users who have been testing our 1.12 features since DockerCon in June. We&#x2019;ve seen contributions ranging from bash tab completion to UX up-and-down votes that helped us understand what users want most. Compared to what we unveiled at DockerCon, we&#x2019;ve ended up with significant improvements in the swarm node join workflow (<a href=\"https://github.com/docker/swarmkit/pull/1189\">it&#x2019;s simpler</a>), error reporting (<a href=\"https://github.com/docker/docker/pull/24917\">easier to view</a>), UX improvements (<a href=\"https://github.com/docker/docker/pull/24816\">more logical</a>), networking (fixed reliability issues) etc.</p>\n<p>The core team also wanted to give a special thanks to one of our external maintainers and Docker Captain, <a href=\"https://github.com/chanwit\">Chanwit Kaewkasi</a>, who through his own undertaking, drove the amazing <a href=\"https://twitter.com/chanwit/status/756528590796234752\">DockerSwarm2000</a> initiative which rallied the entire community around scaling an RC of 1.12 with swarm mode to nearly 2.4K nodes and just under 100K containers. This was achieved through our global community, who donated access to their machines in all shapes and sizes from bare metal, to Raspberry Pis, to various clouds to VMs from x86 architectures to ARM-based systems. <span>Through this evaluation using live data, we identified that built-in orchestration in Docker has already&#x2013;in its first release&#x2013;doubled Docker&#x2019;s </span><a href=\"https://github.com/swarm2k/swarm2k/blob/master/DATA_FILES.md\"><span>orchestration scale</span></a><span> in just a half a year. While this validates the scalability of the architecture, there is still headroom for greater performance optimization in the future.</span><a href=\"https://github.com/swarm2k/swarm2k/blob/master/DATA_FILES.md\"></a></p>\n<p><img class=\"alignnone size-full wp-image-14179\" src=\"https://blog.docker.com/wp-content/uploads/swarm-1-300x152.png%20300w,%20https://blog.docker.com/wp-content/uploads/swarm-1.png%20975w\" alt=\"swarm\" width=\"975\"></p>\n<p>Let&#x2019;s now dig deep into this new built-orchestration architecture and why we took a very different architectural approach than others trying to address container orchestration.</p>\n<h4><span><strong>Swarm Mode Architectural Topology</strong></span></h4>\n<p><img class=\"alignnone size-full wp-image-14180\" src=\"https://blog.docker.com/wp-content/uploads/swarm-topology-300x125.png%20300w,%20https://blog.docker.com/wp-content/uploads/swarm-topology.png%20884w\" alt=\"swarm-topology\" width=\"884\"></p>\n<p>Built-in container orchestration in Docker 1.12 is an optional feature set that involves turning on a capability known as swarm mode. A swarm is a decentralized and highly available group of Docker nodes. Each node is a self-contained orchestration subsystem that has all the inherent capabilities needed to create a pool of common resources to schedule Dockerized services.</p>\n<p>A swarm of Docker nodes creates a programmable topology, enabling the operator to choose which nodes are managers and which are workers. This includes common configurations like distributing managers across multiple availability zones. Because these roles are dynamic, they can be changed at any time through the API or CLI.</p>\n<p>Managers are responsible for orchestrating the cluster, serving the Service API, scheduling tasks (containers), addressing containers that have failed health checks and much more.&#xA0; In contrast, worker nodes serve a much simpler function, which is executing the tasks to spawn containers and routing data traffic intended for specific containers. In production environments, we strongly recommend having nodes designated as either &#x201C;managers&#x201D; or &#x201C;workers&#x201D;. In this mode, managers do not execute containers, thus reducing their workload and attack surface. Separately, one of swarm mode&#x2019;s security advances is that worker nodes do not have access to information in the datastore or the Service API. Worker nodes can only accept work and report on status.&#xA0; Thus, a compromised worker node is limited in the damage it can do to the system.</p>\n<p>Our team focused quite a bit on architecting the communication between these nodes. Managers and workers have different communication requirements in terms of consistency, speed and volume; therefore, they use two distinct communication methods. Raft is used to share data between managers for strong consistency (at the cost of write speed and limited volume) while gossip is used between workers for fast communication and high volume (albeit with only eventual consistency). And communication between managers and workers has separate requirements still.&#xA0; The one thing that they all have in common is that they have encrypted communication by default; mTLS.</p>\n\n<h4><span><strong>Manager to Manager Communication: Always Available Quorums</strong></span></h4>\n<p>When a node is given the role of manager, it joins a Raft consensus group to share information and perform leader election. The leader is the central authority maintaining the state, which includes lists of nodes, services and tasks across the swarm in addition to making scheduling decisions.That state is distributed across each manager node through a built-in Raft store. That is, managers have no dependency on an external key-value store like etcd or Consul, which is one less component for ops to have to manage.&#xA0; Non-leader managers act as hot spares and forward API requests to the current leader. The system is therefore <strong>fault tolerant and highly available</strong>.</p>\n<p>Having an integrated distributed data store allows for many optimizations that could have not been achieved using a generic store&#xA0; &#x2013; this results in our built-in orchestration system being extremely fast. A major optimization is that the entire swarm state is kept in-memory resulting in instant reads. This read optimization is highly beneficial to a critical orchestration; reconciling state which is a read-heavy workflow. Typically, a scheduler has to perform hundreds of reads: read the list of nodes, read what other tasks are running on those nodes, and so on. With the read optimization, there is an increase in velocity,which results from removing the need for hundreds of read network round-trips to the external database.</p>\n<p>Writes are also critical to orchestration and the optimization is that in swarm mode writes can be&#xA0;batched together in a single network round-trip. A common write example is when you scale up a service, the orchestrator has to create a new object for every instance the user requests. With an external store, we would need to send a network request to the store <strong>for every object</strong> we create, wait for them to save the write, and repeat. This can take tens of milliseconds per request and they add up (especially if you&#x2019;re adding hundreds of new instances). With our model, we can batch hundreds of those objects into a single write.</p>\n<p>The same write optimization has major impact on resiliency. For example, if a node that had 100 containers went down, instead of performing 100 writes to move them to different nodes, we can do all of that in a single write.</p>\n<p>The final optimization is in how efficiently the data is persisted both in terms of size (protocol buffers) and performance (domain specific indexing). We can instantly query from memory the containers that are running on a given machine, the&#xA0; containers that are unhealthy for a specific service, etc.</p>\n<p><img class=\"alignnone size-full wp-image-14181\" src=\"https://blog.docker.com/wp-content/uploads/swarm-quorum-layer-300x98.png%20300w,%20https://blog.docker.com/wp-content/uploads/swarm-quorum-layer.png%20975w\" alt=\"swarm-quorum-layer\" width=\"975\"></p>\n<h4><span><strong>Manager to Worker Communication</strong></span></h4>\n<p>Worker nodes talk to manager nodes using gRPC, a fast protocol that works extremely well in harsh networking conditions, allows communication through internet links (built on HTTP/2) and has built-in versioning (so that different worker nodes running different versions of Engine can talk to the same manager node). Managers send workers sets of tasks to run. Workers send managers the status of the tasks in their assignment set, and a heartbeat so the managers can confirm that&#xA0; the worker is still alive.</p>\n<p>As the diagram below illustrates, the dispatcher component of the manager code is what ultimately communicates with workers. It is responsible for dispatching tasks to each worker, while the worker (through it&#x2019;s executor component) is responsible for translating those tasks into containers and creating them.</p>\n<p><img class=\"alignnone size-full wp-image-14182\" src=\"https://blog.docker.com/wp-content/uploads/swarm-node-breakdown-300x183.png%20300w,%20https://blog.docker.com/wp-content/uploads/swarm-node-breakdown.png%20975w\" alt=\"swarm-node-breakdown\" width=\"975\"></p>\n\n<p>Based on the diagram above, let&#x2019;s briefly walk through what happens as a Docker service is created and ultimately spawns that set of containers:</p>\n<ul>\n<li>Service creation\n<ul>\n<li>User sends the service definition to the API. The API accepts and stores</li>\n<li>Orchestrator reconciles desired state (as defined by the user) with the actual state (what&#x2019;s currently running on the swarm). It will pick up the new service created by the API and respond to that by creating a task (assuming in this case, the user requested only one instance of the service)</li>\n<li>Allocator allocates resources for tasks. It will notice a brand new service (created by the API) and a new task (created by the orchestrator) and will allocate IP addresses for both.</li>\n<li>Scheduler is responsible to assign tasks to worker nodes. It will notice a task with no node assigned and therefore will start scheduling. It tries to find the best match (based on constraints, resources, &#x2026;) and finally, it will assign the task to one of the nodes</li>\n<li>Dispatcher is where workers connect to. Once workers are connected to the Dispatcher, they wait for instructions. In this way, a task assigned by the scheduler will eventually flow down to the worker.</li>\n</ul>\n</li>\n</ul>\n\n<ul>\n<li>Service update\n<ul>\n<li>User updates a service definition through the API (e.g. change from 1 to 3 instances). API accepts and stores.</li>\n<li>Orchestrator reconciles desired vs actual. It will notice that even though the user wants 3 instances, only 1 is running and will respond to that by creating two additional tasks.</li>\n<li>Allocator, Scheduler and Dispatcher will perform the same steps as explained above and the two new tasks will land on workers</li>\n</ul>\n</li>\n</ul>\n\n<ul>\n<li>Node failure\n<ul>\n<li>Dispatcher will detect a node that was connected to it just failed (because of heartbeats). It will flag the node as DOWN.</li>\n<li>Orchestrator reconciles. 3 instances should be running but 1 of them just failed. Responds by creating a new task.</li>\n<li>Allocator, Scheduler and Dispatcher will perform the same steps as explained above and the 2 new tasks will land on new workers nodes.</li>\n</ul>\n</li>\n</ul>\n<p><img class=\"alignnone size-full wp-image-14183\" src=\"https://blog.docker.com/wp-content/uploads/worker-to-worker-gossip-300x81.png%20300w,%20https://blog.docker.com/wp-content/uploads/worker-to-worker-gossip.png%20975w\" alt=\"worker-to-worker-gossip\" width=\"975\"></p>\n<ul>\n<li>The workers use a Gossip network to communicate overlay network information to each other. Gossip is a high volume, peer to peer (P2P) network designed to work at high scale. A node accepts a task, starts a container and then it tells the other nodes that a container has started on the specified overlay network. This broadcast communication happens at the worker tier. The scale is achieved because the information is gossiped only to a constant number of random nodes and not to all nodes which works the same way regardless of the swarm size.</li>\n</ul>\n\n<h4><span><strong>What does it all mean?</strong></span></h4>\n<p>&#xA0;<br>\nSo, what does Docker 1.12 orchestration really mean for developers and operators?&#xA0; There are three really important themes in this release and empowered by the rich architecture detailed above.:</p>\n\n<ul>\n<li><strong>Fault-tolerant application deployment platform.</strong> Modern applications are increasingly designed in a microservices architectural pattern where the process of serving back data to a user may need to invoke several different services. Real world machines fail all the time and these microservices need to continue to be available even in the face of such random failures.&#xA0; Docker 1.12 gives you this power by providing a zero-SPOF design leveraging a quorum of managers, plus a service abstraction that spawns multiple replicas and quickly reschedules them if their host disappears.</li>\n</ul>\n\n<ul>\n<li><strong>Scale and performance. </strong>Docker 1.12&#x2019;s swarm mode orchestration was designed from the ground up with scale and performance in mind. For example, the internal Raft distributed store has been optimized for lightning fast reads through an in-memory caching layer.&#xA0; Caching gives fast reads, but what happens when a write comes in?&#xA0; Of course the cache on each machine must be invalidated and updated.&#xA0; Our solution to that problem was to design the rest of the orchestration system in ways that are read-intensive but only <em>write</em> to the Raft store when absolutely necessary.&#xA0; This combination of design decisions leads to an orchestration system that delivers better performance than what is achievable with orchestrators that are based on generic key-value stores.</li>\n</ul>\n\n<ul>\n<li><strong>Secure networking.</strong> In many systems, security is something you have to &#x201C;turn on&#x201D; by generating TLS certificates, running the system on a different port, and figuring out traffic flows to make sure no packets traverse an insecure network unencrypted.&#xA0; With Docker 1.12, all of these things are done for you out of the box [*].&#xA0; The system is &#x201C;secure by default,&#x201D; which means that you don&#x2019;t need to be a security expert to get a secure application management platform.</li>\n</ul>\n\n<p>[*] There is one small exception to this statement.&#xA0; For overlay network traffic, current Docker versions require you to manually specify the flag <code>-o encrypted</code> (on <code>docker network create -d overlay</code>) to enable encryption.&#xA0; For all other traffic, encryption is enabled by default.</p>\n<p><strong>Check out the Docker 1.12 Swarm Mode Deep Dive:</strong></p>\n\n\n\n\t\t\t\t\t<section class=\"learn_more_content_section\"><h3>Learn More about Docker</h3></section>\t\t\t\t\t\t\t\t\t\t\t<span class=\"tags\"><a href=\"https://blog.docker.com/tag/docker-1-12/\" rel=\"tag\">docker 1.12</a>, <a href=\"https://blog.docker.com/tag/docker-swarm/\" rel=\"tag\">docker swarm</a>, <a href=\"https://blog.docker.com/tag/orchestration/\" rel=\"tag\">orchestration</a>, <a href=\"https://blog.docker.com/tag/swarm-mode/\" rel=\"tag\">swarm mode</a></span>\n\t\t\t\t\t\t\t\t</div>\t\n\t\t\t\t\n\t\t\t</div>"
    }, {
        "title": "Build a Swarm Cluster",
        "url": "https://opsnotice.xyz/build-swarm-cluster/",
        "excerpt": "The version 1.12 of Docker has been released few days ago. Among the changes, Docker-Swarm get embedded directly into the Engine that allow easier Swarm deployment. I&#x2019;m going to show that to&hellip;",
        "date_saved": "Sat Jul 30 02:13:44 EDT 2016",
        "html_file": "build_a_swarm_cluster.html",
        "png_file": "build_a_swarm_cluster.png",
        "md_file": "build_a_swarm_cluster.md",
        "content": "<div><header>\n \n \n \n </header>\n\n <section class=\"post-content\">\n <p><img src=\"https://opsnotice.xyz/content/images/2016/07/swarm-cluster.png\" alt=\"\"></p>\n\n<p>The version 1.12 of Docker has been released few days ago. Among the changes, Docker-Swarm get embedded directly into the Engine that allow easier Swarm deployment. I&#x2019;m going to show that to you. </p>\n\n<blockquote>\n <p>Docker Swarm is native clustering for Docker. It turns a pool of Docker hosts into a single, virtual Docker host.</p>\n</blockquote>\n\n<p>Earlier stages, it was quite hard to deploy a Swarm cluster, you had to generate some certs, use a service discovery, configure each node&#x2026; This time is over ! <br>\nToday, we are going to make a Swarm cluster in two commands !</p>\n\n<p>In this post, I&#x2019;ll show you how to make a small cluster between two virtual machines but you can easily repeat operations to make a larger cluster.</p>\n\n<p>I&#x2019;ll use two Ubuntu 16.04 Xenial VM and install Docker 1.12 on each.</p>\n\n<pre><code class=\"language-langage-bash\">export DEBIAN_FRONTEND=noninteractive \nsudo apt-get -y update \nsudo apt-get install apt-transport-https ca-certificates \nsudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D \nsudo echo \"deb https://apt.dockerproject.org/repo ubuntu-xenial experimental\" &gt; /etc/apt/sources.list.d/docker.list \nsudo apt-get -y update \nsudo apt-get purge lxc-docker \nsudo apt-cache policy docker-engine \nsudo apt-get -y install linux-image-extra-$(uname -r) \nsudo apt-get -y install apparmor \nsudo apt-get -y install docker-engine \nsudo service docker start \ndocker -v \n</code></pre>\n\n<p>When your VMs have Docker 1.12 installed on it, we are going to create our Swarm cluster. Launch this command on the VM you want use for Master : </p>\n\n<pre><code class=\"language-langage-docker\">docker swarm init \n</code></pre>\n\n<p>This command will init the cluster and give you, at end, the command to launch on your nodes to join us to the cluster. Launch it on other nodes : </p>\n\n<pre><code class=\"language-langage-docker\">docker swarm join --secret &lt;secret&gt; \\ \n--ca-hash sha256:&lt;hash&gt; \\\nIP_Master_Swarm:2377 \n</code></pre>\n\n<p>With this second command, your node joins directly your Swarm cluster (in two commands, no jokes). You have definitely a Docker Swarm Cluster ! Docker Swarm is too easy since 1.12</p>\n\n<p>For going a bit deeper in Swarm, we&#x2019;ll deploy a service and expose it on our nodes (port 8001/tcp) who will be load-balanced in the cluster. In our example, we&#x2019;re going to use Nginx, but you can use any other image of your choice.</p>\n\n<p>We launch these commands on the Master : </p>\n\n<pre><code class=\"language-langage-docker\">docker network create -d overlay nginx-network \ndocker service create --name nginx --network nginx-network --replicas 5 -p 8001:80/tcp nginx \n</code></pre>\n\n<p><img src=\"https://opsnotice.xyz/content/images/2016/07/nginx-swarm-task.png\" alt=\"\"></p>\n\n<p>With this command, we have created a private network for Nginx and a service (new Docker concept) who expose the port 8001 off our Swarm&#x2019;s nodes and load-balance it to 5 Nginx containers.</p>\n\n<p>After, we can easily scale up our service : </p>\n\n<pre><code class=\"language-langage-docker\">docker service scale nginx=40 \n</code></pre>\n\n<p><img src=\"https://opsnotice.xyz/content/images/2016/07/nginx-scale-40.png\" alt=\"\"></p>\n\n<p>If you want to read more about the new Swarm, <a href=\"https://docs.docker.com/engine/swarm/\">let&#x2019;s RTFM</a>.</p>\n\n<p>For me, this update makes really easier the deployment of a Swarm Cluster from scratch. This is a good point for Docker Inc in the big war or container orchestration (Kubernetes, Mesos, Rancher&#x2026;).</p>\n </section>\n\n </div>"
    }]
}
