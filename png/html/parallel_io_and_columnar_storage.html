<!DOCTYPE html><html><head><title>Parallel I/O and Columnar Storage</title></head><body>
<h1>Parallel I/O and Columnar Storage</h1><p><a href="https://eventql.io/blog/parallel-io-and-columnar-storage/" target="_new">Original URL</a></p>
<p><blockquote>Welcome to our new blog! Today's article is the first in a series of posts discussing the architecture and implementation of massively parallel databases such as Vertica [0], BigQuery [1] and EventQL&hellip;</blockquote></p>
<div><article class="content">
 
 

 
 <p><i>
Welcome to our new blog! Today's article is the first in a series of posts
discussing the architecture and implementation of massively parallel databases
such as Vertica <a href="https://eventql.io/blog/parallel-io-and-columnar-storage/#foot0">[0]</a>, BigQuery <a href="https://eventql.io/blog/parallel-io-and-columnar-storage/#foot1">[1]</a> and EventQL <a href="https://eventql.io/blog/parallel-io-and-columnar-storage/#foot2">[2]</a>
</i></p>

<p><i>
We begin with a high level overview of the system while follow up posts will discuss
specific components in more detail. The target audience are software and systems engineers
with an interest in databases and distributed systems.
</i></p>

<h3>The Challenge</h3>

<p>Let's start off with a challenge: We're given a table with 100TB of web tracking
data collected over a period of a few days. Here is a small sample of rows from
the table:</p>

<p><img src="https://eventql.io/blog/parallel-io-and-columnar-storage/tracking_data.png"></p>

<p>Our goal is to answer queries like <i>"How many people visited the page '/account/signup'
in the last 10 days?"</i>. The are only two rules: The query is not known beforehand
and we must answer it in less than a second. Here is the same example query in SQL:</p>

<pre><code>SELECT count(1)
FROM tracking_data
WHERE url = '/account/signup' AND time &gt; time_at("-10d");
</code></pre>

<p>If that doesn't sound like an interesting challenge yet, consider this quick
back-of-the-envelope calculation: Assuming the average hard disk can read roughly
200MB per second (sequentially), loading 100TB from disk will take 500k disk-seconds
or about 138 disk hours.</p>

<p>Oops. 138 hours is almost 6 days. A long way from "less than a second". We're
seven orders of magnitude off and that's before we even started to process any
of the data &#x2014; just to read it from disk.</p>



<p>How can we still solve the challenge? We can't make a single disk go any faster
but we can break up the data set into smaller pieces, put each piece on its own
disk and read all pieces from all disks in parallel. If we distributed the data
over 500k individual disks we could read our whole data set in one second.</p>

<p><img src="https://eventql.io/blog/parallel-io-and-columnar-storage/parallel_query.png"></p>

<p>There is only one small problem with that scheme &#x2014; half a million disks
cost <em>a lot</em> of money.</p>

<p>So even if we use a lot of machines, reading the full data set from disk
in less than a second is utterly out of reach.</p>



<p>Can we still solve the challenge? Yes, but I'm afraid we'll have to cheat &#x2014;
maybe we can answer the query without actually reading all the data from disk.</p>

<p>If we could come up with an algorithm which computes the query result after
reading only .01% of the data (or 10GB) from disk, then we could return an answer
in one second using just fifty disks. Fifty disks could be hosted in a dozen
servers. Finally, that sounds reasonable!</p>

<p>But how do we compute the answer for the full dataset while reading only .01% of
the data from disk? One approach would be to use sampling and probabalistic algorithms.
However, sampling would give us <i>approximately</i> correct results. Approximate
results are great for some use cases and not so great to unworkable for others.</p>

<p>There is another trick we can use to minimze the amount of data to be read from
disk while always giving correct results. The technique is called "column-oriented"
or "columnar" storage.</p>

<h3>Columnar Storage</h3>

<p>To really understand the benefit of columnar storage for data anlytics we first
have to look at how regular <em>row-oriented</em> databases store tables on disk:</p>

<p>It turns out they do it pretty much like you'd expect them to. They basically
keep a file somwhere which contains all the rows in the table. One row after
another. Hence the name "row-oriented".</p>

<p><img src="https://eventql.io/blog/parallel-io-and-columnar-storage/row_oriented.png">
</p><p class="img_sub">A row-oriented file conceptually looks something like this</p>

<p>What's problematic about row-oriented databases is that to execute a query, they
always have to read the full rows from disk. Even if just a small part of each
row is required to answer the query, the database still has to read every row
in full. <a href="https://eventql.io/blog/parallel-io-and-columnar-storage/#foot3">[3]</a></p>

<p>The not completely intuitive reason for this is that hard disks are only fast if
you read a file sequentially, i.e. only if you read one byte after another.
Jumping around within a file performs very poorly. So poorly in fact, that reading
whole rows is practically always <em>faster</em> than reading partial rows. <a href="https://eventql.io/blog/parallel-io-and-columnar-storage/#foot4">[4]</a></p>

<p>Consider the following example query which calculates the number of page views per
minute:</p>

<pre><code>SELECT time, count(1)
FROM tracking_data
GROUP BY date_trunc("1min", time);
</code></pre>

<p>To compute the answer, we only need to know the value of the <code>time</code> column of
each row. We're not interested in the <code>session_id</code>, <code>url</code> or any of the potentially
hundreds of other columns of the table.</p>

<p>Ideally, we would only load the <code>time</code> column of each row from disk when
executing the query. But we just saw that row-oriented storage can't do that
efficiently. We always have to read the full rows no matter what.</p>

<p>Depending on the table and query, we could be spending 99% of the time reading
data from disk which we're not going to need to answer the query.</p>



<p>This is the problem column-oriented storage tries to solve. The basic idea is that
instead of storing one row after another, we can break up the row into the individual
columns and then store one column after another.</p>

<p>If, for example, our table contained one thousand rows with each three columns
<code>time</code>, <code>session_id</code> and <code>url</code>, we would first store an array of a thousand <code>time</code>
values, then another array of a thousand <code>session_id</code> values and finally an array of
a thousand <code>url</code> values.</p>

<p>You might have come across the concept before under a different name: What we're
doing is basically vectorization <a href="https://eventql.io/blog/parallel-io-and-columnar-storage/#foot5">[5]</a>.</p>

<p><img src="https://eventql.io/blog/parallel-io-and-columnar-storage/columnar_storage.png"></p>

<p>Storing each column seperately has two desirable properties. The first and most
obvious one is that it allows us to also read each column separately &#x2014; we don't
have to load all the extraneous columns from disk anymore.</p>

<p>The second and less obvious upside of columnar storage is that we can compress
the data very efficiently. The compression further reduces the number of bytes
we actually have to fetch from disk to read a row.</p>

<p>To see why compression in columnar files can be very significant, imagine our
table had a fourth <code>is_customer</code> column. Sadly, we don't have any customers yet
so the field is always <code>false</code>.</p>

<p>In a row-oriented database, storing the <code>is_customer</code> field for one million rows
would take at least 1MB (one byte per boolean). In a columnar database we can store
all one million values in a single byte &#x2014; a 1000000x improvement. <a href="https://eventql.io/blog/parallel-io-and-columnar-storage/#foot6">[6]</a></p>



<p>Lastly, it should be noted that columnar storage also has a big downside: It's
less efficient to perform updates on columnar files than on row-oriented files.
So you probably won't see the classical OLTP databases like MySQL
switching to columnar any time soon. Still, for analytical queries on large data
sets columnar is clearly the way to go.</p>

<h3>Are we there yet?</h3>

<p>Looks like we have finally put together a scheme which will allow us to excute a
SQL query on 100TB of data in less than a second, even though just reading the
data from disk would have taken many days.</p>

<p>Let's recapitulate our approach: We're going to split up the data into many small
pieces, then distribute the pieces among a dozen or so machines. On each machine,
we will cheat by storing the rows in columnar format and only actually reading a
small subset of the compressed data to answer the query.</p>

<p>Of course, we have only scratched the surface of the problem so far. In the
<a href="https://eventql.io/blog/dividing-infinity-distributed-partitioning-schemes/">next post</a> we
will discuss how exactly we're going to split up the data into pieces.</p>

<p>You subscribe to email updates for upcoming posts or the rss feed in the
sidebar.</p>



<h4>Next up in the series:</h4>




 <p>
 Tagged with massively parallel databases, internals, eventql, bigquery, vertica, sql, columnar storage
 </p>

 

 <div class="box">
 <h4>Want to learn more about EventQL?</h4>
 <a href="https://eventql.io/" class="button small">Learn more</a>
 </div>
 </article>

 


 </div>
</body></html>
