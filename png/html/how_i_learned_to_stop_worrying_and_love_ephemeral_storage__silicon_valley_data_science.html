<!DOCTYPE html><html><head><title>How I Learned to Stop Worrying and Love Ephemeral Storage - Silicon Valley Data Science</title></head><body>
<h1>How I Learned to Stop Worrying and Love Ephemeral Storage - Silicon Valley Data Science</h1><p><a href="http://www.svds.com/learned-stop-worrying-love-ephemeral-storage/" target="_new">Original URL</a></p>
<p><blockquote>There are many benefits to running data platforms in the cloud&#x2014;elasticity of infrastructure, simplification of management and monitoring, and agility of deployment and expansion. While there&hellip;</blockquote></p>
<div><div class="article-holder">
	<header>
		
		
	</header>
	<p>There are many benefits to running data platforms in the cloud&#x2014;elasticity of infrastructure, simplification of management and monitoring, and agility of deployment and expansion. While there are several good resources for deploying a Hadoop cluster on Amazon&#x2019;s cloud, such as Cloudera&#x2019;s <a href="http://www.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_aws.pdf">AWS Reference Architecture</a> and <a href="https://media.amazonwebservices.com/AWS_Amazon_EMR_Best_Practices.pdf">AWS EMR Best Practices</a>, the steps to storing data in a persistent and reliable data store are not as clear cut.</p>
<p>This post will show architects and developers how to set up Hadoop to communicate with S3, use Hadoop commands directly against S3, use distcp to perform transfers between Hadoop and S3, and how distcp can be used to update on a regular basis based only on differences. I&#x2019;ve also included a quick look at other considerations to keep in mind when putting this all together.</p>
<p>At SVDS, one architecture pattern we use with our clients is to store HDFS data blocks in Hadoop clusters using local instance storage. Until recently this was the pattern recommended by Cloudera, however there is now also support for using EBS volumes for data nodes. Which pattern you choose will ultimately depend on the instance types you need to support and the tradeoffs between instance and EBS storage. For clusters that need to support interactive data science investigations, this deployment mode preserves data locality and offers better performance than querying directly against S3.</p>
<p>Note: this is not the only way to deploy Hadoop clusters; support to query against S3 directly is available from <a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-file-systems.html">EMR</a>, and for tools such as <a href="http://www.cloudera.com/documentation/enterprise/latest/topics/impala_s3.html">Impala</a>, <a href="http://www.cloudera.com/documentation/enterprise/latest/topics/spark_s3.html">Spark</a>, <a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hive-additional-features.html">Hive</a>, and <a href="http://techblog.netflix.com/2014/10/using-presto-in-our-big-data-platform.html">Presto</a>. We will discuss more options in future posts.</p>
<p>The issue with using local instance storage is that it&#x2019;s ephemeral. If a server goes down, whether it is stopped or due to failure, data on instance storage is lost.</p>
<p>Yes, this is <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html">true</a>. However, using local instance storage is also how you can take advantage of physically attached storage on servers in AWS. In the case of Hadoop clusters, assuming data is replicated, all of the data nodes would need to go down for there to be data loss. Since this is possible (data center outage, for example), you must protect against the loss.</p>
<p>How do you do so? The short-answer is to use <a href="http://hadoop.apache.org/docs/stable/hadoop-distcp/DistCp.html">distcp</a>. Distcp is used to <a href="http://www.cloudera.com/documentation/enterprise/latest/topics/cdh_admin_distcp_data_cluster_migrate.html">perform parallel copies</a> between Hadoop clusters. In addition, it comes, out of the box, with <a href="https://wiki.apache.org/hadoop/AmazonS3">support</a> to read and write from S3. It is an efficient tool for data transfers as you can take advantage of the parallelism of the Hadoop cluster and of S3 to maximize throughput.</p>
<p>We&#x2019;ll start with how to setup Hadoop so it can communicate with S3.</p>
<h2>Setting Up Hadoop S3 configuration</h2>
<p>There are a couple of <a href="http://www.cloudera.com/documentation/enterprise/latest/topics/spark_s3.html">ways</a> to enable Hadoop to communicate with S3. You can add the AWS Access Key and Secret Access Key to your Hadoop configuration by updating the core-site.xml with the following values:</p>
<pre class="brush: bash; title: ; notranslate" title="">&amp;amp;amp;lt;property&amp;amp;amp;gt;
 &amp;amp;amp;lt;name&amp;amp;amp;gt;fs.s3a.access.key&amp;amp;amp;lt;/name&amp;amp;amp;gt;
 &amp;amp;amp;lt;value&amp;amp;amp;gt;[access-key]&amp;amp;amp;lt;/value&amp;amp;amp;gt;
&amp;amp;amp;lt;/property&amp;amp;amp;gt;
&amp;amp;amp;lt;property&amp;amp;amp;gt;
 &amp;amp;amp;lt;name&amp;amp;amp;gt;fs.s3a.secret.key&amp;amp;amp;lt;/name&amp;amp;amp;gt;
 &amp;amp;amp;lt;value&amp;amp;amp;gt;[secret-access-key]&amp;amp;amp;lt;/value&amp;amp;amp;gt;
&amp;amp;amp;lt;/property&amp;amp;amp;gt;</pre>
<p>Alternatively, you can pass the credentials with every command you make to S3:</p>
<pre class="brush: bash; title: ; notranslate" title="">hdfs dfs -ls s3a://[access-key]:[secret-access-key]@[bucket-name]/</pre>
<p>Which way you choose to implement will depend on how you want to secure your keys and your specific environment setup. With updating the configuration, you run the risk that the keys could be checked into Git and accessible by others (don&#x2019;t do this). With running from the command line, you run the risk that the keys are logged on the terminal. Your choice ultimately depends on where you want to exercise your security constraints.</p>
<h2>Simple HDFS Commands</h2>
<p>Let&#x2019;s assume you&#x2019;ve created an S3 bucket called has-bucket. Here are some good <a href="http://docs.rightscale.com/faq/clouds/aws/What_are_valid_S3_bucket_names.html">guidelines</a> to choosing a compliant name for your S3 bucket.</p>
<p>A good first test is to verify that Hadoop commands against S3 work. For example, you can access files in this bucket through HDFS fs commands:</p>
<pre class="brush: bash; title: ; notranslate" title="">hdfs dfs -ls -h s3a://has-bucket/</pre>
<p>If this command goes through successfully and you are able to see the contents of your bucket, then you know that the Hadoop S3 configuration should be set.</p>
<p>Other commands you can try are:</p>
<pre class="brush: bash; title: ; notranslate" title="">hdfs dfs -put lorem_ipsum.txt s3a://has-bucket/
hdfs dfs -cat s3a://has-bucket/lorem_ipsum.txt</pre>
<p>Some commands are supported and some don&#x2019;t have an effect. We won&#x2019;t focus on this as part of this post, but feel free to experiment in your own sandbox environment.</p>
<h2>Using Distcp for data transfers between HDFS and S3</h2>
<p>Now that we have confirmed integration between Hadoop and S3, we can begin using distcp. In general, the format of distcp is as follows:</p>
<pre class="brush: bash; title: ; notranslate" title="">hadoop distcp [options] &amp;amp;amp;lt;src-url&amp;amp;amp;gt; &amp;amp;amp;lt;dest-url&amp;amp;amp;gt;</pre>
<p>For example, if we want to copy a folder from HDFS to S3, we would use:</p>
<pre class="brush: bash; title: ; notranslate" title="">hadoop distcp hdfs://nameservice1/user/svds/walrus/ s3a://has-bucket/</pre>
<p>This command copies the folder itself and all of its contents to the destination directory. Therefore, you would have the <code>walrus</code> folder in <code>s3://has-bucket/</code> when using the default distcp command.</p>
<p>You can also pass parameters to only update the destination directory based on the differences with source (i.e. -update) or completely overwrite the destination (i.e. -overwrite). However, the behavior of the command is different when using the -update or -overwrite parameters. In this case, the contents of the directory are copied, but not the folder itself.</p>
<p>If we run the following command:</p>
<pre class="brush: bash; title: ; notranslate" title="">hadoop distcp -update hdfs://nameservice1/user/svds/walrus/ s3a://has-bucket/</pre>
<p>The <em>contents</em> of the walrus directory are compared against has-bucket and whatever files are different are then copied over. If we were trying to compare against the <code>s3a://has-bucket/walrus</code> folder we previously copied over, this command would not work as expected and would instead copy the entire contents of the walrus directory to has-bucket, basically creating duplicates.</p>
<p>What we want is to run an update against the <code>walrus</code> directory itself. To do this we need to run the command as follows:</p>
<pre class="brush: bash; title: ; notranslate" title="">hadoop distcp -update hdfs://nameservice1/user/svds/walrus/ s3a://has-bucket/walrus/</pre>
<p>This process will only copy the files that do not exist in the destination or those that have changed since the last copy. Note that this is not a sync operation; you need to flip the source and destination if you need to copy any changes made in S3 back down to your HDFS cluster.</p>
<h2>Setting up a job to regularly update data in S3</h2>
<p>Now that we understand the behavior of distcp, we can schedule a regular job to update the data in S3 with changes made in the Hadoop cluster. This provides greater durability to the data and makes the data available to be used by other applications and cloud services through S3.</p>
<p>In the example below, we&#x2019;ll schedule it using a cron job, however you can also use Oozie or another workflow scheduler to trigger this job. First, I&#x2019;ll add all of my hadoop distcp commands to a shell script called hadoop_s3_backup.sh. Then, I can use cron to schedule to run this script every day at 2 AM.</p>
<pre class="brush: bash; title: ; notranslate" title="">0 2 data.json data_ml developing_a_street_basketball_game_part_i_getting_workflow_ready_—_whitestormjs_framework.epub developing_a_street_basketball_game_part_i_getting_workflow_ready_—_whitestormjs_framework.md devops docker embedded from_zero_to_frontend_hero_part_1_—_free_code_camp.epub from_zero_to_frontend_hero_part_1_—_free_code_camp.md gaming gdb-example-ncurses.epub github go how_to_check_diskspace_with_the_ncdu_utility.epub how_to_check_diskspace_with_the_ncdu_utility.md html immutable_collections_for_javascript.epub immutable_collections_for_javascript.md javascript linux mobile node-hero-tutorials.epub png programming science social task_automation_with_npm_run.md tidy understanding_javascript_promises_pt_i_background__basics.epub understanding_javascript_promises_pt_i_background__basics.md understanding_object_streams.epub understanding_object_streams.md url_to_filename.csv web_dev writing_a_javascript_framework__execution_timing_beyond_settimeout.epub writing_a_javascript_framework__execution_timing_beyond_settimeout.md writing_a_javascript_framework__project_structuring.epub writing_a_javascript_framework__project_structuring.md data.json data_ml developing_a_street_basketball_game_part_i_getting_workflow_ready_—_whitestormjs_framework.epub developing_a_street_basketball_game_part_i_getting_workflow_ready_—_whitestormjs_framework.md devops docker embedded from_zero_to_frontend_hero_part_1_—_free_code_camp.epub from_zero_to_frontend_hero_part_1_—_free_code_camp.md gaming gdb-example-ncurses.epub github go how_to_check_diskspace_with_the_ncdu_utility.epub how_to_check_diskspace_with_the_ncdu_utility.md html immutable_collections_for_javascript.epub immutable_collections_for_javascript.md javascript linux mobile node-hero-tutorials.epub png programming science social task_automation_with_npm_run.md tidy understanding_javascript_promises_pt_i_background__basics.epub understanding_javascript_promises_pt_i_background__basics.md understanding_object_streams.epub understanding_object_streams.md url_to_filename.csv web_dev writing_a_javascript_framework__execution_timing_beyond_settimeout.epub writing_a_javascript_framework__execution_timing_beyond_settimeout.md writing_a_javascript_framework__project_structuring.epub writing_a_javascript_framework__project_structuring.md data.json data_ml developing_a_street_basketball_game_part_i_getting_workflow_ready_—_whitestormjs_framework.epub developing_a_street_basketball_game_part_i_getting_workflow_ready_—_whitestormjs_framework.md devops docker embedded from_zero_to_frontend_hero_part_1_—_free_code_camp.epub from_zero_to_frontend_hero_part_1_—_free_code_camp.md gaming gdb-example-ncurses.epub github go how_to_check_diskspace_with_the_ncdu_utility.epub how_to_check_diskspace_with_the_ncdu_utility.md html immutable_collections_for_javascript.epub immutable_collections_for_javascript.md javascript linux mobile node-hero-tutorials.epub png programming science social task_automation_with_npm_run.md tidy understanding_javascript_promises_pt_i_background__basics.epub understanding_javascript_promises_pt_i_background__basics.md understanding_object_streams.epub understanding_object_streams.md url_to_filename.csv web_dev writing_a_javascript_framework__execution_timing_beyond_settimeout.epub writing_a_javascript_framework__execution_timing_beyond_settimeout.md writing_a_javascript_framework__project_structuring.epub writing_a_javascript_framework__project_structuring.md /home/mauricio/hadoop_s3_backup.sh</pre>
<h2>Other Considerations</h2>
<p>Here are a few important things to consider when following this pattern:</p>
<ul>
<li>File permissions are not preserved when storing data in S3. When transferring data from S3 to Hadoop, the permissions of the files being copied will be based on the user who runs the distcp command. Because of this, it is important that a process is created to update HDFS-based file permissions and ownership after the distcp step from S3 to HDFS.</li>
<li>AWS provides a <a href="https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/">VPC to S3 endpoint</a> which should seriously be considered if the Hadoop cluster lives in a VPC. This provides a secure and reliable way to connect from Hadoop to S3 without needing to go through a NAT or the Internet Gateway, even if the Hadoop cluster resides in a private subnet.</li>
<li>AWS provides a S3DistCp process that comes with EMR and optimizes transfers between Hadoop and S3. S3DistCp should be considered as an alternative to distcp in this context.</li>
</ul>
<h2>Conclusion</h2>
<p>Hopefully this post helps provide a better idea of how to use distcp to regularly backup your data from ephemeral storage to S3. This allows you to protect your data from disaster and decouples it from Hadoop so that it can be used by other tools and cloud services against S3 directly.</p>
<p>Have any questions or a different approach to this problem? Feel free to post them in the comments section below, or reach out to us via <a href="https://twitter.com/SVDataScience">Twitter</a>.<br>
<em><br>
Thanks to Rick Drushal for pointing me to the S3 VPC endpoint reference and for others who have helped edit and review this blog. </em></p>
	
	
	
	
</div>												</div>
</body></html>
