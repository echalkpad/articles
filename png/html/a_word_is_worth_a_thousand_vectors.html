<!DOCTYPE html><html><head><title>A Word is Worth a Thousand Vectors</title></head><body>
<h1>A Word is Worth a Thousand Vectors</h1><p><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/" target="_new">Original URL</a></p>
<p><blockquote>Standard natural language processing (NLP) is a messy and difficult affair. It requires teaching a computer about English-specific word ambiguities as well as the hierarchical, sparse nature of words&hellip;</blockquote></p>
<div><section class="text-content">
 <p>Standard natural language processing (NLP) is a messy and difficult affair. It requires teaching a computer about English-specific word ambiguities as well as the hierarchical, sparse nature of words in sentences. At Stitch Fix, word vectors help computers learn from the raw text in customer notes. Our systems, composed of machines and human experts, need to recommend the maternity line when she says she&#x2019;s in her &#x2018;third trimester&#x2019;, identify a medical professional when she writes that she &#x2018;used to wear scrubs to work&#x2019;, and distill &#x2018;taking a trip&#x2019; into a Fix for vacation clothing.</p>

<p>While we&#x2019;re not totally &#x201C;there&#x201D; yet with the holy grail to NLP, word vectors (also referred to as distributed representations) are an amazing tool that sweeps away some of the issues of dealing with human language. The machines work in tandem with the stylists as a support mechanism to help identify and summarize textual information from the customers. The human experts will make the final call on what actions will be taken. The goal of this post is to be a motivating introduction to word vectors and demonstrate their real-world utility.</p>

<p>The following example set the natural language community afire<a name="footnote1-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote1">1</a></sup> back in 2013:</p>



<p>In this example, a human posed a question to a computer: what is <code class="highlighter-rouge">king - man + woman</code>? This is similar to an SAT-style analogy (<code class="highlighter-rouge">man</code> is to <code class="highlighter-rouge">woman</code> as <code class="highlighter-rouge">king</code> is to what?). And a computer solved this equation and answered: <code class="highlighter-rouge">queen</code>. Under the hood, the machine gets that the biggest difference between the words for man and woman is gender. Add that gender difference to <code class="highlighter-rouge">king</code>, and you get <code class="highlighter-rouge">queen</code>.</p>

<p><strong>This is astonishing because we&#x2019;ve never explicitly taught the machine anything about gender!</strong></p>

<p>In fact, we&#x2019;ve never handed the computer anything like a dictionary, a thesaurus, or a network of word relationships. We haven&#x2019;t even tried to break apart a sentence into its constituent parts of speech<a name="footnote2-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote2">2</a></sup>. We&#x2019;ve simply fed a mountain of text into an algorithm called <a href="https://code.google.com/p/word2vec/">word2vec</a> and expected it to learn from context. Word by word, it tries to predict the other surrounding words in a sentence. Or rather, it internally represents words as vectors, and given a word vector, it tries to predict the other word vectors in the nearby text<a name="footnote3-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote3">3</a></sup>.</p>

<p>The algorithm eventually sees so many examples that it can infer the gender of a single word, that both the The Times and The Sun are newspapers, that The Matrix is a sci-fi movie, and that the style of an article of clothing might be boho or edgy. That word vectors represent much of the information available in a dictionary definition is a convenient and almost miraculous side effect of trying to predict the context of a word.</p>

<p>Internally high dimensional vectors stand in for the words, and some of those dimensions are encoding gender properties. Each axis of a vector encodes a property, and the magnitude along that axis represents the relevance of that property to the word<a name="footnote4-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote4">4</a></sup>. If the gender axis is more positive, then it&#x2019;s more feminine; more negative, more masculine.</p>

<p><img src="http://multithreaded.stitchfix.com/assets/images/blog/vectors.gif" alt="Vectors"></p>

<p>Applied appropriately,<strong>word vectors are dramatically more meaningful and more flexible than current techniques</strong><a name="footnote5-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote5">5</a></sup> and let computers peer into text in a fundamentally new way. It&#x2019;s surprisingly easy to get started using libraries like <a href="http://radimrehurek.com/2014/02/word2vec-tutorial/">gensim</a> (in Python) or <a href="http://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html#word2vec">Spark</a> (in Scala &amp; Python) &#x2013; all you need to know is how to add, subtract, and multiply vectors!</p>

<p>Let&#x2019;s review the new abilities that word vectors grant us.</p>

<h2 id="similar-words-are-nearby-vectors">Similar words are nearby vectors</h2>
<p>Similar words are nearby vectors in a vector space. This is a powerful convention since it lets us wipe away a lot of the noise and nuance in vocabulary. For example, let&#x2019;s use <a href="https://radimrehurek.com/gensim/">gensim</a> to find a list of words similar to <code class="highlighter-rouge">vacation</code> using the freebase skipgram data<a name="footnote6-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote6">6</a></sup>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
<span class="n">fn</span> <span class="o">=</span> <span class="s">"freebase-vectors-skipgram1000-en.bin.gz"</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">'vacation'</span><span class="p">)</span>

<span class="c"># [('trip', 0.7234684228897095),</span>
<span class="c"># ('honeymoon', 0.6447688341140747),</span>
<span class="c"># ('beach', 0.6249285936355591),</span>
<span class="c"># ('vacations', 0.5868890285491943),</span>
<span class="c"># ('wedding', 0.5541957020759583),</span>
<span class="c"># ('resort', 0.5231006145477295),</span>
<span class="c"># ('traveling', 0.5194448232650757),</span>
<span class="c"># ('vacation.', 0.5068142414093018),</span>
<span class="c"># ('vacationing', 0.5013546943664551)]</span>
</code></pre>
</div>

<p>We&#x2019;ve calculated the vectors most similar to the vector for <code class="highlighter-rouge">vacation</code>, and then looked up what words those vectors represent. As we read the list, we note that these words aren&#x2019;t just similar in vector space, but that they make sense intuitively too.</p>

<p>In this case, we&#x2019;ve looked for vectors that are nearby to the word <code class="highlighter-rouge">vacation</code> by measuring the similarity (usually cosine similarity) to the root word and sorting by that.</p>











<div id="legend">
<p class="l1">Destinations</p>
<p class="l2">Vacation</p>
<p class="l3">Season</p>
<p class="l4">Holidays</p>
<p class="l5">Wedding</p>
<p class="l6">Month</p>
</div>


<p>Above is an interactive visualization of the words nearest to vacation. The more similar a word to it&#x2019;s genre, the larger the radius of the marker. Hover over the bubbles to reveal the words they represent<a name="footnote7-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote7">7</a></sup>.</p>

<p>And these words aren&#x2019;t just nearby; they&#x2019;re also in several clusters. So we can determine that the words most similar to <code class="highlighter-rouge">vacation</code> come in a variety of flavors: one cluster might be <code class="highlighter-rouge">wedding</code>-related, but another might relate to destinations like <code class="highlighter-rouge">Belize</code>.</p>

<p>Of course our human stylists understand when a client says &#x201C;I&#x2019;m going to Belize in March&#x201D; that she has an upcoming vacation. But the computer can potentially tag this as a &#x2018;vacation&#x2019; fix because the word vector for <code class="highlighter-rouge">Belize</code> is similar to that for <code class="highlighter-rouge">vacation</code>. We can then make sure that the Fixes our customers get are vacation-appropriate!</p>


<p>We have the ability to search semantically by adding and subtracting word vectors<a name="footnote8-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote8">8</a></sup>. This empowers us to creatively add and subtract concepts and ideas. Let&#x2019;s start with a style we know a customer liked, <code class="highlighter-rouge">item_3469</code>:</p>

<p><img src="http://multithreaded.stitchfix.com/assets/images/blog/vectors_image1.png" alt="Vectors"></p>

<p>Our customer recently became pregnant, so let&#x2019;s try and find something like <code class="highlighter-rouge">item_3469</code> but along the <code class="highlighter-rouge">pregnant</code> dimension:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">'ITEM_3469'</span><span class="p">,</span> <span class="s">'pregnant'</span><span class="p">)</span>
<span class="n">matches</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s">'ITEM_'</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">matches</span><span class="p">))</span>

<span class="c"># ['ITEM_13792',</span>
<span class="c"># 'ITEM_11275',</span>
<span class="c"># 'ITEM_11868']</span>
</code></pre>
</div>

<p>Of course the item IDs aren&#x2019;t immediately informative, but the pictures let us know that we&#x2019;ve done well:</p>

<p><img src="http://multithreaded.stitchfix.com/assets/images/blog/vectors_images.png" alt=""></p>

<p>The first two are items have prominent black &amp; white stripes like <code class="highlighter-rouge">item_3469</code> but have the added property that they&#x2019;re great maternity-wear. The last item changes the pattern away from stripes but is still a loose blouse that&#x2019;s great for an expectant mother. Here we&#x2019;ve simply added the word vector for <code class="highlighter-rouge">pregnant</code> to the word vector for <code class="highlighter-rouge">item_3469</code>, and looked up the word vectors most similar to that result<a name="footnote9-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote9">9</a></sup>.</p>

<p>Our stylists tailor each Fix to their clients, and this prototype system may free them to mix and match artistic concepts about style, size and fit to creatively search for new items.</p>

<h2 id="summarizing-sentences--documents">Summarizing sentences &amp; documents</h2>
<p>At Stitch Fix, we work hard to craft a uniquely-styled Fix for each of our customers. At every stage of a Fix we collect feedback: what would you like in your next Fix? What did you think of the items we sent you? What worked? What didn&#x2019;t?</p>

<p>The spectrum of responses is myriad, but vectorizing those sentences<a name="footnote10-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote10">10</a></sup> allows us to begin systematically categorizing those documents:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Doc2Vec</span>
<span class="n">fn</span> <span class="o">=</span> <span class="s">"word_vectors_blog_post_v01_notes"</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Doc2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">'pregnant'</span><span class="p">)</span>
<span class="n">matches</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s">'SENT_'</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">matches</span><span class="p">))</span>

<span class="c"># ['...I am currently 23 weeks pregnant...',</span>
<span class="c"># '...I'm now 10 weeks pregnant...',</span>
<span class="c"># '...not showing too much yet...',</span>
<span class="c"># '...15 weeks now. Baby bump...',</span>
<span class="c"># '...6 weeks post partum!...',</span>
<span class="c"># '...12 weeks postpartum and am nursing...',</span>
<span class="c"># '...I have my baby shower that...',</span>
<span class="c"># '...am still breastfeeding...',</span>
<span class="c"># '...I would love an outfit for a baby shower...']</span>
</code></pre>
</div>

<p>In this example we calculate which sentences are closest to the word <code class="highlighter-rouge">pregnant</code>. This list also skips over many literal matches of <code class="highlighter-rouge">pregnant</code> in order to demonstrate the more advanced capabilities. We&#x2019;ve also censored sentences to keep out personally identifying text. Also note that the last sentence is a false positive: while similar to the word pregnant, she&#x2019;s unlikely to be interested in maternity clothing.</p>

<p>This allows us to understand not just what words mean, but condense our client comments, notes, and requests in a quantifiable way. We can for example categorize our sentences by first calculating the similarity between a sentence and a word:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_vector</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
 <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0norm</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">calculate_similarity</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
 <span class="n">vec_a</span> <span class="o">=</span> <span class="n">get_vector</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
 <span class="n">vec_b</span> <span class="o">=</span> <span class="n">get_vector</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
 <span class="n">sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">)</span>
 <span class="k">return</span> <span class="n">sim</span>
<span class="n">calculate_similarity</span><span class="p">(</span><span class="s">'SENT_47973, '</span><span class="n">casual</span><span class="s">')</span><span class="s"># 0.308</span></code></pre>
</div>

<p>We calculated the overlap between a sentence with label <code class="highlighter-rouge">SENT_47973</code> and the word <code class="highlighter-rouge">casual</code>. The sentence is previously trained from this customer text: &#x2018;I need some weekend wear. Comfy but stylish.&#x2019; The similarity to <code class="highlighter-rouge">casual</code> is about 0.308, which is pretty high.</p>

<p>Having built a function that computes the similarity between a sentence and a word, we can build a table of customer comments and their similarities to a given topic:</p>

<table>
 <thead>
 <tr>
 <th>raw text snippets</th>
 <th>&#x2018;broken&#x2019;</th>
 <th>&#x2018;casual&#x2019;</th>
 <th>&#x2018;pregnant&#x2019;</th>
 </tr>
 </thead>
 <tbody>
 <tr>
 <td>&#x2018;&#x2026; unfortunately the lining <strong>ripped</strong> after wearing if twice &#x2026;&#x2019;</td>
 <td>0.281</td>
 <td>0.082</td>
 <td>0.062</td>
 </tr>
 <tr>
 <td>&#x2018;&#x2026; I need some weekend wear. <strong>Comfy</strong> but stylish.&#x2019;</td>
 <td>0.096</td>
 <td>0.308</td>
 <td>0.191</td>
 </tr>
 <tr>
 <td>&#x2018;&#x2026; 12 weeks <strong>postpartum</strong> and am nursing &#x2026;&#x2019;</td>
 <td>0.158</td>
 <td>0.110</td>
 <td>0.378</td>
 </tr>
 </tbody>
</table>

<p><br>
A table like this around helps us quickly answer how many people are looking for comfortable clothes or finding defects in the clothing we send them.</p>

<h2 id="what-we-didnt-mention">What we didn&#x2019;t mention</h2>
<p>While word vectorization is an elegant way to solve many practical text processing problems, it does have a few shortcomings and considerations:</p>

<ol>
 <li>
 <p><strong>Word vectorization requires a lot of text.</strong> You can <a href="https://code.google.com/p/word2vec/#Pre-trained_word_and_phrase_vectors">download pretrained word vectors</a> yourself, but if you have a highly specialized vocabulary then you&#x2019;ll need to train your own word vectors and have a lot of example text. Typically this means hundreds of millions of words, which is the equivalent of 1,000 books, 500,000 comments, or 4,000,000 tweets.</p>
 </li>
 <li>
 <p><strong>Cleaning the text.</strong> You&#x2019;ll need to clean the words of punctuation and normalize Unicode<a name="footnote11-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote11">11</a></sup> characters, which can take significant manual effort. In this case, there are a few tools that can help like <a href="https://github.com/LuminosoInsight/python-ftfy">FTFY</a>, <a href="http://honnibal.github.io/spaCy/">SpaCy</a>, <a href="http://www.nltk.org/">NLTK</a>, and the <a href="http://nlp.stanford.edu/software/corenlp.shtml">Stanford Core NLP</a>. SpaCy even comes with word vector support built-in.</p>
 </li>
 <li>
 <p><strong>Memory &amp; performance.</strong> The training of vectors requires a high-memory and high-performance multicore machine. Training can take several hours to several days but shouldn&#x2019;t need frequent retraining. If you use pretrained vectors, then this isn&#x2019;t an issue.</p>
 </li>
 <li>
 <p><strong>Databases.</strong> Modern SQL systems aren&#x2019;t well-suited to performing the vector addition, subtraction and multiplication searching in vector space requires. There are a few libraries that will help you quickly find the most similar items<a name="footnote12-return"></a><sup><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote12">12</a></sup>: <a href="https://github.com/spotify/annoy">annoy</a>, <a href="http://scikit-learn.org/dev/modules/neighbors.html#ball-tree">ball trees</a>, <a href="http://scikit-learn.org/dev/modules/neighbors.html#mathematical-description-of-locality-sensitive-hashing">locality-sensitive hashing</a> (LSH) or <a href="http://www.cs.ubc.ca/research/flann/">FLANN</a>.</p>
 </li>
 <li>
 <p><strong>False-positives &amp; exactness.</strong> Despite the impressive results that come with word vectorization, no NLP technique is perfect. Take care that your system is robust to results that a computer deems relevant but an expert human wouldn&#x2019;t.</p>
 </li>
</ol>

<h2 id="conclusion">Conclusion</h2>
<p>The goal of this post was to convince you that <strong>word vectors give us a simple and flexible platform for understanding text.</strong> We&#x2019;ve covered a few diverse examples that should help build your confidence in developing and deploying NLP systems and what problems they can solve. While most coverage of word vectors has been from a scientific angle, or demonstrating toy examples, we at Stitch Fix think this technology is ripe for industrial application.</p>

<p>In fact, Stitch Fix is the perfect testbed for these kinds of new technologies: with expert stylists in the loop, we can move rapidly on new and prototypical algorithms without worrying too much about edge and corner cases. The creative world of fashion is one of the few domains left that computers don&#x2019;t understand. If you&#x2019;re interested in helping us break down that wall, <a href="http://technology.stitchfix.com/jobs/index.html">apply</a>!</p>

<h2 id="further-reading">Further reading</h2>
<p>There are a few miscellaneous topics that we didn&#x2019;t have room to cover or were too peripheral:</p>

<ol>
 <li>
 <p>There&#x2019;s an excellent nuts and bolts <a href="http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf">explanation and derivation</a> of the word2vec algorithm. There&#x2019;s a similarly useful <a href="http://nbviewer.ipython.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb">iPython Notebook version</a> too.</p>
 </li>
 <li>
 <p>Translating word-by-word English into Spanish is <a href="http://arxiv.org/pdf/1309.4168.pdf">equivalent to matrix rotations</a>. This means that all of the basic linear algebra operators (addition, subtraction, dot products, and matrix rotations) have meaningful functions on human language.</p>
 </li>
 <li>
 <p>Word vectors can also be used to find the <a href="https://github.com/dhammack/Word2VecExample">odd word out</a>.</p>
 </li>
 <li>
 <p>Interestingly, the same skip-gram algorithm can be <a href="https://sites.google.com/site/bryanperozzi/projects/deepwalk">applied to a social graph</a> instead of sentence structure. The authors equate a sequence of social network graph visits (a random walk) to a sequence of words (a sentence in word2vec) to generate a dense summary vector.</p>
 </li>
 <li>
 <p>A brief but very visual overview of distributed representations is available <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">here</a>.</p>
 </li>
 <li>
 <p>Intriguingly, the word2vec algorithm can be reinterpreted as a <a href="https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf">matrix factorization method using point-wise mutual information</a>. This theoretical breakthrough cleanly connects older and faster but more memory-intensive techniques with word2vec&#x2019;s streaming algorithm approach.</p>
 </li>
</ol>



<p><span>
<a name="footnote1"></a>
<sup>1</sup>
See also the original papers, and the subsequently bombastic <a href="http://www.wired.com/2014/12/googlers-quest-teach-machines-understand-emotions/">media frenzy</a>, the race to understand why word2vec works <a href="https://levyomer.wordpress.com/2014/09/10/neural-word-embeddings-as-implicit-matrix-factorization/">so well</a>, some academic drama on <a href="https://docs.google.com/a/stitchfix.com/document/d/1ydIujJ7ETSZ688RGfU5IMJJsbxAi-kRl8czSwpti15s/edit#heading=h.66rkmh7nd17u">GloVe vs word2vec</a>, and a <a href="https://www.youtube.com/watch?v=wTp3P2UnTfQ">nice introduction</a> to the algorithms behind word2vec from my friend <a href="http://radimrehurek.com/">Radim &#x158;eh&#x16F;&#x159;ek</a>. <a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote1-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote2"></a>
<sup>2</sup>
Although see <a href="https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/">Omer Levy and Yoav Goldberg&#x2019;s post</a> for an interesting approach that has the word2vec context defined by parsing the sentence structure. Doing this introduces a more functional similarity between words (see this <a href="http://irsrv2.cs.biu.ac.il:9998/?word=hogwarts">demo</a>). For example, <em>Hogwarts</em> in word2vec is similar to <em>dementors</em> and <em>dumbledore</em>, as they&#x2019;re all from Harry Potter, while parsing context gives <em>sunnydale</em> and <em>colinwood</em> as they&#x2019;re similarly prestigious schools.
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote2-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote3"></a>
<sup>3</sup>
This is describing the &#x2018;skip-gram&#x2019; mode of word2vec where the target word is asked to predict the surrounding context. Interestingly, we can also get similar results by doing the reverse: using the surrounding text to predict a word in the middle! This model, called continuous bag-of-words (CBOW), loses word order and so we lose a bit of grammatical information since that&#x2019;s very sensitive to the position of a word in a sentence. This means CBOW-trained word vectors tend to do worse in a <em>syntactic</em> sense: the resulting vectors more poorly encode whether a word is an adjective or a verb, or a noun.
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote3-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote4"></a>
<sup>4</sup>
More generally, a linear combination of axes encodes the properties. We can attempt to rotate into the correct basis by using PCA (as long as we only include a few nearby words) or visualize that space using t-SNE (although we lose the concept of a single axis encoding structure).
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote4-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote5"></a>
<sup>5</sup>
Compare word vectors to sentiment analysis, which effectively distills everything into one dimension of &#x2018;happy or sad&#x2019;, or document labeling efforts like Latent Dirichlet Allocations that sort words into a few types. In either case, we can only ask these simpler models to categorize new documents into a few predetermined groups. With word vectors we can encapsulate far more diversity without having to build a labeled training text (and thus with less effort.)
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote5-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote6"></a>
<sup>6</sup>
You can download this file freely from <a href="https://code.google.com/p/word2vec/#Pre-trained_word_and_phrase_vectors">here</a>.
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote6-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote7"></a>
<sup>7</sup>
This is using an advanced visualization technique called <a href="http://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne">t-SNE</a>. This allows us to project down to 2D while still trying to maintain the local structure. This helps pop up the several word clusters that are near to the word <code class="highlighter-rouge">vacation</code>.
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote7-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote8"></a>
<sup>8</sup>
Check out this live demo with just wikipedia words <a href="http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/#wikisim">here</a>.
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote8-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote9"></a>
<sup>9</sup>
We&#x2019;ve used cosine similarity to find the nearest items, but, we could&#x2019;ve chosen the 3COSMUL method. This combines vectors multiplicatively instead of additively and seems to <a href="https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf">get better results</a> (pdf warning!). This stays truer to cosine distance and in general prevents one word from dominating in any one dimension.
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote9-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote10"></a>
<sup>10</sup>
You can easily make a vector for a whole sentence by following the <a href="http://radimrehurek.com/2014/12/doc2vec-tutorial/">Doc2Vec</a> tutorial (also called <a href="http://cs.stanford.edu/~quocle/paragraph_vector.pdf">paragraph vector</a>) in <a href="https://radimrehurek.com/gensim/">gensim</a>, or by clustering words using the <a href="http://eng.kifi.com/from-word2vec-to-doc2vec-an-approach-driven-by-chinese-restaurant-process/">Chinese Restaurant Process</a>.
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote10-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote11"></a>
<sup>11</sup>
If you&#x2019;re using Python 2, this is a great reason to reduce Unicode headaches and switch to Python 3.
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote11-return">&#x2190;</a>
</span></p>

<p><span>
<a name="footnote12"></a>
<sup>12</sup>
See a comparison of these techniques <a href="http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/">here</a>. My recommendation is using LSH if you need a pure Python solution, and annoy if you need a solution that is memory light.
<a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote12-return">&#x2190;</a>
</span></p>

 </section>
 </div>
</body></html>
