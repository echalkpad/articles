<!DOCTYPE html><html><head><title>10 Misconceptions about Neural Networks</title></head><body>
<h1>10 Misconceptions about Neural Networks</h1><p><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/" target="_new">Original URL</a></p>
<p><blockquote>StuartReid | On May 8, 2014 Neural networks are one of the most popular and powerful classes of machine learning algorithms. In quantitative finance neural networks are often used for time-series&hellip;</blockquote></p>
<div><div class="blogcontent">

<meta value="2014-05-08">
<p class="name vcard author"><span class="fn"><a href="http://www.turingfinance.com/author/StuartReid/" title="Posts by StuartReid" rel="author">StuartReid</a></span> | On <span class="updated">May 8, 2014</span></p>
 
<p><span><a id="top"></a>Neural networks are one of the most popular and powerful classes of machine learning algorithms. In quantitative finance neural networks are often used for time-series forecasting, constructing proprietary indicators, algorithmic trading, securities classification and credit risk modelling. They have also been used to construct&#xA0;stochastic process models and price derivatives. Despite their usefulness neural networks tend to have a bad reputation because their performance is "temperamental". In my opinion this can be attributed to poor network design owing to misconceptions regarding how neural networks work. This article discusses some of those misconceptions.</span></p>


<h3><a id="brain"></a>1. Neural networks are not models of the human&#xA0;brain</h3>
<p><span>The human brain is one of the great&#xA0;mysteries of our time&#xA0;and&#xA0;scientists have not reached a consensus on&#xA0;exactly how it works. Two theories of the brain exist namely&#xA0;the <a href="http://en.wikipedia.org/wiki/Grandmother_cell" target="_blank">grandmother cell theory</a> and the distributed representation theory. The first theory asserts that individual neurons have high information capacity and are capable of representing complex concepts such as your grandmother or even&#xA0;<a href="http://www.npr.org/blogs/krulwich/2012/03/30/149685880/neuroscientists-battle-furiously-over-jennifer-aniston" target="_blank">Jennifer Aniston</a>. The second theory neurons asserts that neurons are much&#xA0;more simple and representations of complex objects are distributed across many neurons. Artificial neural networks are loosely inspired by the second theory. </span></p>
<p><span>One reason why I believe current generation neural networks are not capable of sentience (a different concept to intelligence) is because I believe that biological neurons are much more complex than artificial neurons.</span></p>
<blockquote>
<p><span><a href="https://medium.com/backchannel/google-brains-co-inventor-tells-why-hes-building-chinese-neural-networks-662d03a8b548" target="_blank">A single neuron in the brain is an incredibly complex machine that even today we don&#x2019;t understand. A single &#x201C;neuron&#x201D; in a neural network is an incredibly simple mathematical function that captures a minuscule fraction of the complexity of a biological neuron. So to say neural networks mimic the brain, that is true at the level of loose inspiration, but really artificial neural networks are nothing like what the biological brain does. - Andrew Ng</a></span></p>
</blockquote>
<p><span>Another big difference between the brain and neural networks is size and organization. Human brains contain many more neurons and synapses than neural network and they are self-organizing and adaptive. Neural networks, by comparison, are organized according to an architecture. Neural networks are not "self-organizing" in the same sense as the brain which much more closely resemble a graph than an ordered network.</span></p>
<div id="attachment_2214" class="wp-caption alignnone"><img class="wp-image-2214 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Brain-connections-300x132.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Brain-connections-358x158.png%20358w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Brain-connections-700x309.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Brain-connections.png%20934w" alt="Brain connections" width="934"><p class="wp-caption-text">Some very interesting views of the brain as created by state of the art brain imagine techniques. Click on the image for more information.</p></div>
<p><span>So what does that mean? Think of it this way: a neural network is inspired&#xA0;by the brain in the same way that the Olympic stadium in Beijing is inspired by a bird's nest. That does not mean that the Olympic stadium is-a bird's nest, it means that some elements of birds nests are present in the design of the stadium. In other words, elements of the brain are present in the design of neural networks but they are a lot less similar than you&#xA0;might think.</span></p>
<p><span>In fact neural networks are more closely related to statistical methods such as&#xA0;<a href="http://en.wikipedia.org/wiki/Curve_fitting" target="_blank">curve fitting</a>&#xA0;and&#xA0;<a href="http://en.wikipedia.org/wiki/Regression_analysis" target="_blank">regression analysis</a>&#xA0;than the human brain. In the context of quantitative finance I think it is important to remember that because whilst it may sound cool&#xA0;to say that something is 'inspired by the brain', this statement may&#xA0;result unrealistic expectations&#xA0;or fear. For more info&#xA0;see <em><a href="https://www.linkedin.com/pulse/artificial-intelligence-existential-threat-stuart-gordon-reid" target="_blank">'No! Artificial Intelligence is not an existential threat'</a>.</em></span></p>
<div id="attachment_2864" class="wp-caption aligncenter"><img class="wp-image-2864 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/07/Non-Linear-Regression.gif.pagespeed.ce.sH7uWKa_0d.gif" alt="Non-Linear Regression" width="610"><p class="wp-caption-text">An example of curve fitting also known as function approximation. Neural networks are quite often used to approximate complex mathematical functions.</p></div>
<p><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></p>

<h3><a id="stats"></a>2. Neural networks aren't a "weak form" of statistics</h3>
<p><span>Neural networks consist of layers of interconnected nodes. Individual nodes are called perceptrons and resemble a <a href="https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_regression" target="_blank">multiple linear regression</a>. The difference between a multiple linear regression and a perceptron is that a perceptron feeds the signal generated by a multiple linear regression into an&#xA0;activation function which may or may not be non-linear. In a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" target="_blank">multi layered perceptron</a> (MLP) perceptrons are arranged into layers and layers are connected with other another. In the MLP there are three types of layers namely, the input layer, hidden layer(s), and the output layer. The input layer receives input patterns&#xA0;and the output layer could contain a list of&#xA0;classifications or output signals&#xA0;to which&#xA0;those input patterns may map. Hidden layers adjust the weightings on those inputs until the error of the neural network is minimized. One interpretation of this is that the hidden layers extract salient features in the input data which have predictive power with respect to the outputs.</span></p>

<h5><span>Mapping Inputs : Outputs</span></h5>
<p><span>A&#xA0;perceptron receives a vector of inputs, <span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{z} = (z_1,z_2,\ldots,z_n)"></span>, consisting on <span class="MathJax_Preview"><img src="" class="tex" alt="n"></span> attributes. This vector of inputs is called an input pattern. These inputs are weighted according to the weight vector belonging to that&#xA0;perceptron, <span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{v} = (v_1,v_2,\ldots,v_n)"></span>. In the context of multiple linear regression these can be thought of as regression co-efficients or beta's. The net input signal, <span class="MathJax_Preview"><img src="" class="tex" alt="net"></span>, of the perceptron is usually the sum product of the input pattern and their weights. Neurons which use the sum-product for <span class="MathJax_Preview"><img src="" class="tex" alt="net"></span> are called summation units.</span></p>
<div class="one_half">
<p><span><span class="MathJax_Preview"><img src="" class="tex" alt="net = \sum^n_{i-1} z_i v_i"></span></span></p>
<p><span>The net input signal, minus a bias <span class="MathJax_Preview"><img src="" class="tex" alt="\theta"></span> is then fed into some&#xA0;activation function, <span class="MathJax_Preview"><img src="" class="tex" alt="f()"></span>. Activation functions are usually monotonically increasing functions which are bounded between either <span class="MathJax_Preview"><img src="" class="tex" alt="(0,1)"></span> or <span class="MathJax_Preview"><img src="" class="tex" alt="(-1,1)"></span> (this&#xA0;is discussed further on in this article). Activation functions can be linear or non-linear.</span></p>
</div>
<div class="one_half column-last">
<p><img class="aligncenter size-full wp-image-5860" src="http://www.turingfinance.com/wp-content/uploads/2014/05/Perceptron-300x135.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/05/Perceptron.png%20624w" alt="Perceptron" width="624"></p>
</div><span>Some popular activation functions used in neural networks are shown below,</span>
<p><img class="size-full wp-image-2226" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Activations-Functions-300x203.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Activations-Functions-700x473.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Activations-Functions.png%20930w" alt="Activations Functions" width="930"></p>
<p><span>The simplest neural network is one which has&#xA0;just one neuron which maps&#xA0;inputs to an output. Given a pattern, <span class="MathJax_Preview"><img src="" class="tex" alt="p"></span>, the objective of this network would be to minimize the error of the output signal, <span class="MathJax_Preview"><img src="" class="tex" alt="o_p"></span>, relative to some known target value for some given training pattern, <span class="MathJax_Preview"><img src="" class="tex" alt="t_p"></span>. For example,&#xA0;if the neuron was supposed to map <span class="MathJax_Preview"><img src="" class="tex" alt="p"></span> to -1 but it mapped it to 1 then the error,&#xA0;as measured by sum-squared distance, of the neuron would be 4, <span class="MathJax_Preview"><img src="" class="tex" alt="(-1 - 1)^2"></span>.&#xA0;</span></p>

<h5><span>Layering</span></h5>
<p><img class="size-full wp-image-2165 aligncenter" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Multilayer-perceptron-300x126.jpg%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Multilayer-perceptron.jpg%20601w" alt="Multilayer perceptron" width="601"></p>
<p><span>As shown in the image above perceptrons are organized into layers. The first layer or perceptrons, called the input later, receives the patterns, <span class="MathJax_Preview"><img src="" class="tex" alt="p"></span>, in the training set, <span class="MathJax_Preview"><img src="" class="tex" alt="P_T"></span>. The last layer maps to the expected outputs for those patterns. An example of this is that the&#xA0;patterns may be a list of quantities for different technical indicators regarding a security and the potential outputs may be the categories <span class="MathJax_Preview"><img src="" class="tex" alt="\{BUY, HOLD, SELL\}"></span>.</span></p>
<p><span>A hidden layer is one which receives as inputs the outputs from another&#xA0;layer; and for which the&#xA0;outputs form the inputs into yet another layer. So what do these hidden layers do? One interpretation is that they extract salient features in the input data which have predictive power with respect to the outputs. This is called <a href="https://en.wikipedia.org/?title=Feature_extraction" target="_blank">feature extraction</a>&#xA0;and in a way it performs a similar function&#xA0;to&#xA0;statistical techniques such as&#xA0;<a href="http://www.turingfinance.com/artificial-intelligence-and-statistics-principal-component-analysis-and-self-organizing-maps/" target="_blank">principal component analysis.</a></span></p>
<p><span>Deep neural networks have&#xA0;a large number of hidden layers and are able to extract much deeper features from the data. Recently, deep neural networks have performed particularly well for <a href="https://gigaom.com/2013/08/16/were-on-the-cusp-of-deep-learning-for-the-masses-you-can-thank-google-later/" target="_blank">image recognition problems</a>. An illustration of feature extraction in the context of image recognition&#xA0;is shown below,</span></p>
<p><img class="aligncenter size-full wp-image-5863" src="http://www.turingfinance.com/wp-content/uploads/2014/05/viz-features-300x163.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/05/viz-features.png%20682w" alt="viz-features" width="682"></p>
<p><span>I think&#xA0;that one of the problems facing the use of deep neural networks for trading (in addition to the <a href="http://www.turingfinance.com/perils-optimization-in-investment-management/" target="_blank">obvious risk of overfitting</a>) is that the inputs into the neural network are almost always heavily pre-processed meaning that there may be few features to actually extract because the inputs are already to some extent features.</span></p>

<h5><span>Learning Rules</span></h5>
<p><span>As mentioned previously the objective of the neural network is to minimize some measure of error, <span class="MathJax_Preview"><img src="" class="tex" alt="\epsilon"></span>. The most common measure of error is sum-squared-error although this metric is sensitive to outliers and may be less appropriate than <a href="https://en.wikipedia.org/wiki/Tracking_error" target="_blank">tracking error</a> in the context of financial markets.</span></p>
<p><span>Sum squared error (SSE), <span class="MathJax_Preview"><img src="" class="tex" alt="\epsilon = \sum^{P_T}_{p=1} \big ( t_p - o_p \big )^2"></span></span></p>
<p><span>Given that the objective of the network is to minimize <span class="MathJax_Preview"><img src="" class="tex" alt="\epsilon"></span> we can use an optimization algorithm to adjust the weights in the neural network. The most common learning algorithm for neural networks is the gradient descent algorithm although other and potentially better optimization algorithms can be used.&#xA0;</span><span>Gradient descent works by calculating the partial derivative of the error with respect to the weights for each layer in the neural network and then moving in the opposite direction to the gradient (because we want to <em>minimize</em> the error of the neural network). By minimizing the error we maximize the performance of the neural network <em>in-sample</em>.</span></p>
<p><span>Expressed mathematically the update rule for the weights in the neural network (<span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{v}"></span>) is given by,</span></p>
<p><span><span class="MathJax_Preview"><img src="" class="tex" alt="v_i(t) = v_i(t - 1) + \delta v_i(t)"></span> where</span></p>
<p><span><span class="MathJax_Preview"><img src="" class="tex" alt="\delta v_i(t) = \eta(-\frac{\partial \epsilon}{\partial v_i})"></span> where</span></p>
<p><span><span class="MathJax_Preview"><img src="" class="tex" alt="\frac{\partial \epsilon}{\partial v_i} = -2(t_p - o_p) \frac{\partial f}{\partial net_p}z_{i,p}"></span></span></p>
<p><span>where <span class="MathJax_Preview"><img src="" class="tex" alt="\eta"></span> is the learning rate which controls how quickly or slowly the neural network converges. It is worth nothing that the calculation of the partial derivative of <span class="MathJax_Preview"><img src="" class="tex" alt="f"></span> with respect to the net input signal for a pattern <span class="MathJax_Preview"><img src="" class="tex" alt="p"></span> represents a problem for any discontinuous activation functions; which is one reason why alternative optimization algorithms may be used. The choice of learning rate has a large impact on the performance of the neural network. Small values for <span class="MathJax_Preview"><img src="" class="tex" alt="\eta"></span> may result in very slow convergence whereas high values for <span class="MathJax_Preview"><img src="" class="tex" alt="\eta"></span> could result in a lot of variance in the training.</span></p>
<div class="one_half">
<p><img class="aligncenter size-full wp-image-5884" src="" alt="Small Learning Rate"></p>
</div>
<div class="one_half column-last">
<p><img class="aligncenter size-full wp-image-5885" src="" alt="High Learning Rate"></p>
</div>

<h5><span>Summary</span></h5>
<p><span>Despite what some of the statisticians I have met in my time believe, neural networks are not just a "weak form of statistics for lazy analysts" (<em>I have actually been told this before and it was quite funny</em>); neural networks represent an abstraction of solid statistical techniques which date back hundreds of years. For a fantastic explanation of the statistics behind neural networks I recommend <a href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K9.pdf" target="_blank">reading this chapter</a>. That having been said&#xA0;I do agree&#xA0;that some practitioners like to treat neural networks as a "black box" which can be thrown at any problem without first taking the time to understand the nature of the problem and whether or not neural networks are an appropriate choice. An example of this is the use of neural networks for trading; markets are dynamic yet neural networks assume the distribution of input patterns&#xA0;remains stationary over time. This is discussed in <a href="http://www.turingfinance.com/perils-optimization-in-investment-management/" target="_blank">more detail here</a>.&#xA0;</span></p>
<p><span><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></span></p>

<h3><a id="network"></a>3.&#xA0;Neural networks come in many architectures</h3>
<p><span>Up until now we have just discussed the most simple neural network architecture, namely the multi-layer perceptron. There are many different neural network architectures (far too many to mention here) and the performance of any neural network is a function of its&#xA0;architecture and weights. Many modern day advances in the field of machine learning do&#xA0;not come from rethinking the way that perceptrons and optimization algorithms work but rather from being creative regarding how these components fit together. Below I discuss some very&#xA0;interesting and creative neural network architectures which have been developed over time,&#xA0;</span></p>
<p><span><span><a href="http://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank">Recurrent Neural Networks</a></span>&#xA0;-&#xA0;some or all connections flow backwards&#xA0;meaning that&#xA0;feed back loops exist in the network. These networks are believed to perform better on time series data. As such, they may be particularly&#xA0;relevant in the context of the financial markets. For more information here is a link to a&#xA0;fantastic article entitled, <em><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">The unreasonable performance of recurrent [deep] neural networks</a></em>.</span></p>
<div id="attachment_2192" class="wp-caption aligncenter"><img class="wp-image-2192 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Recurrent-Neural-Network-Architectures-300x111.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Recurrent-Neural-Network-Architectures-700x259.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Recurrent-Neural-Network-Architectures.png%20941w" alt="Recurrent Neural Network Architectures" width="941"><p class="wp-caption-text"><span>This diagram shows three popular recurrent Neural Network Architectures namely the Elman neural network, the Jordan neural network, and the Hopfield single-layer neural network.</span></p></div>
<p><span>A more recent interesting recurrent neural network architecture is the <a href="http://arxiv.org/pdf/1410.5401.pdf" target="_blank">Neural Turing Machine</a>. This network combines a recurrent neural network architecture with memory. It has been shown that these neural networks are Turing complete and were able to learn sorting algorithms and other computing tasks.</span></p>
<p><span><span><a href="http://en.wikipedia.org/wiki/Boltzmann_machine" target="_blank">Boltzmann neural network</a></span> - one of the first fully connected neural networks was the&#xA0;Boltzmann neural network a.k.a Boltzmann machine. These networks were the first networks capable of&#xA0;learning internal representations and solving very difficult combinatoric problems. One interpretation of the Boltzmann machine is that it is a Monte Carlo version of the Hopfield recurrent neural network. Despite this, the neural network can be quite difficult to train but when constrained they can prove more efficient than traditional neural networks. The most popular constraint on Boltzmann machines is to disallow direct connections between hidden neurons. This particular architecture is referred to as a&#xA0;<a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" target="_blank">Restricted Boltzmann Machine</a>, which are used in&#xA0;<a href="http://deeplearning.net/tutorial/rbm.html" target="_blank">Deep Botlzmann Machines</a>.</span></p>
<div id="attachment_2193" class="wp-caption aligncenter"><img class=" wp-image-2193" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Boltzmann-Machine-300x138.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Boltzmann-Machine-1024x471.png%201024w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Boltzmann-Machine-700x322.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Boltzmann-Machine.png%203985w" alt="Boltzmann Machine" width="1435"><p class="wp-caption-text"><span>This diagram shows how different Boltzmann Machines with connections between the different nodes can significantly affect the results of the neural network (graphs to the right of the networks)</span></p></div>
<p><span><a href="http://en.wikipedia.org/wiki/Deep_neural_network#Deep_neural_networks" target="_blank"><span>Deep neural networks</span></a> - there are neural networks with multiple hidden layers. Deep neural networks have become extremely popular in more recent years due to their unparalleled success in image and voice recognition problems.&#xA0;The number of <a href="https://en.wikipedia.org/wiki/Deep_learning#Deep_learning_architectures" target="_blank">deep neural network architectures</a> is growing quite quickly but some of the most popular architectures&#xA0;include <a href="https://en.wikipedia.org/wiki/Deep_belief_network" target="_blank">deep belief networks</a>, <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank">convolutional neural networks</a>, deep restricted Boltzmann machines, stacked auto-encoders, and many more. One of the biggest problems with deep neural networks, especially in the context of financial markets which are non-stationary, is overfitting. More more info see <a href="http://deeplearning.net/reading-list/tutorials/" target="_blank">DeepLearning.net</a>.</span></p>
<div id="attachment_2189" class="wp-caption aligncenter"><img class="wp-image-2189 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Deep-Neural-Network-Cat2-300x147.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Deep-Neural-Network-Cat2-642x316.png%20642w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Deep-Neural-Network-Cat2.png%20670w" alt="Deep Neural Network Cat" width="670"><p class="wp-caption-text"><span>This diagram shows a deep neural network which consists of multiple hidden layers.</span></p></div>
<p><span><a href="http://www.sciencedirect.com/science/article/pii/S0169207004001116" target="_blank"><span>Adaptive neural networks</span></a> - are neural networks which simultaneously adapt and optimize their architectures whilst learning. This is done by either growing the architecture (adding more hidden neurons) or shrinking it (pruning unnecessary hidden neurons). I believe that adaptive neural networks are most appropriate for financial markets because markets are non-stationary. I say this because the features extracted by the neural network may strengthen or weaken over time depending on market dynamics. The implication of this is that&#xA0; any&#xA0;architecture which worked optimally in the past would need to be altered to work optimally today.</span></p>
<div id="attachment_2208" class="wp-caption aligncenter"><img class="wp-image-2208 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Adaptive-architecture-neural-networks-300x136.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Adaptive-architecture-neural-networks-700x316.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Adaptive-architecture-neural-networks.png%20790w" alt="Adaptive architecture neural networks" width="790"><p class="wp-caption-text"><span>This diagram shows two different types of adaptive neural network architectures. The left image is a cascade neural network and the right image is a self-organizing map.</span></p></div>
<p><span><span><a href="http://en.wikipedia.org/wiki/Radial_basis_function_network" target="_blank">Radial basis networks</a></span>&#xA0;- although not a different type of architecture in the sense of perceptrons and connections, radial basis functions make use of radial basis functions as their activation functions, these are real valued functions whose output depends on the distance from a particular point. The most commonly used radial basis functions is the Gaussian distribution.&#xA0;Because radial basis functions can take on much more complex forms, they were originally used for performing function interpolation. As such, a radial basis function neural network can have a much higher information capacity. Radial basis functions&#xA0;are also used in the kernel of a <a href="http://en.wikipedia.org/wiki/Support_vector_machine" target="_blank">Support Vector Machine</a>.</span></p>
<div id="attachment_2246" class="wp-caption aligncenter"><img class="wp-image-2246 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Radial-basis-function-fitting1-300x92.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Radial-basis-function-fitting1-1024x315.png%201024w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Radial-basis-function-fitting1-700x215.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Radial-basis-function-fitting1.png%201125w" alt="Radial basis function fitting" width="1125"><p class="wp-caption-text"><span>This diagram shows how curve fitting can be done using radial basis functions</span></p></div>
<p><span>In summary, many hundreds of&#xA0;neural network architectures exist and the performance of one neural network can be significantly&#xA0;superior to another. As such, quantitative analysts interested in using neural networks&#xA0;should probably test multiple neural network architectures and consider combining their outputs together in an ensemble to maximize their investment performance. I recommend reading my article, <em><a href="http://www.turingfinance.com/perils-optimization-in-investment-management/" target="_blank">All Your Models are Wrong, 7 Sources of Model Risk</a></em>, before using Neural Networks for trading because many of the problems still apply.</span></p>
<p><span><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></span></p>

<h3><a id="size"></a>4.&#xA0;Size matters, but bigger isn't always better</h3>
<p><span>Having selected an architecture one must then decide how large or small the neural network should be. How many inputs are there? How many hidden neurons&#xA0;should be used? How many hidden layers should be used (if we are using a deep neural network)? And how many outputs neurons are required? </span><span>The reasons why these questions are important is because if the neural network is too large (too small) the neural network could potentially overfit (underfit) the data meaning that the network would&#xA0;not generalize well out of sample.</span></p>

<h5><span>How many and which inputs should be used?</span></h5>
<p><span>The number of inputs depends on the problem being solved,&#xA0;the quantity and quality of&#xA0;available data, and perhaps some creativity. Inputs are simply variables which we believe have some predictive power over the dependent variable being predicted. If the inputs to a problem are unclear, you can systematically determine which variables should be included by looking at the correlations and cross-correlation between potential&#xA0;independent variables and the dependent variables. This approach is detailed in the article, <em><a href="http://www.turingfinance.com/what-drives-real-gdp-growth-part-one/" target="_blank">What Drives Real GDP Growth?</a></em></span></p>
<p><span>There are two problems with using correlations to select input variables. Firstly, if you are using a linear correlation metric you may inadvertently exclude&#xA0;useful variables. Secondly, two relatively uncorrelated variables could potentially be combined to produce a strongly correlated variable. If you look at the variables in isolation you may miss this opportunity. To overcome the second problem you could use <a href="http://www.turingfinance.com/artificial-intelligence-and-statistics-principal-component-analysis-and-self-organizing-maps/" target="_blank">principal component analysis</a> to extract useful eigenvectors (linear combinations of the variables) as inputs. That said a&#xA0;problem with this is that the eigenvectors&#xA0;may not generalize well and they also assume the&#xA0;distributions of input patterns is stationary.</span></p>
<p><span>Another problem when selecting variables is multicollinearity. Multicollinearity is when two or more of the independent variables being fed into the model are highly correlated. In the context of regression models this may cause regression co-efficients to change erratically&#xA0;in response to small changes in the model or the data. Given that neural networks and regression models are similar I suspect this is also a problem for neural networks. </span></p>
<p><span>Last, but not least, one statistical bias which may be introduced&#xA0;when selecting variables is <a href="http://www.turingfinance.com/perils-optimization-in-investment-management/" target="_blank">omitted-variable bias</a>. Omitted variable bias occurs when a model is created which leaves out one or more important causal variables. The bias is created when the model incorrectly compensates for the missing variable by over or underestimating the effect of one of the other variables i.e. the weights may&#xA0;become too large on these variables or SSE will be large.&#xA0;</span></p>

<h5><span>How many hidden neurons should I use?</span></h5>
<p><span>The optimal number of hidden units is <span>problem specific</span>. That said, as a general rule of thumb&#xA0;the more hidden units used the more probable the risk of overfitting becomes. Overfitting is when the neural network does not learn the underlying statistical properties of the data, but rather 'memorizes' the patterns and any noise they may contain. This results in neural networks which perform well in sample but poorly out of sample.&#xA0;So how can we avoid overfitting? There are two popular approaches used in industry namely early stopping and regularization and then there is my personal favourite approach, global search,</span></p>
<div class="one_half">
<p><span><a href="https://en.wikipedia.org/wiki/Early_stopping" target="_blank">Early stopping</a> involves splitting your training set into the main training set and a validation set. Then&#xA0;instead of training a neural network for a fixed number of iterations, you train then until the performance of the neural network on the validation set begins to deteriorate. Essentially this prevents the neural network from using all of the available parameters and limits it's ability to simply memorize every pattern it sees. The image on the right shows two potential stopping points for the neural network (a and b).</span></p>
</div>
<div class="one_half column-last">
<p><img class="aligncenter wp-image-5874" src="http://www.turingfinance.com/wp-content/uploads/2014/05/Early-Stopping-300x247.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/05/Early-Stopping.png%20519w" alt="Early Stopping" width="243"></p></div><span>The image below shows the performance and over-fitting of the neural network when stopped at a or b,</span>
<p><img class="aligncenter size-large wp-image-5875" src="http://www.turingfinance.com/wp-content/uploads/2014/05/Early-Stopping-II-300x116.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/05/Early-Stopping-II-1024x397.png%201024w,%20http://www.turingfinance.com/wp-content/uploads/2014/05/Early-Stopping-II-700x271.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/05/Early-Stopping-II.png%201112w" alt="Early Stopping II" width="620"></p>
<p><span>Regularization penalizes the neural network for using complex architectures. Complexity in this approach is measured by the size of the neural network weights. Regularization&#xA0;is done by adding a term to sum squared error objective function which depends on the size of the weights. This is the equivalent of adding a prior which essentially makes the neural network believe that the function it is approximating is smooth,</span></p>
<p><span><span class="MathJax_Preview"><img src="" class="tex" alt="\epsilon = \beta \sum^{P_T}_{p=1} \big ( t_p - o_p \big )^2 + \alpha \sum^n_{j=1} v_j^2"></span></span></p>
<p><span>where <span class="MathJax_Preview"><img src="" class="tex" alt="n"></span> is the number of weights in the neural network. The parameters <span class="MathJax_Preview"><img src="" class="tex" alt="\alpha"></span> and <span class="MathJax_Preview"><img src="" class="tex" alt="\beta"></span> control the degree to which the neural network over or underfits the data. Good values for <span class="MathJax_Preview"><img src="" class="tex" alt="\alpha"></span> and <span class="MathJax_Preview"><img src="" class="tex" alt="\beta"></span> can be derived using Bayesian analysis and optimization. This, and the above, are explained in considerably more detail in this <a href="http://hagan.okstate.edu/NNDesign.pdf#page=469" target="_blank">brilliant chapter</a>.</span></p>
<p><img class="aligncenter size-large wp-image-5876" src="http://www.turingfinance.com/wp-content/uploads/2014/05/Neural-Network-Regularization-300x234.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/05/Neural-Network-Regularization-1024x800.png%201024w,%20http://www.turingfinance.com/wp-content/uploads/2014/05/Neural-Network-Regularization-700x547.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/05/Neural-Network-Regularization.png%201185w" alt="Neural Network Regularization" width="620"></p>
<p><span>My favourite technique, which is also by far the most computationally expensive, is global search. In this approach a search algorithm is used to try different neural network architectures and arrive at a near optimal choice. This is most often done using genetic algorithms which are discussed further on in this article. </span></p>

<h5><span>What Are the Outputs?</span></h5>
<p><span>Neural networks can be used for either regression or classification. Under regression model a single value is&#xA0;outputted which may be mapped to a set of real numbers meaning that only one output neuron is required. Under&#xA0;classification model an output neuron is required for each potentially class to which the pattern may belong. If the classes are unknown unsupervised neural network techniques such as self organizing maps should be used.</span></p>

<p><span>In conclusion, the best approach is to follow <a href="https://en.wikipedia.org/wiki/Occam's_razor" target="_blank">Ockhams Razor.</a> Ockham's razor argues that for two models of equivalent performance, the model with fewer free parameters will generalize better. On the other hand, one should never opt for an overly simplistic model at the cost of performance. Similarly, one should not assume that just because a neural network has more hidden neurons and maybe more hidden layers it will outperform a much simpler network. Unfortunately it seems to me that too much emphasis is placed on large networks and too little emphasis is placed on making good design decisions. In the case of neural networks, bigger isn't always better.&#xA0;</span></p>
<p><div><div class="one_half">

<blockquote>
<p><span>Entities must not be multiplied beyond necessity -&#xA0;<a href="https://en.wikipedia.org/wiki/William_of_Ockham" target="_blank">William of Ockham</a></span></p>
</blockquote>
</div></div></p>
<p><div><div class="one_half column-last">

<blockquote>
<p><span>Entities must not be reduced to the point of inadequacy - <a href="https://en.wikipedia.org/wiki/Karl_Menger" target="_blank">Karl Menger</a></span></p>
</blockquote>
</div></div></p>
<p><span><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></span></p>

<h3><a id="algo"></a>5.&#xA0;Many training algorithms exist for neural networks</h3>
<p><span>The learning algorithm of a neural network tries to optimize the neural network's weights until some stopping condition has been met. This condition is typically either&#xA0;when the error of the network reaches an&#xA0;acceptable level of accuracy on the training set, when the error of the network on the validation set begins to deteriorate, or when&#xA0;the specified&#xA0;computational budget has been exhausted.&#xA0;The most common learning algorithm for neural networks is the&#xA0;<a href="http://en.wikipedia.org/wiki/Backpropagation" target="_blank">backpropagation</a>&#xA0;algorithm which uses <a href="http://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank">stochastic gradient descent</a>&#xA0;which was discussed earlier on in this article. Backpropagation&#xA0;consists of two steps:</span></p>
<ol>
<li><span><span>The feedforward pass</span> -&#xA0;&#xA0;the training data set is passed through the network and the output from the neural network is recorded and the error of the network is calculated</span></li>
<li><span><span>Backward propagation</span> - the error signal is passed back through the network and the weights of the neural network are optimized using gradient descent.</span></li>
</ol>
<p><span>The&#xA0;are some problems with this approach. Adjusting all the weights at once can result in a <a href="http://changelog.ca/quote/2011/11/21/why_is_back-propagation_learning_so_slow" target="_blank">significant movement</a> of the neural network in weight space, the gradient descent algorithm is quite slow, and is&#xA0;susceptible to local minima. Local minima are a problem for specific types of neural networks including all product link neural networks. The first two problems can be addressed by using <a href="https://en.wikipedia.org/?title=Gradient_descent#Extensions" target="_blank">variants of gradient descent</a> including momentum gradient descent (QuickProp), Nesterov's Accelerated Momentum (NAG) gradient descent, the <a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf" target="_blank">Adaptive Gradient Algorithm</a>&#xA0;(AdaGrad), Resilient Propagation (RProp), and <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank">Root Mean Squared Propagation</a> (RMSProp). As can be seen from the image below significant improvements can be made on the classical gradient descent algorithm.&#xA0;</span></p>
<p><a href="http://imgur.com/a/Hqolp" target="_blank"><img class="aligncenter wp-image-5864 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/05/Long-Valley-Training-Algorithms.gif.pagespeed.ce.SjtKOauOXF.gif" alt="Long Valley Training Algorithms" width="620"></a></p>
<p><span>That having been said, these algorithms cannot overcome local minima and are also less useful when trying to optimize both the architecture and weights of the neural network concurrently. In order to achieve this&#xA0;global optimization algorithms are needed. Two popular global optimization algorithms are the Particle Swarm Optimization (PSO) and the Genetic Algorithm (GA). Here is how they can be used to train neural networks:</span></p>
<p><span><span>Neural network vector representation</span> - by encoding the neural network as a vector of weights, each representing the weight of a connection in the neural network, we can train neural networks using&#xA0;most meta-heuristic search algorithms. This technique does not work well with deep neural networks because the vectors become too large.</span></p>
<div id="attachment_2294" class="wp-caption aligncenter"><img class="wp-image-2294 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Vector-Representation-Neural-Network1-300x125.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Vector-Representation-Neural-Network1-1024x428.png%201024w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Vector-Representation-Neural-Network1-700x292.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Vector-Representation-Neural-Network1.png%201303w" alt="Vector Representation Neural Network" width="1303"><p class="wp-caption-text"><span>This diagram illustrates how a neural network can be represented in a vector notation and related to the concept of a search space or fitness landscape.</span></p></div>
<p><span><a href="http://en.wikipedia.org/wiki/Particle_swarm_optimization" target="_blank"><span>Particle Swarm Optimization</span></a> - to train a neural network using a PSO we construct a population / swarm of those neural networks. Each neural network is represented as a vector of weights and is adjusted according to it's position from the global best particle and it's personal best.</span></p>
<p><span>The fitness function is calculated as the sum-squared error of the reconstructed neural network after completing one feedforward pass of the training data set. The main consideration with this approach is the velocity of the weight updates. This is because if the weights are adjusted too quickly, the sum-squared error of the neural networks will stagnate and no learning will occur.</span></p>
<div id="attachment_2316" class="wp-caption aligncenter"><img class="wp-image-2316 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Particle-Swarm-Optimization-Single-Swarm-300x119.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Particle-Swarm-Optimization-Single-Swarm-1024x405.png%201024w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Particle-Swarm-Optimization-Single-Swarm-700x277.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Particle-Swarm-Optimization-Single-Swarm.png%201464w" alt="Particle Swarm Optimization Single Swarm" width="1464"><p class="wp-caption-text"><span>This diagram shows how particles are attracted to one another in a single swarm Particle Swarm Optimization algorithm.</span></p></div>
<p><span><a href="http://en.wikipedia.org/wiki/Genetic_algorithm" target="_blank"><span>Genetic Algorithm</span></a> - to train a neural network using a genetic algorithm we first construct a population of vector represented neural networks. Then we apply the three genetic operators on that population to evolve better and better neural networks. These three operators are,</span></p>
<ol>
<li><span><a href="http://en.wikipedia.org/wiki/Selection_(genetic_algorithm)" target="_blank">Selection</a> - Using the sum-squared error of each network calculated after one feedforward pass, we rank the population of neural networks. The top x% of the population are selected to 'survive' to the next generation and be used for crossover.</span></li>
<li><span><a href="http://en.wikipedia.org/wiki/Crossover_(genetic_algorithm)" target="_blank">Crossover</a>&#xA0;-&#xA0;The top x% of the population's genes are allowed to cross over with one another. This process forms 'offspring'. In context, each offspring will represent a new neural network with weights from both of the 'parent' neural networks.</span></li>
<li><span><a href="http://en.wikipedia.org/wiki/Mutation_(genetic_algorithm)" target="_blank">Mutation</a> - this operator is required to maintain genetic diversity in the population. A small percentage of the population are selected to undergo mutation. Some of the weights in these neural networks will be adjusted randomly within a particular range.</span></li>
</ol>
<div id="attachment_2320" class="wp-caption aligncenter"><img class="wp-image-2320 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Genetic-Algorithm1-300x160.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Genetic-Algorithm1-700x373.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Genetic-Algorithm1.png%20961w" alt="Genetic Algorithm" width="961"><p class="wp-caption-text"><span>This algorithm shows the selection, crossover, and mutation genetic operators being applied to a population of neural networks represented as vectors.</span></p></div>
<p><span>In addition to these population-based metaheuristic search algorithms, other algorithms have been used to train of neural networks including backpropagation with added momentum, <a href="http://en.wikipedia.org/wiki/Differential_evolution" target="_blank">differential evolution</a>, <a href="http://en.wikipedia.org/wiki/Levenberg_Marquardt" target="_blank">Levenberg Marquardt</a>, <a href="http://en.wikipedia.org/wiki/Simulated_annealing" target="_blank">simulated annealing</a>, and many&#xA0;<a href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K8.pdf" target="_blank">more</a>. Personally I would recommend using a combination of local and global optimization algorithms to overcome the shortcomings of both.</span></p>
<p><span><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></span></p>

<h3><a id="data"></a>6.&#xA0;Neural networks do not always require a lot of data</h3>
<p><span>Neural networks can use one of three learning strategies namely a supervised learning strategy, an unsupervised learning strategy, or a reinforcement learning strategy.&#xA0;</span><span><a href="http://en.wikipedia.org/wiki/Supervised_learning" target="_blank">Supervised learning</a>&#xA0;require at least two data sets, a training set which consists of inputs with the expected output, and a testing&#xA0;set which consists of inputs without the expected output. Both of these data sets must consist of labelled data i.e. data patterns for which the target is known upfront.&#xA0;</span><span><a href="http://en.wikipedia.org/wiki/Unsupervised_learning" target="_blank">Unsupervised learning</a>&#xA0;strategies are typically used to discover hidden structures (such as hidden Markov chains) in unlabeled data. They behave in a&#xA0;similar way to&#xA0;clustering algorithms. <a href="http://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank">Reinforcement learning</a>&#xA0;are based on the simple premise of rewarding neural networks for good behaviours and punishing them for bad behaviours.&#xA0;</span><span>Because unsupervised and reinforcement learning strategies do not require that&#xA0;data be labelled&#xA0;they can be applied to under-formulated problems where the correct output is not known. &#xA0;</span></p>

<h5><span>Unsupervised&#xA0;Learning</span></h5>
<p><span>One of the most popular unsupervised neural network architectures is the Self Organizing Map (also known as the Kohonen Map). Self Organizing Maps are&#xA0;essentially a multi-dimensional scaling technique which construct an approximation of&#xA0;the <a href="http://en.wikipedia.org/wiki/Probability_density_function" target="_blank">probability density function</a> of some underlying data set, <span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{Z}"></span>, whilst&#xA0;preserving the topological structure of that data set.&#xA0;</span><span>This is done by&#xA0;mapping input vectors, <span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{z}_i"></span>, in the data set, <span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{Z}"></span>, to weight vectors, <span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{v}_j"></span>, (neurons) in the feature map, <span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{V}"></span>. Preserving the topological structure simply means that if two&#xA0;input vectors are close together in <span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{Z}"></span>, then the neurons to which those input vectors&#xA0;map in <span class="MathJax_Preview"><img src="" class="tex" alt="\textbf{V}"></span> will also be close together.&#xA0;</span></p>
<p><img class="aligncenter wp-image-3900 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/10/410x290xSelf-Organizing-Feature-Map.gif.pagespeed.ic.dbQPw_XimP.png" alt="Dimensionality Reduction using Principal Component Analysis and Self Organizing Maps" width="410"></p>
<p><span>For more information on self organizing maps and how they can be used to produce lower-dimensionality data sets <a href="http://www.turingfinance.com/artificial-intelligence-and-statistics-principal-component-analysis-and-self-organizing-maps/" target="_blank">click here</a>. Another interesting&#xA0;application of SOM's is in colouring time series charts for stock trading. This is done to show what the market conditions are at that point in time. This <a href="http://cortex.snowcron.com/forex_som.htm#visualization" target="_blank">website</a> provides a detailed tutorial and code snippets for implementing the idea for improved Forex trading strategies.</span></p>

<h5><span>Reinforcement Learning</span></h5>
<p><span>Reinforcement learning strategies consist of three components.&#xA0;A policy which specifies how the neural network will make decisions e.g. using technical and fundamental indicators.&#xA0;A reward function which distinguishes good from bad e.g. making vs. losing money. And a value function which specifies the long term goal. In the context of financial markets (and game playing) reinforcement learning strategies are particularly useful because the neural network learns to optimize a particular quantity such as an appropriate <a href="http://www.turingfinance.com/computational-investing-with-python-week-one/" target="_blank">measure of risk adjusted return</a>.&#xA0;</span></p>
<div id="attachment_2379" class="wp-caption aligncenter"><img class="wp-image-2379 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Reinforcement-Learning-300x154.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Reinforcement-Learning-420x215.png%20420w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Reinforcement-Learning-700x359.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Reinforcement-Learning.png%20701w" alt="Reinforcement Learning" width="701"><p class="wp-caption-text"><span>This diagram shows how a neural network can be either negatively or positively reinforced.</span></p></div>
<p><span><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></span></p>

<h3><a id="prep"></a>7.&#xA0;Neural networks cannot be trained on any data</h3>
<p><span>One of the biggest reasons why neural networks may not work is because&#xA0;people do not properly pre-process the data being fed into the neural network. Data normalization, removal of redundant information, and outlier removal should all be performed to&#xA0;improve the probability of good neural network performance.</span></p>
<p><span><span><a href="http://en.wikipedia.org/wiki/Normalization_(statistics)" target="_blank">Data normalization</a></span> - neural networks consist of various layers of perceptrons linked together by&#xA0;weighted connections. Each perceptron contains an activation function which each have an '<a href="http://www.jatit.org/volumes/Vol47No3/61Vol47No3.pdf" target="_blank">active range</a>' (except for radial basis functions). Inputs into the neural network need to&#xA0;be scaled within this range so that the neural network is able to differentiate between different input patterns.</span></p>
<p><span>For example, given&#xA0;a neural network trading system which receives indicators&#xA0;about a set&#xA0;of&#xA0;securities&#xA0;as inputs&#xA0;and outputs whether&#xA0;each&#xA0;security&#xA0;should be bought or sold. One of the inputs is the price of the security and we are using the Sigmoid activation function.&#xA0;</span><span>However, most of the securities cost between 5$ and 15$ per share&#xA0;and&#xA0;the output of the Sigmoid function approaches 1.0. So&#xA0;the output of the Sigmoid function will be be 1.0 for all securities, all of the perceptrons will 'fire' and the neural network will not learn.</span></p>
<blockquote>
<p><span>Neural networks trained on unprocessed&#xA0;data produce models where 'the lights are on but nobody's home'</span></p>
</blockquote>
<p><span><span><a href="http://en.wikipedia.org/wiki/Outlier#Exclusion" target="_blank">Outlier removal</a></span> - an outlier is&#xA0;value that is much smaller or larger than most of the other values in some set of data. Outliers can cause problems with statistical techniques like&#xA0;regression analysis and curve fitting because when the model tries to 'accommodate' the outlier, performance of the model across all other data&#xA0;deteriorates,</span></p>
<div id="attachment_2403" class="wp-caption aligncenter"><img class="wp-image-2403 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Outlier-Removal-300x147.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Outlier-Removal.png%20513w" alt="Outlier Removal" width="513"><p class="wp-caption-text"><span>This diagram shows the effect of removing an outlier from the training data for a linear regression. The results are comparable for neural networks. Image source: https://statistics.laerd.com/statistical-guides/img/pearson-6.png</span></p></div>
<p><span>The illustration shows that trying to accommodate an outlier into the linear regression model results in a poor fits of the data set. The effect of outliers on non-linear regression models, including neural networks, &#xA0;is similar. Therefore it is good practice is to remove outliers from the training data set. That said, identifying outliers&#xA0;is a challenge in and of itself, this <a href="https://www.siam.org/meetings/sdm10/tutorial3.pdf" target="_blank">tutorial</a>&#xA0;and <a href="http://www.eng.tau.ac.il/~bengal/outlier.pdf" target="_blank">paper</a>&#xA0;discuss existing&#xA0;techniques for outlier detection and removal.</span></p>
<p><span><span><a href="http://en.wikipedia.org/wiki/Redundancy_(information_theory)" target="_blank">Remove redundancy</a></span> - when two or more of the independent variables being fed into the neural network are highly correlated (multiplecolinearity) this can negatively affect the neural networks learning ability.&#xA0;Highly correlated inputs also</span><span>&#xA0;mean that the amount of unique&#xA0;information presented by each variable&#xA0;is&#xA0;small, so&#xA0;the less significant input can be removed. Another benefit to removing redundant variables is faster training times.&#xA0;</span><span>Adaptive neural networks can be used to&#xA0;prune redundant connections and perceptrons.</span></p>
<p><span><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></span></p>

<h3><a id="break"></a>8.&#xA0;Neural networks may need to be retrained</h3>
<p><span>Given that you were able to train a neural network to trade successfully in and out of sample this neural network may still&#xA0;stop working over time. This is not a poor reflection&#xA0;on neural networks but rather an accurate&#xA0;reflection of the financial markets. Financial markets are complex adaptive systems meaning that they&#xA0;are&#xA0;constantly changing so what worked yesterday may not work tomorrow. This characteristic is called non-stationary or dynamic optimization problems and neural networks are not particularly good at handling them.</span></p>
<p><span>Dynamic environments, such as financial markets, are&#xA0;extremely difficult for&#xA0;neural networks to model. Two&#xA0;approaches are either to keep&#xA0;retraining the neural network&#xA0;over-time, or to use a dynamic neural network. Dynamic neural networks 'track' changes to the environment over time and adjust&#xA0;their architecture and weights&#xA0;accordingly. They are adaptive over time.&#xA0;</span><span>For dynamic problems, multi-solution&#xA0;meta-heuristic optimization algorithms can be used to track changes to local optima over time. One such algorithm is the <a href="http://en.wikipedia.org/wiki/Multi-swarm_optimization" target="_blank">multi-swarm optimization</a> algorithm, a derivative of the particle swarm optimization. Additionally, genetic algorithms with enhanced diversity or memory have also been shown to be robust in dynamic environments.</span></p>
<p><span>The illustration below demonstrates how a genetic algorithm evolves over time to find new optima in a dynamic environment. This illustration also happens to&#xA0;mimic&#xA0;trade&#xA0;crowding&#xA0;which is&#xA0;when market participants&#xA0;crowd a profitable&#xA0;trading strategy, thereby&#xA0;exhausting trading opportunities causing&#xA0;the&#xA0;trade to become less profitable.</span></p>
<div id="attachment_2418" class="wp-caption aligncenter"><img class="wp-image-2418 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Dynamic-Environment.gif.pagespeed.ce.hNjfn4okrI.gif" alt="Dynamic Environment" width="640"><p class="wp-caption-text"><span>This animated image shows a dynamic fitness landscape (search space) change over time. Image source: http://en.wikipedia.org/wiki/Fitness_landscape</span></p></div>
<p><span><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></span></p>

<h3><a id="blackbox"></a>9.&#xA0;Neural networks are not black boxes</h3>
<p><span>By itself a neural network is a <a href="http://en.wikipedia.org/wiki/Black_box" target="_blank">black-box</a>. This presents&#xA0;problems for people&#xA0;wanting to use them. For example, fund managers&#xA0;wouldn't&#xA0;know how a neural network makes trading&#xA0;decisions,&#xA0;so it is impossible to assess the risks of the&#xA0;trading strategies learned&#xA0;by the neural network.&#xA0;Similarly, banks using neural networks for credit risk modelling would not be able to justify why a customer has&#xA0;a particular credit rating, which is a regulatory requirement.&#xA0;</span><span>That having been said, state of the art <a href="https://www.google.co.za/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0CEcQFjAC&amp;url=http%3A%2F%2Fwww.mini.pw.edu.pl%2F~mandziuk%2F09-02-06.ppt&amp;ei=DSRiU_-oEoev7AaQtoEI&amp;usg=AFQjCNHnNR7aNo_EMWcDya4ENk5JDG3cSA&amp;sig2=vA7JyJ-uYZqxxWaITp-leg&amp;bvm=bv.65636070,d.ZGU" target="_blank">rule-extraction algorithms</a> have been developed to vitrify some neural network architectures. These&#xA0;algorithms extract knowledge&#xA0;from the neural networks as&#xA0;either mathematical expressions, symbolic logic, fuzzy logic, or decision trees.</span></p>
<div id="attachment_2435" class="wp-caption aligncenter"><img class="wp-image-2435 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Black-box1-300x210.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Black-box1.png%20624w" alt="Black box" width="624"><p class="wp-caption-text"><span>This image shows a neural network as a black box and how it related to rule extraction techniques.</span></p></div>
<p><span><a href="http://www.sciencedirect.com/science/article/pii/S0893608002000898" target="_blank"><span>Mathematical rules</span></a> - algorithms have been developed which can extract multiple linear regression lines from neural networks. The problem with these techniques is that the rules are often still difficult to understand, therefore these do not solve the 'black-box' problem.</span></p>
<p><span><a href="http://en.wikipedia.org/wiki/Propositional_logic" target="_blank"><span>Propositional logic</span></a> - propositional&#xA0;logic is a branch&#xA0;of mathematical logic which&#xA0;deals with operations done on discrete valued variables. These variables, such as A or B, are often&#xA0;either TRUE or FALSE, but they could occupy values within a discrete range e.g. {BUY,HOLD,SELL}.</span></p>
<p><span>Logical operations can then be applied to those variables such as&#xA0;OR, AND, and XOR. The results are called predicates which&#xA0;can also&#xA0;be quantified over sets using the exists or for-all quantifiers. This is the difference between predicate and propositional logic.&#xA0;</span><span>If we had a simple neural network which Price (P),&#xA0;Simple Moving Average (SMA), and Exponential Moving Average (EMA) as inputs and we extracted a&#xA0;trend following strategy from&#xA0;the neural network in propositional logic, we might get rules like this,</span></p>
<p><span><img class="aligncenter size-full wp-image-2446" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Propositional-Logic-Example-300x38.gif%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Propositional-Logic-Example.gif%20398w" alt="Propositional Logic Example"></span></p>
<p><span><span><a href="http://en.wikipedia.org/wiki/Fuzzy_logic" target="_blank">Fuzzy logic</a></span> - fuzzy logic is where probability and propositional logic meet. The problem with propositional logic is that is deals in absolutes e.g.&#xA0;BUY or SELL, TRUE or FALSE, 0 or 1. Therefore for traders there is no way to determine the&#xA0;confidence of these results.&#xA0;</span><span>Fuzzy logic overcomes this limitation by introducing a membership function which specifies how much a&#xA0;variable belongs to a particular domain. For example, a company (GOOG) might belong 0.7 to the domain {BUY} and 0.3 to the domain {SELL}.&#xA0;</span><span>Combinations of neural networks and fuzzy logic are called <a href="http://en.wikipedia.org/wiki/Neuro-fuzzy" target="_blank">Neuro-Fuzzy systems</a>. This <a href="http://www.ncbi.nlm.nih.gov/pubmed/18249802" target="_blank">research survey</a> discusses various fuzzy rule extraction techniques.</span></p>
<p><span><span><a href="http://en.wikipedia.org/wiki/Decision_tree" target="_blank">Decision trees</a></span> - decision trees show how decisions are made&#xA0;when given certain information. This article describes&#xA0;how to evolve <a href="http://www.turingfinance.com/using-gp-to-evolve-security-analysis-decision-trees/" target="_blank">security analysis decision trees using genetic programming</a>.&#xA0;Decision tree induction is the term given to the process of extracting decision trees from neural networks.</span></p>
<div id="attachment_2457" class="wp-caption aligncenter"><img class="wp-image-2457 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Trading-Strategy-Decision-Tree-300x124.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Trading-Strategy-Decision-Tree-700x289.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Trading-Strategy-Decision-Tree.png%20950w" alt="Trading Strategy Decision Tree" width="950"><p class="wp-caption-text"><span>An example of a simple trading strategy represented using a decision tree. The triangular boxes represent decision nodes, these could be to BUY, HOLD, or SELL a company. Each box represents a tuple of &lt;indicator, inequality,="" value=""&gt;. An example might be &lt;sma,&gt;, 25&gt; or &lt;ema, &lt;="," 30=""&gt;.</span></p></div>
<p><span><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></span></p>

<h3><a id="difficult"></a>10.&#xA0;Neural networks are not hard to implement</h3>
<p><span><em>This list is updated, from time to time, when I have time. Last updated: November 2015.</em></span></p>
<p><span>Speaking from experience, neural networks are quite challenging&#xA0;to code from scratch. Luckily there are now hundreds&#xA0;open source and proprietary packages which make working with neural networks a lot easier. </span><span>Below is a list of packages which quants may find useful for quantitative finance. The list is NOT exhaustive, and is ordered alphabetically. If you have any additional comments, or frameworks to add, please share via the comment section.</span></p>

<h5><span>Caffe</span></h5>
<p><span>Webpage -&#xA0;<a href="http://caffe.berkeleyvision.org/" target="_blank">http://caffe.berkeleyvision.org/</a></span></p>
<p><span>GitHub Repository -&#xA0;<a href="https://github.com/BVLC/caffe" target="_blank">https://github.com/BVLC/caffe</a></span></p>
<p><em><span>"Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (<a href="http://bvlc.eecs.berkeley.edu/">BVLC</a>) and by community contributors.<a href="http://daggerfs.com/">Yangqing Jia</a> created the project during his PhD at UC Berkeley." - Caffe webpage (November 2015)</span></em></p>

<h5><span>Encog</span></h5>
<p><span>Webpage -&#xA0;<a href="http://www.heatonresearch.com/encog/" target="_blank">http://www.heatonresearch.com/encog/</a></span></p>
<p><span>GitHub Repositories -&#xA0;<a href="https://github.com/encog" target="_blank">https://github.com/encog</a></span></p>
<p><em><span>"Encog is an advanced machine learning framework that supports a variety of advanced algorithms, as well as support classes to normalize and process data. Machine learning algorithms such as Support Vector Machines, Artificial Neural Networks, Genetic Programming, Bayesian Networks, Hidden Markov Models, Genetic Programming and Genetic Algorithms are supported. Most Encog training algoritms are multi-threaded and scale well to multicore hardware. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train machine learning algorithms." - Encog webpage</span></em></p>

<h5><span>H2O</span></h5>
<p><span>Webpage -&#xA0;<a href="http://h2o.ai/" target="_blank">http://h2o.ai/</a></span></p>
<p><span>GitHub Repositories -&#xA0;<a href="https://github.com/h2oai" target="_blank">https://github.com/h2oai</a></span></p>
<p><span>H2O is not strictly a package for machine learning, instead they expose an API for doing fast and scalable machine learning for&#xA0;smarter applications which use big data. Their API supports deep learning model, generalized boosting models, generalized linear models, and more. They also host a cool conference, <a href="https://www.youtube.com/playlist?list=PLNtMya54qvOFQhSZ4IKKXRbMkyLMn0caa" target="_blank">checkout the videos</a> :).</span></p>

<h5><span>Google TensorFlow</span></h5>
<p><span>Webpage -&#xA0;<a href="http://www.tensorflow.org/" target="_blank">http://www.tensorflow.org/</a></span></p>
<p><span>GitHub repository -&#xA0;<a href="https://github.com/tensorflow/tensorflow" target="_blank">https://github.com/tensorflow/tensorflow</a></span></p>
<p><em><span>"</span><span>TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them. This flexible architecture lets you deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device without rewriting code." - GitHub repository (<em>November 2015)</em></span></em></p>

<h5><span>Microsoft Distributed Machine Learning Tookit</span></h5>
<p><span>Webpage -&#xA0;<a href="http://www.dmtk.io/" target="_blank">http://www.dmtk.io/</a></span></p>
<p><span>GitHub repository -&#xA0;<a href="https://github.com/Microsoft/DMTK" target="_blank">https://github.com/Microsoft/DMTK</a></span></p>
<p><em><span>"DMTK includes the following projects: </span></em><em><span><a href="https://github.com/Microsoft/multiverso">DMTK framework(Multiverso)</a>: The parameter server framework for distributed machine learning. </span></em><em><span><a href="https://github.com/Microsoft/lightlda">LightLDA</a>: Scalable, fast and lightweight system for large-scale topic modeling.&#xA0;</span></em><em><span><a href="https://github.com/Microsoft/distributed_word_embedding">Distributed word embedding</a>: Distributed algorithm for word embedding.&#xA0;</span></em><em><span><a href="https://github.com/Microsoft/distributed_skipgram_mixture">Distributed skipgram mixture</a>: Distributed algorithm for multi-sense word embedding." - GitHub repository (November 2015)</span></em></p>

<h5><span>Microsoft Azure Machine Learning</span></h5>
<p><span>Webpage -&#xA0;<a href="https://azure.microsoft.com/en-us/services/machine-learning" target="_blank">https://azure.microsoft.com/en-us/services/machine-learning</a></span></p>
<p><span>GitHub Repositories - <a href="https://github.com/Azure?utf8=%E2%9C%93&amp;query=MachineLearning" target="_blank">https://github.com/Azure?utf8=%E2%9C%93&amp;query=MachineLearning</a>&#xA0;</span></p>
<p><span>The machine learning / predictive analytics platform in Microsoft Azure is a fully managed cloud service that enables you to easily build, deploy, and share predictive analytics solutions. This software basically allows you to&#xA0;drag and drop pre-built components (including machine learning models) and custom-built&#xA0;components&#xA0;which manipulate data sets into a&#xA0;process.&#xA0;This flow-chart is then compiled into a program and can be deployed as a web-service. It is similar to the older SAS enterprise miner solution except that is it more modern, more functional, supports deep learning models, and exposes clients for Python and R.&#xA0;</span></p>

<h5><span>MXNet</span></h5>
<p><span>Webpage -&#xA0;<a href="http://mxnet.readthedocs.org/en/latest/" target="_blank">http://mxnet.readthedocs.org/en/latest/</a></span></p>
<p><span>GitHub Repositories -&#xA0;<a href="https://github.com/dmlc/mxnet" target="_blank">https://github.com/dmlc/mxnet</a></span></p>
<p><em><span>"MXNet is a deep learning framework designed for both efficiency and flexibility. It allows you to mix the flavours of symbolic programming and imperative programming together to maximize the efficiency and your productivity. In its core, a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly. A graph optimization layer is build on top, which makes symbolic execution fast and memory efficient. The library is portable and lightweight, and is ready scales to multiple GPUs, and multiple machines." - MXNet GitHub Repository (November 2015)</span></em></p>

<h5><span>Neon</span></h5>
<p><span>Webpage -&#xA0;<a href="http://neon.nervanasys.com/docs/latest/index.html" target="_blank">http://neon.nervanasys.com/docs/latest/index.html</a></span></p>
<p><span>GitHub Repository -&#xA0;<a href="https://github.com/nervanasystems/neon" target="_blank">https://github.com/nervanasystems/neon</a></span></p>
<p><em><span>"neon is Nervana's Python based Deep Learning framework and achieves the fastest performance on many common deep neural networks such as AlexNet, VGG and GoogLeNet. We have designed it with the following functionality in mind: 1)&#xA0;Support for commonly used models and examples: convnets, MLPs, RNNs, LSTMs, autoencoders, 2)&#xA0;Tight integration with nervanagpu kernels for fp16 and fp32 (<a href="https://github.com/soumith/convnet-benchmarks">benchmarks</a>) on Maxwell GPUs, 3)&#xA0;Basic automatic differentiation support, 4)&#xA0;Framework for visualization, and 5)&#xA0;Swappable hardware backends&#xA0;..." - neon GitHub repository (November 2015)</span></em></p>

<h5><span>Theano</span></h5>
<p><span>Webpage -&#xA0;<a href="http://deeplearning.net/software/theano/" target="_blank">http://deeplearning.net/software/theano/</a></span></p>
<p><span>GitHub repository -&#xA0;<a href="https://github.com/Theano/Theano" target="_blank">https://github.com/Theano/Theano</a></span></p>
<p><em><span>"Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation." - Theano GitHub repository (November 2015).&#xA0;</span></em><span>Theano, like TensorFlow and Torch, is more broadly applicable than just Neural Networks. It is a framework for implementing existing or creating new machine learning models using off-the-shelf data-structures and algorithms.&#xA0;</span></p>

<h5><span>Torch</span></h5>
<p><span>Webpage -&#xA0;<a href="http://torch.ch/" target="_blank">http://torch.ch/</a></span></p>
<p><span>GitHub Repository -&#xA0;<a href="https://github.com/torch/torch7" target="_blank">https://github.com/torch/torch7</a></span></p>
<p><span><em>"Torch is a scientific computing framework with wide support for machine learning algorithms ... A summary of core features include&#xA0;an N-dimensional array,&#xA0;routines for indexing, slicing, transposing, an&#xA0;interface to C, via LuaJIT,&#xA0;linear algebra routines,&#xA0;neural network, energy-based models,&#xA0;numeric optimization routines,&#xA0;Fast and efficient GPU support,&#xA0;Embeddable, with ports to iOS, Android and FPGA" - Torch Webpage (November 2015).</em> Like Tensorflow and Theano, Torch is more broadly applicable than just Neural Networks.&#xA0;It is a framework for implementing existing or creating new machine learning models using off-the-shelf data-structures and algorithms.&#xA0;</span></p>

<h5><span>SciKit Learn</span></h5>
<p><span>Webpage -&#xA0;<a href="http://scikit-learn.org/stable/" target="_blank">http://scikit-learn.org/stable/</a></span></p>
<p><span>GitHub Repository -&#xA0;<a href="https://github.com/scikit-learn/scikit-learn" target="_blank">https://github.com/scikit-learn/scikit-learn</a></span></p>
<p><span>SciKit Learn is a very popular package for doing machine learning in Python. It is built on NumPy, SciPy, and matplotlib Open source,&#xA0;and exposes implementations of various machine learning models for classification, regression, clustering, dimensionality reduction, model selection, and data preprocessing.</span></p>

<p><span>As I mentioned, there are now hundreds of machine learning packages and frameworks out there. Before committing to any one solution I would recommend doing a best-fit analysis to see which open source or proprietary machine learning package or software best matches your use-cases. Generally speaking a good rule to follow in software engineering and model development for quantitative finance is to not reinvent the wheel ... that said, for any sufficiently advanced model you should expect to have to&#xA0;write some of your own code.</span></p>
<p><span><a href="http://www.turingfinance.com/misconceptions-about-neural-networks/#top">Back to the top</a></span></p>

<h3>Conclusion</h3>
<p><span>Neural networks are a class of powerful machine learning algorithms. They are based on solid statistical foundations and have been applied successfully in financial models as well as in trading strategies for many years. Despite this, they have a bad reputation due to the&#xA0;many unsuccessful attempts to use them in practice.&#xA0;</span><span>In most cases, unsuccessful neural network implementations can be traced back to inappropriate neural network design decisions and general misconceptions about how they work. This article aims to articulate some of these misconceptions in the hopes that they might help individuals implementing neural networks meet with success.</span></p>
<p><span>For readers interested in getting more information, I have found the following books to be quite instructional when it comes to neural networks and their role in financial modelling&#xA0;and algorithmic trading.&#xA0;</span></p>
<div id="attachment_2473" class="wp-caption aligncenter"><img class="wp-image-2473 size-full" src="http://www.turingfinance.com/wp-content/uploads/2014/04/Neural-network-textbooks-300x118.png%20300w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Neural-network-textbooks-1024x404.png%201024w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Neural-network-textbooks-700x276.png%20700w,%20http://www.turingfinance.com/wp-content/uploads/2014/04/Neural-network-textbooks.png%201032w" alt="Neural network textbooks" width="1032"><p class="wp-caption-text">Some instructional textbooks when it comes to implementing neural networks and other machine learning algorithms in finance. Many of the misconceptions presented in this article are discussed in more detail in Professor Andries Engelbrecht's book, 'An Introduction to Computational Intelligence'</p></div>

 


</div> 
</div>
</body></html>
