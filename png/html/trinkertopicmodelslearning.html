<!DOCTYPE html><html><head><title>trinker/topicmodels_learning</title></head><body>
<h1>trinker/topicmodels_learning</h1><p><a href="https://github.com/trinker/topicmodels_learning" target="_new">Original URL</a></p>
<p><blockquote>Topic Models Learning and R Resources This is a collection documenting the resources I find related to topic models with an R flavored focus. A topic model is a type of generative model used to&hellip;</blockquote></p>
<div><article class="markdown-body entry-content"><h2><a id="user-content-topic-models-learning-and-r-resources" class="anchor" href="https://github.com/trinker/topicmodels_learning#topic-models-learning-and-r-resources"></a>Topic Models Learning and R Resources</h2>

<p>This is a collection documenting the resources I find related to topic
models with an R flavored focus. A <em>topic model</em> is a type of
<a href="http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm"><em>generative</em></a>
model used to "discover" latent topics that compose a <em>corpus</em> or
collection of documents. Typically topic modeling is used on a
collection of text documents but can be used for other modes including
use as caption generation for images.</p>

<p><a href="https://github.com/trinker/topicmodels_learning/blob/master/inst/figure/topic-model.jpg" target="_blank"><img src="https://github.com/trinker/topicmodels_learning/raw/master/inst/figure/topic-model.jpg" alt=""></a></p>

<h2><a id="user-content-table-of-contents" class="anchor" href="https://github.com/trinker/topicmodels_learning#table-of-contents"></a>Table of Contents</h2>



<h2><a id="user-content-just-the-essentials" class="anchor" href="https://github.com/trinker/topicmodels_learning#just-the-essentials"></a>Just the Essentials</h2>

<p>This is my run down of the minimal readings, websites, videos, &amp; scripts
the reader needs to become familiar with topic modeling. The list is in
an order I believe will be of greatest use and contains a nice mix of
introduction, theory, application, and interpretation. As you want to
learn more about topic modeling, the other sections will become more
useful.</p>

<ol>
<li> Boyd-Graber, J. (2013). <a href="https://www.youtube.com/watch?v=4p9MSJy761Y">Computational Linguistics I: Topic
Modeling</a><br></li>
<li> Underwood, T. (2012). <a href="http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/">Topic Modeling Made Just Simple
Enough</a></li>
<li> Weingart, S. (2012). <a href="http://www.scottbot.net/HIAL/?p=19113">Topic Modeling for Humanists: A Guided
Tour</a></li>
<li> Blei, D. M. (2012). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Blei2012.pdf">Probabilistic topic
models</a>. <em>Communications of the ACM, (55)</em>4,
77-84. doi:10.1145/2133806.2133826<br></li>
<li> inkhorn82 (2014). <a href="http://rforwork.info/2014/02/17/a-delicious-analysis/">A Delicious Analysis! (aka topic modelling using
recipes)</a>
<a href="https://gist.githubusercontent.com/inkhorn/9044779/raw/c7f0ba30d424aaeb75c5e221d12566f6732c4f29/recipe%20analysis.R">(CODE)</a></li>
<li> Gr&#xFC;en, B. &amp; Hornik, K. (2011). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Gruen2011.pdf">topicmodels: An R Package for
Fitting Topic Models.</a>. <em>Journal of
Statistical Software, 40</em>(13), 1-30.</li>
<li> Marwick, B. (2014a). <a href="http://stats.stackexchange.com/a/25128/7482">The input parameters for using latent
Dirichlet allocation</a></li>
<li> Tang, J., Meng, Z., Nguyen, X. , Mei, Q. , &amp; Zhang, M. (2014).
<a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Tang2014.pdf">Understanding the limiting factors of topic modeling via posterior
contraction analysis</a>. In <em>31 st
International Conference on Machine Learning</em>, 190-198.</li>
<li> Sievert, C. (2014). <a href="https://www.youtube.com/watch?v=IksL96ls4o0">LDAvis: A method for visualizing and
interpreting topic
models</a></li>
<li>Rhody, L. M. (2012). <a href="http://www.lisarhody.com/some-assembly-required">Some Assembly Required: Understanding and Interpreting Topics in LDA Models of Figurative Language</a></li>
<li>Rinker, T.W. (2015). <a href="https://raw.githubusercontent.com/trinker/topicmodels_learning/master/scripts/Example_topic_model_analysis.R">R Script: Example Topic Model
Analysis</a></li>
</ol>

<h2><a id="user-content-key-players" class="anchor" href="https://github.com/trinker/topicmodels_learning#key-players"></a>Key Players</h2>

<p>Papadimitriou, Raghavan, Tamaki &amp; Vempala, Santosh (1997) first
introduced the notion of topic modeling in their <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Papadimitriou1997.pdf">"Latent Semantic
Indexing: A probabilistic analysis"</a>.
Thomas Hofmann (1999) developed "Probabilistic latent semantic
indexing". Blei, Ng, &amp; Jordan (2003) proposed <em>latent Dirichlet
allocation</em> (LDA) as a means of modeling documents with multiple topics
but assumes the topic are uncorrelated. Blei &amp; Lafferty (2007) proposed
<em>correlated topics model</em> (CTM), extending LDA to allow for correlations
between topics. Roberts, Stewart, Tingley, &amp; Airoldi (2013) propose a
<a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Roberts2013.pdf"><em>Structural Topic Model</em></a> (STM), allowing
the inclusion of meta-data in the modeling process.</p>

<h2><a id="user-content-videos" class="anchor" href="https://github.com/trinker/topicmodels_learning#videos"></a>Videos</h2>

<h2><a id="user-content-introductory" class="anchor" href="https://github.com/trinker/topicmodels_learning#introductory"></a>Introductory</h2>

<ul>
<li> Boyd-Graber, J. (2013). <a href="https://www.youtube.com/watch?v=4p9MSJy761Y">Computational Linguistics I: Topic
Modeling</a></li>
</ul>

<h2><a id="user-content-theory" class="anchor" href="https://github.com/trinker/topicmodels_learning#theory"></a>Theory</h2>



<h2><a id="user-content-visualization" class="anchor" href="https://github.com/trinker/topicmodels_learning#visualization"></a>Visualization</h2>



<h2><a id="user-content-articles" class="anchor" href="https://github.com/trinker/topicmodels_learning#articles"></a>Articles</h2>

<h2><a id="user-content-applied" class="anchor" href="https://github.com/trinker/topicmodels_learning#applied"></a>Applied</h2>



<h2><a id="user-content-theoretical" class="anchor" href="https://github.com/trinker/topicmodels_learning#theoretical"></a>Theoretical</h2>

<ul>
<li> Blei, D. M. (2012). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Blei2012.pdf">Probabilistic topic
models</a>. <em>Communications of the ACM, (55)</em>4,
77-84. doi:10.1145/2133806.2133826</li>
<li> Blei, D. M. &amp; Lafferty, J. D. (2007) <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Blei2007.pdf">A correlated topic model of
Science</a>. <em>The Annals of Applied Statistics
1</em>(1), 17-35. doi:10.1214/07-AOAS114</li>
<li> Blei, D. M. &amp; Lafferty, J. D. (2009) <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Blei2009.pdf">Topic
models</a>. In A Srivastava, M Sahami (eds.),
<a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Srivastava2009.pdf"><em>Text mining: classification, clustering, and
applications</em></a>. Chapman &amp; Hall/CRC
Press. 71-93.<br></li>
<li> Blei, D. M. &amp; McAuliffe, J. (2008). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Blei2008.pdf">Supervised topic
models</a>. In Advances in Neural Information
Processing Systems 20, 1-8.</li>
<li> Blei, D. M., Ng, A.Y., &amp; Jordan, M.I. (2003). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Blei2003.pdf">Latent Dirichlet
Allocation</a>. <em>Journal of Machine Learning
Research, 3</em>, 993-1022.</li>
<li> Chang, J., Boyd-Graber, J. , Wang, C., Gerrish, S., &amp; Blei. D.
(2009). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Chang2009.pdf">Reading tea leaves: How humans interpret topic
models</a>. In <em>Neural Information Processing
Systems</em>.</li>
<li> Griffiths, T.L. &amp; Steyvers, M. (2004). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Griffiths2004.pdf">Finding Scientific
Topics</a>. Proceedings of the National
Academy of Sciences of the United States of America, 101, 5228-5235.</li>
<li> Griffiths, T.L., Steyvers, M., &amp; Tenenbaum, J.B.T. (2007). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Griffiths2007.pdf">Topics
in Semantic Representation</a>.
<em>Psychological Review, 114</em>(2), 211-244.</li>
<li> Gr&#xFC;en, B. &amp; Hornik, K. (2011). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Gruen2011.pdf">topicmodels: An R Package for
Fitting Topic Models.</a>. <em>Journal of
Statistical Software, 40</em>(13), 1-30.</li>
<li> Mimno, D. &amp; A. Mccallum. (2007). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Mimno2007.pdf">Organizing the OCA: learning
faceted subjects from a library of digital
books</a>. In <em>Joint Conference on Digital
Libraries</em>. ACM Press, New York, NY, 376&#xE2;&#x20AC;&#x201C;385.</li>
<li> Ponweiser, M. (2012). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Ponweiser2012.pdf">Latent Dirichlet Allocation in R (Diploma
Thesis)</a>. Vienna University of
Economics and Business, Vienna</li>
<li> Roberts M.E., Stewart B.M., Tingley D., &amp; Airoldi E.M. (2013) <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Roberts2013.pdf">The
Structural Topic Model and Applied Social
Science</a>. <em>Advances in Neural Information
Processing Systems Workshop on Topic Models: Computation,
Application, and Evaluation</em>, 1-4.<br></li>
<li> Roberts, M., Stewart, B., Tingley, D., Lucas, C., Leder-Luis, J.,
Gadarian, S., Albertson, B., et al. (2014). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Roberts2014.pdf">Structural topic models
for open ended survey responses</a>.
<em>American Journal of Political Science, American Journal of
Political Science, 58</em>(4), 1064-1082.</li>
<li> Roberts, M., Stewart, B., Tingley, D. (n.d.). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Robertsnd.pdf">stm: R Package for
Structural Topic Models</a>, 1-49.</li>
<li> Sievert, C. &amp; Shirley, K. E. (2014a). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Sievert2014a.pdf">LDAvis: A Method for
Visualizing and Interpreting Topics.</a> in
<em>Proceedings of the Workshop on Interactive Language Learning,
Visualization, and Interfaces</em> 63-70.</li>
<li> Steyvers, M. &amp; Griffiths, T. (2007). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Steyvers2007.pdf">Probabilistic topic
models</a>. In T. Landauer, D McNamara, S.
Dennis, and W. Kintsch (eds), <em>Latent Semantic Analysis: A Road to
Meaning</em>. Laurence Erlbaum</li>
<li> Taddy, M.A. (2012). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Taddy2012.pdf">On Estimation and Selection for Topic
Models</a> In <em>Proceedings of the 15th
International Conference on Artificial Intelligence and Statistics
(AISTATS 2012)</em>, 1184-1193.</li>
<li> Tang, J., Meng, Z., Nguyen, X. , Mei, Q. , &amp; Zhang, M. (2014).
<a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Tang2014.pdf">Understanding the limiting factors of topic modeling via posterior
contraction analysis</a>. In <em>31 st
International Conference on Machine Learning</em>, 190-198.</li>
</ul>

<h2><a id="user-content-websites--blogs" class="anchor" href="https://github.com/trinker/topicmodels_learning#websites--blogs"></a>Websites &amp; Blogs</h2>

<ul>
<li> Blei, D. (n.d.). <a href="https://www.cs.princeton.edu/%7Eblei/topicmodeling.html">Topic
Modeling</a></li>
<li> Jockers, M.L. (2013). <a href="http://www.matthewjockers.net/2013/04/12/secret-recipe-for-topic-modeling-themes/">"Secret" Recipe for Topic Modeling
Themes</a></li>
<li> Jones, T. (n.d.). <a href="http://www.biasedestimates.com/p/topic-models-reading-list.html">Topic Models Reading
List</a></li>
<li> Marwick, B. (2014a). <a href="http://stats.stackexchange.com/a/25128/7482">The input parameters for using latent
Dirichlet allocation</a></li>
<li> Marwick, B. (2014b). <a href="http://stackoverflow.com/a/21394092/1000343">Topic models: cross validation with
loglikelihood or
perplexity</a></li>
<li> Rhody, L. M. (2012). <a href="http://www.lisarhody.com/some-assembly-required">Some Assembly Required: Understanding and Interpreting 
Topics in LDA Models of Figurative Language</a></li>
<li> Schmidt, B.M. (2012). <a href="http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/">Words Alone: Dismantling Topic Models in the
Humanities</a></li>
<li> Underwood, T. (2012a). <a href="http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/">Topic Modeling Made Just Simple
Enough</a></li>
<li> Underwood, T. (2012b). <a href="http://tedunderwood.com/2012/04/01/what-kinds-of-topics-does-topic-modeling-actually-produce/">What kinds of "topics" does topic modeling
actually
produce?</a></li>
<li> Weingart, S. (2012). <a href="http://www.scottbot.net/HIAL/?p=19113">Topic Modeling for Humanists: A Guided
Tour</a></li>
<li> Weingart, S. (2011). <a href="http://www.scottbot.net/HIAL/?p=221">Topic Modeling and Network
Analysis</a></li>
</ul>

<h2><a id="user-content-r-resources" class="anchor" href="https://github.com/trinker/topicmodels_learning#r-resources"></a>R Resources</h2>

<h2><a id="user-content-package-comparisons" class="anchor" href="https://github.com/trinker/topicmodels_learning#package-comparisons"></a>Package Comparisons</h2>

<table>
<thead>
<tr>
<th>Package</th>
<th>Functionality</th>
<th>Pluses</th>
<th>Author</th>
<th>R Language Interface</th>
</tr>
</thead>
<tbody>
<tr>
<td>lda*</td>
<td>Collapsed Gibbs for LDA</td>
<td>Graphing utilities</td>
<td>Chang</td>
<td>R</td>
</tr>
<tr>
<td>topicmodels</td>
<td>LDA and CTM</td>
<td>Follows Blei's implementation; great vignette; takes</td>
<td>C</td>
<td><a href="https://en.wikipedia.org/wiki/Document-term_matrix">DTM</a></td>
</tr>
<tr>
<td>stm</td>
<td>Model w/ meta-data</td>
<td>Great documentation; nice visualization</td>
<td>Roberts, Stewart, &amp; Tingley</td>
<td>C</td>
</tr>
<tr>
<td>LDAvis</td>
<td>Interactive visualization</td>
<td>Aids in model interpretation</td>
<td>Sievert &amp; Shirley</td>
<td>R + Shiny</td>
</tr>
<tr>
<td>mallet**</td>
<td>LDA</td>
<td><a href="http://programminghistorian.org/lessons/topic-modeling-and-mallet">MALLET</a> is well known</td>
<td>Mimno</td>
<td>Java</td>
</tr>
</tbody>
</table>

<p>*<a href="http://stats.stackexchange.com/questions/24441/two-r-packages-for-topic-modeling-lda-and-topicmodels"><em>StackExchange discussion of lda vs.
topicmodels</em></a><br>
**<a href="http://programminghistorian.org/lessons/topic-modeling-and-mallet"><em>Setting Up
MALLET</em></a></p>

<h2><a id="user-content-r-specific-references" class="anchor" href="https://github.com/trinker/topicmodels_learning#r-specific-references"></a>R Specific References</h2>

<ul>
<li> Chang J. (2010). lda: Collapsed Gibbs Sampling Methods for Topic
Models. <a href="http://CRAN.R-project.org/package=lda">http://CRAN.R-project.org/package=lda</a>.</li>
<li> Gr&#xFC;en, B. &amp; Hornik, K. (2011). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Gruen2011.pdf">topicmodels: An R Package for
Fitting Topic Models.</a>. <em>Journal of
Statistical Software, 40</em>(13), 1-30.</li>
<li> Mimno, D. (2013). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Mimno2013.Rmd">vignette-mallet: A wrapper around the Java
machine learning tool MALLET</a>.
<a href="https://CRAN.R-project.org/package=mallet">https://CRAN.R-project.org/package=mallet</a></li>
<li> Ponweiser, M. (2012). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Ponweiser2012.pdf">Latent Dirichlet Allocation in R (Diploma
Thesis)</a>. Vienna University of
Economics and Business, Vienna.</li>
<li> Roberts, M., Stewart, B., Tingley, D. (n.d.). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Robertsnd.pdf">stm: R Package for
Structural Topic Models</a>, 1-49.</li>
<li> Sievert, C. &amp; Shirley, K. E. (2014a). <a href="https://github.com/trinker/topicmodels_learning/blob/master/Sievert2014a.pdf">LDAvis: A Method for
Visualizing and Interpreting Topics.</a> <em>Proceedings
of the Workshop on Interactive Language Learning, Visualization, and
Interfaces</em> 63-70.</li>
<li> Sievert, C. &amp; Shirley, K. E. (2014b). <a href="https://github.com/trinker/topicmodels_learning/blob/master/articles/Sievert2014b.pdf">Vignette: LDAvis
details.</a> 1-5.</li>
</ul>

<h2><a id="user-content-example-modeling" class="anchor" href="https://github.com/trinker/topicmodels_learning#example-modeling"></a>Example Modeling</h2>



<h2><a id="user-content-topic-modeling-r-demo" class="anchor" href="https://github.com/trinker/topicmodels_learning#topic-modeling-r-demo"></a>Topic Modeling R Demo</h2>

<h2><a id="user-content-topicmodels-package" class="anchor" href="https://github.com/trinker/topicmodels_learning#topicmodels-package"></a>topicmodels Package</h2>

<p>The .R script for this demonstration can be downloaded from
<a href="https://raw.githubusercontent.com/trinker/topicmodels_learning/master/scripts/Example_topic_model_analysis.R">scripts/Example_topic_model_analysis.R</a></p>

<h3><a id="user-content-installload-tools--data" class="anchor" href="https://github.com/trinker/topicmodels_learning#installload-tools--data"></a>Install/Load Tools &amp; Data</h3>

<pre><code>if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("trinker/gofastr")
pacman::p_load(tm, topicmodels, dplyr, tidyr, igraph, devtools, LDAvis, ggplot2)

## Source topicmodels2LDAvis &amp; optimal_k functions
invisible(lapply(
 file.path(
 "https://raw.githubusercontent.com/trinker/topicmodels_learning/master/functions", 
 c("topicmodels2LDAvis.R", "optimal_k.R")
 ),
 devtools::source_url
))

## SHA-1 hash of file is 5ac52af21ce36dfe8f529b4fe77568ced9307cf0
## SHA-1 hash of file is 7f0ab64a94948c8b60ba29dddf799e3f6c423435

data(presidential_debates_2012)
</code></pre>

<h3><a id="user-content-generate-stopwords" class="anchor" href="https://github.com/trinker/topicmodels_learning#generate-stopwords"></a>Generate Stopwords</h3>

<pre><code>stops &lt;- c(
 tm::stopwords("english"),
 tm::stopwords("SMART"),
 "governor", "president", "mister", "obama","romney"
 ) %&gt;%
 gofastr::prep_stopwords() 
</code></pre>

<h3><a id="user-content-create-the-documenttermmatrix" class="anchor" href="https://github.com/trinker/topicmodels_learning#create-the-documenttermmatrix"></a>Create the DocumentTermMatrix</h3>

<pre><code>doc_term_mat &lt;- presidential_debates_2012 %&gt;%
 with(gofastr::q_dtm_stem(dialogue, paste(person, time, sep = "_"))) %&gt;% 
 gofastr::remove_stopwords(stops, stem=TRUE) %&gt;% 
 gofastr::filter_tf_idf() %&gt;%
 gofastr::filter_documents() 
</code></pre>

<h3><a id="user-content-control-list" class="anchor" href="https://github.com/trinker/topicmodels_learning#control-list"></a>Control List</h3>

<pre><code>control &lt;- list(burnin = 500, iter = 1000, keep = 100, seed = 2500)
</code></pre>

<h3><a id="user-content-determine-optimal-number-of-topics" class="anchor" href="https://github.com/trinker/topicmodels_learning#determine-optimal-number-of-topics"></a>Determine Optimal Number of Topics</h3>

<p>The plot below shows the harmonic mean of the log likelihoods against k
(number of topics).</p>

<pre><code>(k &lt;- optimal_k(doc_term_mat, 40, control = control))

## 
## Grab a cup of coffee this could take a while...

## 10 of 40 iterations (Current: 08:54:32; Elapsed: .2 mins)
## 20 of 40 iterations (Current: 08:55:07; Elapsed: .8 mins; Remaining: ~2.3 mins)
## 30 of 40 iterations (Current: 08:56:03; Elapsed: 1.7 mins; Remaining: ~1.3 mins)
## 40 of 40 iterations (Current: 08:57:30; Elapsed: 3.2 mins; Remaining: ~0 mins)
## Optimal number of topics = 20
</code></pre>

<p><a href="https://github.com/trinker/topicmodels_learning/blob/master/inst/figure/unnamed-chunk-7-1.png" target="_blank"><img src="https://github.com/trinker/topicmodels_learning/raw/master/inst/figure/unnamed-chunk-7-1.png" alt=""></a></p>

<p>It appears the optimal number of topics is ~k = 20.</p>

<h3><a id="user-content-run-the-model" class="anchor" href="https://github.com/trinker/topicmodels_learning#run-the-model"></a>Run the Model</h3>

<pre><code>control[["seed"]] &lt;- 100
lda_model &lt;- topicmodels::LDA(doc_term_mat, k=as.numeric(k), method = "Gibbs", 
 control = control)
</code></pre>

<h3><a id="user-content-plot-the-topics-per-person--time" class="anchor" href="https://github.com/trinker/topicmodels_learning#plot-the-topics-per-person--time"></a>Plot the Topics Per Person &amp; Time</h3>

<pre><code>topics &lt;- topicmodels::posterior(lda_model, doc_term_mat)[["topics"]]
topic_dat &lt;- dplyr::add_rownames(as.data.frame(topics), "Person_Time")
colnames(topic_dat)[-1] &lt;- apply(terms(lda_model, 10), 2, paste, collapse = ", ")

tidyr::gather(topic_dat, Topic, Proportion, -c(Person_Time)) %&gt;%
 tidyr::separate(Person_Time, c("Person", "Time"), sep = "_") %&gt;%
 dplyr::mutate(Person = factor(Person, 
 levels = c("OBAMA", "ROMNEY", "LEHRER", "SCHIEFFER", "CROWLEY", "QUESTION" ))
 ) %&gt;%
 ggplot2::ggplot(ggplot2::aes(weight=Proportion, x=Topic, fill=Topic)) +
 ggplot2::geom_bar() +
 ggplot2::coord_flip() +
 ggplot2::facet_grid(Person~Time) +
 ggplot2::guides(fill=FALSE) +
 ggplot2::xlab("Proportion")
</code></pre>

<p><a href="https://github.com/trinker/topicmodels_learning/blob/master/inst/figure/unnamed-chunk-9-1.png" target="_blank"><img src="https://github.com/trinker/topicmodels_learning/raw/master/inst/figure/unnamed-chunk-9-1.png" alt=""></a></p>

<h3><a id="user-content-plot-the-topics-matrix-as-a-heatmap" class="anchor" href="https://github.com/trinker/topicmodels_learning#plot-the-topics-matrix-as-a-heatmap"></a>Plot the Topics Matrix as a Heatmap</h3>

<pre><code>heatmap(topics, scale = "none")
</code></pre>

<p><a href="https://github.com/trinker/topicmodels_learning/blob/master/inst/figure/unnamed-chunk-10-1.png" target="_blank"><img src="https://github.com/trinker/topicmodels_learning/raw/master/inst/figure/unnamed-chunk-10-1.png" alt=""></a></p>

<h3><a id="user-content-network-of-the-word-distributions-over-topics-topic-relation" class="anchor" href="https://github.com/trinker/topicmodels_learning#network-of-the-word-distributions-over-topics-topic-relation"></a>Network of the Word Distributions Over Topics (Topic Relation)</h3>

<pre><code>post &lt;- topicmodels::posterior(lda_model)

cor_mat &lt;- cor(t(post[["terms"]]))
cor_mat[ cor_mat &lt; .05 ] &lt;- 0
diag(cor_mat) &lt;- 0

graph &lt;- graph.adjacency(cor_mat, weighted=TRUE, mode="lower")
graph &lt;- delete.edges(graph, E(graph)[ weight &lt; 0.05])

E(graph)$edge.width &lt;- E(graph)$weight*20
V(graph)$label &lt;- paste("Topic", V(graph))
V(graph)$size &lt;- colSums(post[["topics"]]) articles _articles bin CAs devops Documents dotfiles gethtml go hnews jason js netdata start-thesrc thesrc ucii 15

par(mar=c(0, 0, 3, 0))
set.seed(110)
plot.igraph(graph, edge.width = E(graph)$edge.width, 
 edge.color = "orange", vertex.color = "orange", 
 vertex.frame.color = NA, vertex.label.color = "grey30")
title("Strength Between Topics Based On Word Probabilities", cex.main=.8)
</code></pre>

<p><a href="https://github.com/trinker/topicmodels_learning/blob/master/inst/figure/unnamed-chunk-11-1.png" target="_blank"><img src="https://github.com/trinker/topicmodels_learning/raw/master/inst/figure/unnamed-chunk-11-1.png" alt=""></a></p>

<h3><a id="user-content-network-of-the-topics-over-dcouments-topic-relation" class="anchor" href="https://github.com/trinker/topicmodels_learning#network-of-the-topics-over-dcouments-topic-relation"></a>Network of the Topics Over Dcouments (Topic Relation)</h3>

<pre><code>minval &lt;- .1
topic_mat &lt;- topicmodels::posterior(lda_model)[["topics"]]

graph &lt;- graph_from_incidence_matrix(topic_mat, weighted=TRUE)
graph &lt;- delete.edges(graph, E(graph)[ weight &lt; minval])

E(graph)$edge.width &lt;- E(graph)$weight*17
E(graph)$color &lt;- "blue"
V(graph)$color &lt;- ifelse(grepl("^\\d+$", V(graph)$name), "grey75", "orange")
V(graph)$frame.color &lt;- NA
V(graph)$label &lt;- ifelse(grepl("^\\d+$", V(graph)$name), paste("topic", V(graph)$name), gsub("_", "\n", V(graph)$name))
V(graph)$size &lt;- c(rep(10, nrow(topic_mat)), colSums(topic_mat) articles _articles bin CAs devops Documents dotfiles gethtml go hnews jason js netdata start-thesrc thesrc ucii 20)
V(graph)$label.color &lt;- ifelse(grepl("^\\d+$", V(graph)$name), "red", "grey30")

par(mar=c(0, 0, 3, 0))
set.seed(369)
plot.igraph(graph, edge.width = E(graph)$edge.width, 
 vertex.color = adjustcolor(V(graph)$color, alpha.f = .4))
title("Topic &amp; Document Relationships", cex.main=.8)
</code></pre>

<p><a href="https://github.com/trinker/topicmodels_learning/blob/master/inst/figure/unnamed-chunk-12-1.png" target="_blank"><img src="https://github.com/trinker/topicmodels_learning/raw/master/inst/figure/unnamed-chunk-12-1.png" alt=""></a></p>

<h3><a id="user-content-ldavis-of-model" class="anchor" href="https://github.com/trinker/topicmodels_learning#ldavis-of-model"></a>LDAvis of Model</h3>

<p>The output from <strong>LDAvis</strong> is not easily embedded within an R markdown
document, however, the reader may <a href="http://trinker.github.io/LDAvis/example/">see the results
here</a>.</p>

<pre><code>lda_model %&gt;%
 topicmodels2LDAvis() %&gt;%
 LDAvis::serVis()
</code></pre>

<h3><a id="user-content-apply-model-to-new-data" class="anchor" href="https://github.com/trinker/topicmodels_learning#apply-model-to-new-data"></a>Apply Model to New Data</h3>

<pre><code>## Create the DocumentTermMatrix for New Data
doc_term_mat2 &lt;- partial_republican_debates_2015 %&gt;%
 with(gofastr::q_dtm_stem(dialogue, paste(person, location, sep = "_"))) %&gt;% 
 gofastr::remove_stopwords(stops, stem=TRUE) %&gt;% 
 gofastr::filter_tf_idf() %&gt;%
 gofastr::filter_documents() 


## Update Control List
control2 &lt;- control
control2[["estimate.beta"]] &lt;- FALSE


## Run the Model for New Data
lda_model2 &lt;- topicmodels::LDA(doc_term_mat2, k = k, model = lda_model, 
 control = list(seed = 100, estimate.beta = FALSE))


## Plot the Topics Per Person &amp; Location for New Data
topics2 &lt;- topicmodels::posterior(lda_model2, doc_term_mat2)[["topics"]]
topic_dat2 &lt;- dplyr::add_rownames(as.data.frame(topics2), "Person_Location")
colnames(topic_dat2)[-1] &lt;- apply(terms(lda_model2, 10), 2, paste, collapse = ", ")

tidyr::gather(topic_dat2, Topic, Proportion, -c(Person_Location)) %&gt;%
 tidyr::separate(Person_Location, c("Person", "Location"), sep = "_") %&gt;%
 ggplot2::ggplot(ggplot2::aes(weight=Proportion, x=Topic, fill=Topic)) +
 ggplot2::geom_bar() +
 ggplot2::coord_flip() +
 ggplot2::facet_grid(Person~Location) +
 ggplot2::guides(fill=FALSE) +
 ggplot2::xlab("Proportion")


## LDAvis of Model for New Data
lda_model2 %&gt;%
 topicmodels2LDAvis() %&gt;%
 LDAvis::serVis()
</code></pre>

<h2><a id="user-content-contributing" class="anchor" href="https://github.com/trinker/topicmodels_learning#contributing"></a>Contributing</h2>

<p>You are welcome to: </p>


</article>
 </div>
</body></html>
