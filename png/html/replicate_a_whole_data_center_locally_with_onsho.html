<!DOCTYPE html><html><head><title>Replicate A Whole Data Center Locally With Onsho</title></head><body>
<h1>Replicate A Whole Data Center Locally With Onsho</h1><p><a href="https://blog.giantswarm.io/replicate-data-center-locally-with-onsho/" target="_new">Original URL</a></p>
<p><blockquote>Two weeks ago we introduced Mayu and Yochu, a set of tools to set up customized CoreOS clusters on bare metal, available as open source. However, not everyone has access to some bare metal or spare&hellip;</blockquote></p>
<div><div class="body">
 <p>Two weeks ago we introduced <a href="https://blog.giantswarm.io/mayu-yochu-provisioning-tools-for-coreos-bare-metal/">Mayu and Yochu</a>, a set of tools to set up customized CoreOS clusters on bare metal, available as open source. However, not everyone has access to some bare metal or spare servers sitting in his basement. This is why we created <a href="https://github.com/giantswarm/onsho">Onsho</a>, which we are releasing today as open source software.</p>

<p><img src="https://blog.giantswarm.io/content/images/2016/02/1024px-Apis_mellifera_carnica_worker_honeycomb_2.jpg" alt=""></p>

<p>Onsho is a tool that manages <a href="http://qemu.org/">QEMU</a> VMs. We use it to start one or more iPXE-enabled virtual machines that will then be provisioned by <a href="https://github.com/giantswarm/mayu">Mayu</a> (and if you want also <a href="https://github.com/giantswarm/yochu">Yochu</a>). You can also use it with any other PXE-serving software or let it boot from non-PXE images. With that you can replicate a whole data center on a single machine or VM. You can then use that DC to test out infrastructure software. For example we use it to to simulate whole instantiations of our microservice platform and test out new modules like e.g. <a href="https://prometheus.io/">Prometheus monitoring</a>.</p>

<h2 id="lesstalkmorevmaction">Less Talk, More VM Action</h2>

<p>To make this more tangible, let&#x2019;s just try it out. Note that this currently only works under Linux with <a href="https://en.wikibooks.org/wiki/QEMU/Installing_QEMU">QEMU installed</a>. If you are using it in a Linux VM, be sure that your VM provider supports nested VMs. On Macs as of this moment this is possible with VMWare Fusion or Parallels Desktop, which both have a free trial version. VirtualBox is currently not supported on the Mac. If you want to write an <a href="https://github.com/mist64/xhyve">xhyve</a> or other driver for Onsho feel free to contribute to the <a href="https://github.com/giantswarm/onsho">GitHub repo</a>.</p>

<p>First, we need to set up a bridge for the Onsho machines to have a separate network they can have fun in.</p>

<pre><code>sudo -i 
brctl addbr onsho0 
ip link set up dev onsho0 
ip addr add 10.0.3.251/22 dev onsho0 
mkdir -p /etc/qemu 
echo 'allow onsho0' &gt;&gt; /etc/qemu/bridge.conf 
</code></pre>

<p>Note that this is not persistent after reboot. For examples on how to do this with <code>systemd-networkd</code> or on fedora see the <a href="https://github.com/giantswarm/onsho/tree/master/host-conf/"><code>host-conf</code> directory in the repo</a>.</p>

<p>Next, we want to get <a href="https://github.com/giantswarm/mayu">Mayu</a> and run it on our host machine, so we have a PXE endpoint the Onsho VMs will be able to talk to. We&#x2019;re using the Docker image here, so naturally you need <a href="https://docs.docker.com/engine/installation/">Docker installed</a> for this. For the image to be self-contained, we recommend to build your own image with a customized config file like follows:</p>

<pre><code>wget https://downloads.giantswarm.io/mayu/latest/mayu.tar.gz 
mkdir mayu 
tar xzf mayu.tar.gz -C mayu 
cd mayu 
</code></pre>

<p>Fetch the CoreOS version you would like to use:</p>

<pre><code>./fetch-coreos-image 835.13.0
</code></pre>

<p>Check the versions of docker, etcd and fleet you would like to install. There are defaults defined in the <code>./fetch-mayu-asset</code> script.</p>

<pre><code>grep 'VERSION=' fetch-mayu-assets 
./fetch-mayu-assets
</code></pre>

<p>There are a few things you should configure before Mayu is usable:</p>

<ul>
<li>add your SSH key to the config.yaml (replace '<your>')</your></li>
<li>adapt docker, fleet and etcd versions</li>
<li>add <code>no_secure: true</code> to the config if you don't want to use TLS in your local setup</li>
<li>change the interface (bond0) to something like onsho0</li>
</ul>

<pre><code>cp config.yaml.dist config.yaml 
vi config.yaml 
</code></pre>

<p>Now Mayus configuration is in place and we can build and run the container.</p>

<pre><code>docker build -t mayu . 
</code></pre>

<p>and run it.</p>

<pre><code>docker run --rm -it \ 
 --cap-add=NET_ADMIN \
 --net=host \
 -v /var/lib/mayu/cluster:/opt/mayu/cluster \
 mayu \
 -v=12
</code></pre>

<p>Now that we have Mayu running, we need to get Onsho.</p>

<pre><code>wget https://github.com/giantswarm/onsho/releases/download/0.4.0/onsho.0.4.0.tar.gz 
tar xzf onsho.0.4.0.tar.gz 
cd onsho 
</code></pre>

<p>Finally, we can start our cluster with a single command. We&#x2019;ll start three machines to get an etcd quorum. If your machine doesn&#x2019;t have a lot of RAM, be sure to start less machines or give each machine less RAM as the default per machine is 1024 MB.</p>

<pre><code>./onsho create --num-vms=3 --image=ipxe/ipxe.iso
</code></pre>

<p>Now, we just attach to the created <code>tmux</code> session (called <code>zoo</code>), lean back and watch the cluster get bootstrapped.</p>

<pre><code>tmux a -t zoo 
</code></pre>

<p>When following the bootstrapping process you can see how each machine boots multiple times until it&#x2019;s fully provisioned. First, it boots over DHCP, and gets its first kernel image and subsequently the initial root directory from Mayu. Then, on the next boot it gets bootstrapped with the desired vanilla CoreOS version and configured with the <a href="https://coreos.com/os/docs/latest/cloud-config.html">Cloud-Config</a> that Mayu provides. On the final boot we have a fully configured and running CoreOS machine. Once all machines are through this process (which roughly runs in parallel), you can check the status of the cluster in <code>tmux</code>:</p>

<pre><code>$ fleetctl list-machines
MACHINE IP METADATA 
00006811af601fe8e1d3f37902021ae0 10.0.3.31 role-core=true 
0000906eb12096e3d94b002c663943f9 10.0.3.33 role-core=true 
0000d71391dc5317a0a1798d6bd5448f 10.0.3.32 role-core=true 
</code></pre>

<p>Now you can use <code>onsho</code> to manage this cluster. You can for example add or remove machines, start and stop them, or wipe a machine so it gets set up fresh. With this you have a clean way to provision clusters locally that very closely resemble a bare metal setup provisioned by Mayu. As mentioned above, you can use this to test out infrastructure modules as well as whole microservice platforms based on CoreOS. You can also test out different versions of fleet, etcd, and Docker provisioned by <a href="https://github.com/giantswarm/yochu">Yochu</a> onto different versions of CoreOS. As fleet is particularly central to these clusters, we are working extensively on <a href="https://github.com/coreos/fleet/pull/1426">making it more performant</a>. Be sure to watch this blog for a post about our fleet benchmarking tool - to be published very soon.</p>
 </div>

 </div>
</body></html>
