<!DOCTYPE html><html><head><title>Principal Component Analysis in 4 Steps</title></head><body>
<h1>Principal Component Analysis in 4 Steps</h1><p><a href="http://alexhwoods.com/pca/" target="_new">Original URL</a></p>
<p><blockquote>Recently I wrote a couple posts about&#xA0;eigenvectors&#xA0;and&#xA0;eigenvalues. I thought it would be cool to go from that slightly more theoretical material and show something super useful in&hellip;</blockquote></p>
<div><article id="453" class="single-story post-453 post type-post status-publish format-standard has-post-thumbnail hentry category-data-science category-linear-algebra-2 category-machine-learning category-r">
							
												
			
			
						<p>Recently I wrote a couple posts about&#xA0;<a href="http://alexhwoods.com/eigenvectors/">eigenvectors</a>&#xA0;and&#xA0;<a href="http://alexhwoods.com/eigenvalues/">eigenvalues</a>. I thought it would be cool to go from that slightly more theoretical material and show something super useful in which eigenvectors / eigenvalues are an integral part. So today, I&#x2019;m going to talk about Principal Component Analysis.</p>
<p>This algorithm is used everywhere, notably in <a href="https://en.wikipedia.org/wiki/Principal_component_analysis#Neuroscience">neuroscience</a> and computer graphics. The idea is you have a dataset with a ton of features and you want to reduce it to it&#x2019;s core components. With high dimensionality, not only is the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a> a problem, but you also just can&#x2019;t visualize the data, which prevents a lot of basic insights. So we want to reduce the dimensionality without losing vital information. This is where PCA comes in. In PCA, we go from a large number of features to a small number of components, which still contain a sufficient proportion of the information.</p>
<p>Before we talk about the algorithm itself, there are a few important math concepts which you must be familiar with in order to proceed. The first is eigenvalues and eigenvectors. You can read about them&#xA0;<a href="http://alexhwoods.com/eigenvalues/">here</a>.&#xA0;The second is the covariance matrix.</p>
<h3>Covariance Matrix</h3>
<p><a href="http://mathworld.wolfram.com/Covariance.html">Covariance</a>&#xA0;is the measure of how two different variables change together. The covariance between two variables, X and Y, can be given by the following formula.</p>
<p><a href="http://i1.wp.com/alexhwoods.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-18-at-9.37.50-AM.jpg" rel="attachment wp-att-454"><img class="alignnone size-full wp-image-454" src="http://i1.wp.com/alexhwoods.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-18-at-9.37.50-AM.jpg?resize=238%2C53" alt="Screen Shot 2016-01-18 at 9.37.50 AM"></a></p>



<p>Now, if we wanted to look at all the possible covariances in a dataset, we can compute the covariance matrix, which has this form &#x2013;</p>
<p><a href="http://i1.wp.com/alexhwoods.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-18-at-9.43.14-AM.jpg" rel="attachment wp-att-455"><img class="alignnone size-full wp-image-455" src="http://i1.wp.com/alexhwoods.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-18-at-9.43.14-AM.jpg?resize=264%2C72" alt="Screen Shot 2016-01-18 at 9.43.14 AM"></a></p>



<p>Notice that this matrix will be <a href="https://en.wikipedia.org/wiki/Symmetric_matrix">symmetric</a>&#xA0;(A = A^T), and will have a diagonal of just variances, because cov(x, x) is the same thing as the variance of x. If you understand the covariance matrix and eigenvalues/vectors, you&#x2019;re ready to learn about PCA.</p>

<h3>Principal Component Analysis</h3>
<p>Here is the central idea of PCA &#x2013;&#xA0;<b>the components are the eigenvectors of the covariance matrix.</b>&#xA0;Moreover, the amount of variance each component captures can be represented by the magnitude of its corresponding eigenvalue. Read that multiple times.</p>
<p>So basically,&#xA0;PCA can be represented in 4 pretty simple steps.</p>
<p>1. <b>Normalize the data.</b>&#xA0;We&#x2019;re dealing with covariance, so it&#x2019;s a good idea to have features on the same scale.<br>
2. <b>Calculate the covariance matrix.&#xA0;</b><br>
3. <b>Find the eigenvectors of the covariance matrix.</b><br>
4. <b>Translate the data to be in terms of the components</b>. This involves just a simple matrix multiplication.</p>

<p>One more important thing to mention about this algorithm. We are going from<em> features</em> to <em>components</em>. The translated dataset won&#x2019;t have a literal significance with respect to each component. It&#x2019;s very easy to digest what a <em>feature</em>&#xA0;is&#x2014;shoe size, weight, etc.&#x2014;but a component we can only talk about from a mathematical perspective. If you want to gain a sense of understanding about a data point, you&#x2019;ll have to reference the original dataset.</p>

<h3>Example</h3>
<p>Now I&#x2019;ll show you how to do PCA manually, and then using R&#x2019;s built-in function. In practice you&#x2019;ll usually use the built in function, but doing it manually is a great way to learn what&#x2019;s happening.</p>
<p>I found some data on the psychology within a large financial company. The dataset is called <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/attitude.html">attitude</a>, and is accessible within R.</p>

		<div id="crayon-56a2408694d63675213342" class="crayon-syntax crayon-theme-solarized-light crayon-font-monaco crayon-os-pc print-yes notranslate">
		
			<p class="crayon-toolbar"><span class="crayon-title">Get the data</span>
			</p>
			
			
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums ">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56a2408694d63675213342-1"><span class="crayon-r">library</span><span class="crayon-sy">(</span><span class="crayon-v">MASS</span><span class="crayon-sy">)</span><span class="crayon-c"># install.packages("MASS") if you have to</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d63675213342-2"><span class="crayon-v">attitude</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-v">attitude</span><span class="crayon-c"># put the data in our workspace in R</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>


<p>Ok, now let&#x2019;s do those 4 steps.<br>
1. Normalize the data. This literally means put each feature on a normal curve. Just like you would calculate a z-score, subtract the mean and divide by the standard deviation, to the entire feature vector.</p>

		<div id="crayon-56a2408694d75561885439" class="crayon-syntax crayon-theme-solarized-light crayon-font-monaco crayon-os-pc print-yes notranslate">
		
			<p class="crayon-toolbar"><span class="crayon-title">Step 1 - Normalize the data</span>
			</p>
			
			
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums ">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56a2408694d75561885439-1"><span class="crayon-r">attach</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span><span class="crayon-c"># to save me having to type 'attitude' 20 times</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d75561885439-2"><span class="crayon-v">attitude</span><span class="crayon-sy">$</span><span class="crayon-v">rating</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-sy">(</span><span class="crayon-v">rating</span><span class="crayon-o">-</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-v">rating</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-o">/</span><span class="crayon-e">sd</span><span class="crayon-sy">(</span><span class="crayon-v">rating</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56a2408694d75561885439-3"><span class="crayon-v">attitude</span><span class="crayon-sy">$</span><span class="crayon-v">complaints</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-sy">(</span><span class="crayon-v">complaints</span><span class="crayon-o">-</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-v">complaints</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-o">/</span><span class="crayon-e">sd</span><span class="crayon-sy">(</span><span class="crayon-v">complaints</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d75561885439-4"><span class="crayon-v">attitude</span><span class="crayon-sy">$</span><span class="crayon-v">privileges</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-sy">(</span><span class="crayon-v">privileges</span><span class="crayon-o">-</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-v">privileges</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-o">/</span><span class="crayon-e">sd</span><span class="crayon-sy">(</span><span class="crayon-v">privileges</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56a2408694d75561885439-5"><span class="crayon-v">attitude</span><span class="crayon-sy">$</span><span class="crayon-v">learning</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-sy">(</span><span class="crayon-v">learning</span><span class="crayon-o">-</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-v">learning</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-o">/</span><span class="crayon-e">sd</span><span class="crayon-sy">(</span><span class="crayon-v">learning</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d75561885439-6"><span class="crayon-v">attitude</span><span class="crayon-sy">$</span><span class="crayon-v">raises</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-sy">(</span><span class="crayon-v">raises</span><span class="crayon-o">-</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-v">raises</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-o">/</span><span class="crayon-e">sd</span><span class="crayon-sy">(</span><span class="crayon-v">raises</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56a2408694d75561885439-7"><span class="crayon-v">attitude</span><span class="crayon-sy">$</span><span class="crayon-v">critical</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-sy">(</span><span class="crayon-v">critical</span><span class="crayon-o">-</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-v">critical</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-o">/</span><span class="crayon-e">sd</span><span class="crayon-sy">(</span><span class="crayon-v">critical</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d75561885439-8"><span class="crayon-v">attitude</span><span class="crayon-sy">$</span><span class="crayon-v">advance</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-sy">(</span><span class="crayon-v">advance</span><span class="crayon-o">-</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-v">advance</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-o">/</span><span class="crayon-e">sd</span><span class="crayon-sy">(</span><span class="crayon-v">advance</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d75561885439-10"><span class="crayon-c"># re-attach so it calls the updated feature</span></p><p class="crayon-line" id="crayon-56a2408694d75561885439-11"><span class="crayon-r">attach</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56a2408694d75561885439-13"><span class="crayon-e">summary</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span><span class="crayon-c"># means are all 0</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d75561885439-14"><span class="crayon-e">sd</span><span class="crayon-sy">(</span><span class="crayon-v">privileges</span><span class="crayon-sy">)</span><span class="crayon-c"># and sd's are all 1 (you can check them all if you like)</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>


<p>2. Get the covariance matrix</p>

		<div id="crayon-56a2408694d79719868712" class="crayon-syntax crayon-theme-solarized-light crayon-font-monaco crayon-os-pc print-yes notranslate">
		
			<p class="crayon-toolbar"><span class="crayon-title">Step 2 - The Covariance Matrix</span>
			</p>
			
			
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums ">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56a2408694d79719868712-1"><span class="crayon-p"># this is actually really simple..thanks R :)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d79719868712-2"><span class="crayon-e">cov</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d79719868712-4"><span class="crayon-p"># Quiz question - Why is the diagonal all 1's? </span></p><p class="crayon-line" id="crayon-56a2408694d79719868712-5"><span class="crayon-p"># Because we normalized each feature to have a variance of 1!</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>


<p>3. The Principal Components</p>
<p>Remember, the principal components are the eigenvectors of the covariance matrix.</p>

		<div id="crayon-56a2408694d7c865529002" class="crayon-syntax crayon-theme-solarized-light crayon-font-monaco crayon-os-pc print-yes notranslate">
		
			<p class="crayon-toolbar"><span class="crayon-title">Step 3 - The Principal Components</span>
			</p>
			
			
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums ">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56a2408694d7c865529002-1"><span class="crayon-v">x</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-e">eigen</span><span class="crayon-sy">(</span><span class="crayon-e">cov</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d7c865529002-2"><span class="crayon-v">x</span><span class="crayon-sy">$</span><span class="crayon-v">vectors</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d7c865529002-4"><span class="crayon-c"># just out of curiosity, I'm going to check that it did this right (%*% is matrix multiplication in R)</span></p><p class="crayon-line" id="crayon-56a2408694d7c865529002-5"><span class="crayon-e">cov</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span><span class="crayon-o">%</span><span class="crayon-o">*</span><span class="crayon-o">%</span><span class="crayon-v">x</span><span class="crayon-sy">$</span><span class="crayon-v">vectors</span><span class="crayon-sy">[</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-c"># gives the same values as...</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d7c865529002-6"><span class="crayon-v">x</span><span class="crayon-sy">$</span><span class="crayon-v">values</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-o">*</span><span class="crayon-v">x</span><span class="crayon-sy">$</span><span class="crayon-v">vectors</span><span class="crayon-sy">[</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">]</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>


<p>4.&#xA0;Putting the data in terms of the components</p>
<p>We do this by matrix multiplying the transpose of the feature vector and the transpose of matrix containing the data. Why transpose? Theoretical reasons aside, the dimensions have to line up.</p>
<p><a href="http://i2.wp.com/alexhwoods.com/wp-content/uploads/2016/01/Step4Formula.png" rel="attachment wp-att-456"><img class="alignnone size-medium wp-image-456" src="http://i2.wp.com/alexhwoods.com/wp-content/uploads/2016/01/Step4Formula.png?resize=300%2C95%20300w,%20http://i2.wp.com/alexhwoods.com/wp-content/uploads/2016/01/Step4Formula.png?w=631%20631w" alt="Step4Formula"></a></p>




		<div id="crayon-56a2408694d80653234410" class="crayon-syntax crayon-theme-solarized-light crayon-font-monaco crayon-os-pc print-yes notranslate">
		
			<p class="crayon-toolbar"><span class="crayon-title">Step 4 - Translating the data</span>
			</p>
			
			
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums ">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56a2408694d80653234410-1"><span class="crayon-v">A</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-v">x</span><span class="crayon-sy">$</span><span class="crayon-v">vectors</span><span class="crayon-sy">[</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-o">:</span><span class="crayon-cn">3</span><span class="crayon-sy">]</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d80653234410-2"><span class="crayon-v">B</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-v">data</span><span class="crayon-sy">.</span><span class="crayon-e">matrix</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span><span class="crayon-c"># because we can't do matrix multiplication with data frames!</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d80653234410-4"><span class="crayon-c"># now we arrive at the new data by the above formula!</span></p><p class="crayon-line" id="crayon-56a2408694d80653234410-5"><span class="crayon-v">newData</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-t ">t</span><span class="crayon-sy">(</span><span class="crayon-v">A</span><span class="crayon-sy">)</span><span class="crayon-o">%</span><span class="crayon-o">*</span><span class="crayon-o">%</span><span class="crayon-t ">t</span><span class="crayon-sy">(</span><span class="crayon-v">B</span><span class="crayon-sy">)</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>


<p>And then just a hint of data cleaning so we can have a nice data frame to work with and run algorithms on.</p>

		<div id="crayon-56a2408694d84876580690" class="crayon-syntax crayon-theme-solarized-light crayon-font-monaco crayon-os-pc print-yes notranslate">
		
			<p class="crayon-toolbar"><span class="crayon-title">Some data cleaning</span>
			</p>
			
			
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums ">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56a2408694d84876580690-1"><span class="crayon-c"># note - in the newData matrix, each row is a feature, and each column is a data point. Let's change that</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d84876580690-2"><span class="crayon-v">newData</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-t ">t</span><span class="crayon-sy">(</span><span class="crayon-v">newData</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56a2408694d84876580690-3"><span class="crayon-v">newData</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-v">data</span><span class="crayon-sy">.</span><span class="crayon-e">frame</span><span class="crayon-sy">(</span><span class="crayon-v">newData</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d84876580690-4"><span class="crayon-e">names</span><span class="crayon-sy">(</span><span class="crayon-v">newData</span><span class="crayon-sy">)</span><span class="crayon-o">&lt;</span><span class="crayon-o">-</span><span class="crayon-e">c</span><span class="crayon-sy">(</span><span class="crayon-s">"feat1"</span><span class="crayon-sy">,</span><span class="crayon-s">"feat2"</span><span class="crayon-sy">,</span><span class="crayon-s">"feat3"</span><span class="crayon-sy">)</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>


<p>So, we successfully reduced a dataset of 7 features down to 3, maintaining a large amount of the information. But why did I choose 3? How do you know how many components to take?</p>
<p>Recall that the magnitude of the eigenvalue (corresponding to each eigenvector) tells us how much variance is represented by each component. Knowing this, we can graph the proportion of variance captured in each component, and decide how much is sufficient. I would advise a maximum of 3, because after that you can&#x2019;t visualize them all, and that&#x2019;s one of the reasons we did this in the first place!</p>
<p><a href="http://i0.wp.com/alexhwoods.com/wp-content/uploads/2016/01/EigenvaluePlot1.jpeg" rel="attachment wp-att-457"><img class="wp-image-457 alignleft" src="http://i0.wp.com/alexhwoods.com/wp-content/uploads/2016/01/EigenvaluePlot1.jpeg?resize=438%2C320" alt="EigenvaluePlot1"></a></p>









<p>The first two components capture 70% of the variance, and the third captures another 11%. I&#x2019;m definitely comfortable reducing down to 3 dimensions while maintaining 81% of the information.</p>
<p>And our data looks like this, with respect to the components. At this point, I would start <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a>.</p>
<p><a href="http://i1.wp.com/alexhwoods.com/wp-content/uploads/2016/01/DataAfterPCA.jpeg" rel="attachment wp-att-458"><img class=" wp-image-458 alignleft" src="http://i1.wp.com/alexhwoods.com/wp-content/uploads/2016/01/DataAfterPCA.jpeg?resize=411%2C329" alt="DataAfterPCA"></a></p>









<h3>PCA Using R&#x2019;s Built-in Function</h3>
<p>R has a built-in &#x2018;prcomp&#x2019; function for PCA. It&#x2019;s really straightforward.</p>

		<div id="crayon-56a2408694d88384632218" class="crayon-syntax crayon-theme-solarized-light crayon-font-monaco crayon-os-pc print-yes notranslate">
		
			<p class="crayon-toolbar"><span class="crayon-title">Using prcomp</span>
			</p>
			
			
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums ">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56a2408694d88384632218-1"><span class="crayon-c"># Extra Step - How To Do PCA Automatically in R (prcomp)</span></p><p class="crayon-line" id="crayon-56a2408694d88384632218-3"><span class="crayon-c"># Don't forget to scale the data, or set scale = True</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d88384632218-4"><span class="crayon-e">prcomp</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d88384632218-6"><span class="crayon-c"># does it look like the eigenvectors from before? It should! Note - if some vector v is an eigenvector,</span></p><p class="crayon-line" id="crayon-56a2408694d88384632218-7"><span class="crayon-c"># then c*v is an eigenvector, for any scalar c. (My point is, it's cool if the signs are switched).</span></p><p class="crayon-line" id="crayon-56a2408694d88384632218-9"><span class="crayon-e">plot</span><span class="crayon-sy">(</span><span class="crayon-e">prcomp</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-c"># similar to my plot above with the blue bars</span></p><p class="crayon-line crayon-striped-line" id="crayon-56a2408694d88384632218-10"><span class="crayon-e">summary</span><span class="crayon-sy">(</span><span class="crayon-e">prcomp</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56a2408694d88384632218-11"><span class="crayon-e">biplot</span><span class="crayon-sy">(</span><span class="crayon-e">prcomp</span><span class="crayon-sy">(</span><span class="crayon-v">attitude</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-c"># This one is pretty cool...</span></p><p class="crayon-line" id="crayon-56a2408694d88384632218-13"><span class="crayon-c"># At the very least, this affirms that what we did above is right! </span></p></div></td>
					</tr>
				</table>
			</div>
		</div>


<h3>Final Note: Eigenfaces</h3>
<p>There&#x2019;s this super cool application of PCA in computer vision called&#xA0;<a href="http://laid.delanover.com/explanation-face-recognition-using-eigenfaces/">Eigenfaces</a>. It basically uses these ideas to do&#xA0;face recognition. It makes me laugh quite a bit to think about the orthogonal components of someone&#x2019;s face hahaha.</p>
<p><a href="http://i2.wp.com/alexhwoods.com/wp-content/uploads/2016/01/3.png" rel="attachment wp-att-470"><img class="alignnone size-medium wp-image-470" src="http://i2.wp.com/alexhwoods.com/wp-content/uploads/2016/01/3.png?resize=300%2C201%20300w,%20http://i2.wp.com/alexhwoods.com/wp-content/uploads/2016/01/3.png?w=508%20508w" alt="3"></a></p>







<p id="jp-relatedposts" class="jp-relatedposts">
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
</p>			
		</article>
		
		
		
				
				
		</div>
</body></html>
