# Title: Recurrent Neural Networks Hardware Implementation on FPGA

[Original URL](http://arxiv.org/abs/1511.05552v1)

> Full-text links: PDF Other formats Current browse context: cs.NE < prev | next >

<span class="descriptor">Full-text links:</span>

- [PDF](http://arxiv.org/pdf/1511.05552v1)
- [Other formats](http://arxiv.org/format/1511.05552v1)

[![](http://arxiv.org/icons/licenses/by-4.0.png)](http://creativecommons.org/licenses/by/4.0/ "Rights to this article")

## Current browse context:

cs.NE

<span class="arrow">
  <a href="http://arxiv.org/prevnext?site=arxiv.org&amp;id=1511.05552&amp;context=cs.NE&amp;function=prev" title="previous in cs.NE (accesskey p)">&lt; prev</a>
</span>

 | 

<span class="arrow">
  <a href="http://arxiv.org/prevnext?site=arxiv.org&amp;id=1511.05552&amp;context=cs.NE&amp;function=next" title="next in cs.NE (accesskey n)">next &gt;</a>
</span>

[new](http://arxiv.org/list/cs.NE/new) | [recent](http://arxiv.org/list/cs.NE/recent) | [1511](http://arxiv.org/list/cs.NE/1511)

## Change to browse by:

[cs](http://arxiv.org/abs/1511.05552?context=cs)

## References & Citations

- [NASA ADS](http://adsabs.harvard.edu/cgi-bin/bib_query?arXiv:1511.05552)

## Bookmark

([what is this?](http://arxiv.org/help/social_bookmarking))

[![CiteULike logo](http://static.arxiv.org/icons/social/citeulike.png)](http://arxiv.org/ct?url=http%3A%2F%2Fwww.citeulike.org%2Fposturl%3Furl%3Dhttp%3A%2F%2Farxiv.org%2Fabs%2F1511.05552%26title%3DRecurrent%2520Neural%2520Networks%2520Hardware%2520Implementation%2520on%2520FPGA%26authors%3D&v=24080dde "Bookmark on CiteULike") [![BibSonomy logo](http://static.arxiv.org/icons/social/bibsonomy.png)](http://arxiv.org/ct?url=http%3A%2F%2Fwww.bibsonomy.org%2FBibtexHandler%3FrequTask%3Dupload%26url%3Dhttp%3A%2F%2Farxiv.org%2Fabs%2F1511.05552%26description%3DRecurrent%2520Neural%2520Networks%2520Hardware%2520Implementation%2520on%2520FPGA&v=2502a53f "Bookmark on BibSonomy") [![Mendeley logo](http://static.arxiv.org/icons/social/mendeley.png)](http://arxiv.org/ct?url=http%3A%2F%2Fwww.mendeley.com%2Fimport%2F%3Furl%3Dhttp%3A%2F%2Farxiv.org%2Fabs%2F1511.05552&v=ad291bcb "Bookmark on Mendeley") [![del.icio.us logo](http://static.arxiv.org/icons/social/delicious.png)](http://arxiv.org/ct?url=http%3A%2F%2Fdel.icio.us%2Fpost%3Furl%3Dhttp%3A%2F%2Farxiv.org%2Fabs%2F1511.05552%26description%3DRecurrent%2520Neural%2520Networks%2520Hardware%2520Implementation%2520on%2520FPGA&v=cda221c5 "Bookmark on del.icio.us") [![Digg logo](http://static.arxiv.org/icons/social/digg.png)](http://arxiv.org/ct?url=http%3A%2F%2Fdigg.com%2Fsubmit%3Furl%3Dhttp%3A%2F%2Farxiv.org%2Fabs%2F1511.05552%26title%3DRecurrent%2520Neural%2520Networks%2520Hardware%2520Implementation%2520on%2520FPGA&v=1f9c8d3c "Bookmark on Digg") [![Reddit logo](http://static.arxiv.org/icons/social/reddit.png)](http://arxiv.org/ct?url=http%3A%2F%2Freddit.com%2Fsubmit%3Furl%3Dhttp%3A%2F%2Farxiv.org%2Fabs%2F1511.05552%26title%3DRecurrent%2520Neural%2520Networks%2520Hardware%2520Implementation%2520on%2520FPGA&v=e56f4334 "Bookmark on Reddit") [![ScienceWISE logo](http://static.arxiv.org/icons/social/sciencewise.png)](http://arxiv.org/ct?url=http%3A%2F%2Fsciencewise.info%2Fbookmarks%2Fadd%3Furl%3Dhttp%3A%2F%2Farxiv.org%2Fabs%2F1511.05552&v=818b0957 "Bookmark on ScienceWISE")

<span class="descriptor">Authors:</span>

 [Andre Xian Ming Chang](http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1), [Berin Martini](http://arxiv.org/find/cs/1/au:+Martini_B/0/1/0/all/0/1), [Eugenio Culurciello](http://arxiv.org/find/cs/1/au:+Culurciello_E/0/1/0/all/0/1)

(Submitted on 17 Nov 2015)

> <span class="descriptor">Abstract:</span>

>  Recurrent Neural Networks (RNNs) have the ability to retain memory and learn data sequences, and are a recent breakthrough of machine learning. Due to the recurrent nature of RNNs, it is sometimes hard to parallelize all its computations on conventional hardware. CPUs do not currently offer large parallelism, while GPUs offer limited parallelism due to branching in RNN models. In this paper we present a hardware implementation of Long-Short Term Memory (LSTM) recurrent network on the programmable logic Zynq 7020 FPGA from Xilinx. We implemented a RNN with 2 layers and 128 hidden units in hardware and it has been tested using a character level language model. The implementation is more than $21\times$ faster than the ARM CPU embedded on the Zynq 7020 FPGA. This work can potentially evolve to a RNN co-processor for future mobile devices.

Comments: | 9 pages, 9 figures, International Conference on Learning Representations (ICLR)
--------- | ---------------------------------------------------------------------------------------------------------------------------------
Subjects: | <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>
Cite as:  | [arXiv:1511.05552](http://arxiv.org/abs/1511.05552) [cs.NE]
          | (or 

<span class="arxivid"><a href="http://arxiv.org/abs/1511.05552v1">arXiv:1511.05552v1</a> [cs.NE]</span>

 for this version)

## Submission history

From: Andre Xian Ming Chang [[view email](https://arxiv.org/show-email/5d6bd836/1511.05552)]<br>
**[v1]** Tue, 17 Nov 2015 02:20:37 GMT (1490kb,D)

[Which authors of this paper are endorsers?](http://arxiv.org/auth/show-endorsers/1511.05552) | [Disable MathJax]() ([What is MathJax?](http://arxiv.org/help/mathjax/))
